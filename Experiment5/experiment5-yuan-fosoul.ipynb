{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-05: word2vec Recommender System\n",
    "### Amirreza Fosoul and Bithiah Yuan\n",
    "\n",
    "1. [Pre-processing Text for word2vec](#section1)\n",
    "2. [Analogies with word2vec](#section2)\n",
    "3. [From Embeddings to Paper Recommendation](#section3)\n",
    "4. [Evaluation of Recommender System](#section4)\n",
    "5. [Improving the Recommender System with TF-IDF](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex5').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unexplode a column to an array of given datatype\n",
    "def unExplode(df, groupByColName, collectColName, colType):\n",
    "    types = {'string': StringType(), 'integer': IntegerType()}\n",
    "    df_collected = df.groupby(groupByColName).agg(f.concat_ws(\", \", f.collect_list(df[collectColName])).alias(collectColName))\n",
    "    result = df_collected.withColumn(collectColName, split(col(collectColName), \",\\s*\").cast(ArrayType(types[colType])).alias(collectColName)).orderBy(groupByColName)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "# Pre-explode user_df\n",
    "user_df_pre = user_df\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./paper0.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "#w_df.show()\n",
    "\n",
    "paper_df = w_df\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "### Exercise 5. 1 (Pre-processing Text for word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10 most similar words to “science” using conservative pre-processing\n",
      "\n",
      "+------------+------------------+\n",
      "|        word|        similarity|\n",
      "+------------+------------------+\n",
      "|    sciences| 0.868623673915863|\n",
      "|differential|0.8665519952774048|\n",
      "|      theory|0.8658170700073242|\n",
      "|    coverage| 0.864836573600769|\n",
      "|        such|0.8645626902580261|\n",
      "|       guide|  0.86416095495224|\n",
      "|  challenges|0.8610662221908569|\n",
      "|evolutionary|0.8600710034370422|\n",
      "|     problem|0.8578568696975708|\n",
      "|relationship|0.8570491075515747|\n",
      "+------------+------------------+\n",
      "\n",
      "top10 most similar words to “science” using intensive pre-processing\n",
      "\n",
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|  commun| 0.973919689655304|\n",
      "| modular|0.9605692028999329|\n",
      "| network| 0.959109902381897|\n",
      "|  physic|0.9511327743530273|\n",
      "|   natur|0.9416936039924622|\n",
      "|structur|0.9234450459480286|\n",
      "|   modul|0.9188250303268433|\n",
      "|    goal| 0.915787398815155|\n",
      "|   evolv|0.9104613661766052|\n",
      "|    real| 0.909706711769104|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################### Conservative pre-processing ###################################################\n",
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "# Conservative pre-processing df\n",
    "conservative_df = df\n",
    "conservative_df = conservative_df.withColumn(\"paperID\", conservative_df[\"paperID\"].cast(IntegerType()))\n",
    "\n",
    "# Define word2vec parameters\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "# Fit the w2v_cp model\n",
    "model = word2Vec.fit(conservative_df)\n",
    "\n",
    "print(\"top10 most similar words to “science” using conservative pre-processing\")\n",
    "print()\n",
    "# Find the words most similar to science (top 10)\n",
    "synonyms = model.findSynonyms('science', 10)\n",
    "synonyms.show()\n",
    "\n",
    "# #################################### Intensive pre-processing #####################################################\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('tokens', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# Intensive pre-processing df\n",
    "intensive_df = df\n",
    "paper_terms = df\n",
    "\n",
    "# w2v_ip model\n",
    "model2 = word2Vec.fit(intensive_df)\n",
    "\n",
    "print(\"top10 most similar words to “science” using intensive pre-processing\")\n",
    "print()\n",
    "# Find the words most similar to science (top 10)\n",
    "# We stem science to have token matches\n",
    "synonyms2 = model2.findSynonyms(ps.stem('science'), 10)\n",
    "synonyms2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results for intensive pre-processing are better (the similarity scores are higher) because unimportant words are removed and words with the same stems are shown in the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "### Exercise 5. 2 (Analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analogy for the conservative pre-processing model:\n",
      "\n",
      "+------------+------------------+\n",
      "|        word|        similarity|\n",
      "+------------+------------------+\n",
      "|   filtering|0.9699006080627441|\n",
      "|     studied|0.9678228497505188|\n",
      "|increasingly|0.9665361642837524|\n",
      "|         roc|0.9576231241226196|\n",
      "|  analytical| 0.954590916633606|\n",
      "+------------+------------------+\n",
      "\n",
      "analogy for the intensive pre-processing model:\n",
      "\n",
      "+------+------------------+\n",
      "|  word|        similarity|\n",
      "+------+------------------+\n",
      "|precis|0.8722060918807983|\n",
      "| regul|0.8286675214767456|\n",
      "|length|0.8236488699913025|\n",
      "|obtain| 0.819008469581604|\n",
      "|  valu|0.8135963678359985|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For queries with multiple words\n",
    "# Store the words in an array of strings in a dataframe\n",
    "# Get the average vector using word2vec\n",
    "def getAverageVector(query, model):\n",
    "    df = sqlContext.createDataFrame([[query]], ['tokens'])\n",
    "    return model.transform(df)\n",
    "\n",
    "def analogy(word1, word2, word3, model, stemming=False):\n",
    "    # Get the vectors from the word2vec model\n",
    "    vec = model.getVectors()\n",
    "    # Store the query words into a list\n",
    "    words = [word1, word2, word3]\n",
    "    keywords = []\n",
    "    vectors = []\n",
    "    for i in words:\n",
    "        # Clean the query and store them in a list\n",
    "        # Note: multiple words in one query will result in a nested list\n",
    "        keywords.append([x.lower().strip() for x in re.split(\"[^A-Za-z]+\", i)])\n",
    "        \n",
    "    # When using intensive preprocessing, stemming needs to be applied to the queries\n",
    "    if stemming:\n",
    "        for i, query in enumerate(keywords):\n",
    "            for j, word in enumerate(query):\n",
    "                # Apply stemming to each word in query\n",
    "                keywords[i][j] = ps.stem(word)\n",
    "    \n",
    "    for query in keywords:\n",
    "        # If multiple words are in one query then get the average of their vectors\n",
    "        if len(query) > 1:\n",
    "            vectors.append(getAverageVector(query, model).head()[1])\n",
    "        else:\n",
    "            # Otherwise append the vector of the word in a list\n",
    "            vectors.append(vec.where(vec.word==query[0]).head()[1])\n",
    "    # Perform analogy effect from word2vec\n",
    "    w = vectors[0] - vectors[1] + vectors[2]\n",
    "    # Call the word2vec model to find the top 5 similar words\n",
    "    result = model.findSynonyms((-1)*w,5)\n",
    "    return result\n",
    "\n",
    "print(\"analogy for the conservative pre-processing model:\")\n",
    "print()\n",
    "analogy(\"machine learning\", \"prediction\", \"recommender system\", model, stemming=False).show()\n",
    "\n",
    "print(\"analogy for the intensive pre-processing model:\")\n",
    "print()\n",
    "analogy(\"machine learning\", \"prediction\", \"recommender systems\", model2, stemming=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "### Exercise 5. 3 (From Embeddings to Paper Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|        user_profile|\n",
      "+--------------------+--------------------+\n",
      "|4c8912d1b04471cf5...|[0.00670253718110...|\n",
      "|90f1a3e6fcdbf9bc5...|[0.00633062649154...|\n",
      "|d503571e44a0373eb...|[0.00427934238594...|\n",
      "|b36c3189bb1457cd0...|[0.00901340864598...|\n",
      "|bbcd9dae3160ddcb9...|[0.00732479973691...|\n",
      "|589b870a611c25fa9...|[0.00621315018896...|\n",
      "|f1e1cd4ff25018273...|[0.00903882173893...|\n",
      "|a0bbf6bb9b1c818f3...|[0.00821659074022...|\n",
      "|1eac022a97d683eac...|[0.00773287280969...|\n",
      "|3b715ebaf1f8f81a1...|[0.00861022538106...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# average of the embeddings of all words which appear in the paper’s text\n",
    "paper_w2v_cp = model.transform(conservative_df)\n",
    "paper_w2v_cp = paper_w2v_cp.select(\"paperID\", col(\"result\").alias(\"paper_profile\"))\n",
    "\n",
    "# compute the user profile using word2vec\n",
    "def getUserProfile(user_df, preprocessed_df):\n",
    "    # concatenate the papers in user libraries with their tokens inside w2v_cp df\n",
    "    user_doc = user_df.join(preprocessed_df, ['paperID']).orderBy(\"userID\")\n",
    "    user_doc = user_doc.drop(\"paperID\")\n",
    "    # Concatenate all words of the user library\n",
    "    user_doc = user_doc.rdd.map(lambda user_doc: (user_doc.userID, user_doc.tokens)).reduceByKey(lambda x,y: x + y).toDF(['userID','tokens'])\n",
    "    \n",
    "    # Use word2Vec model fitting\n",
    "    model_user = word2Vec.fit(user_doc)\n",
    "    # Get average of the embeddings of all words in users' libraries\n",
    "    user_w2v_cp = model_user.transform(user_doc)\n",
    "    user_w2v_cp = user_w2v_cp.select(\"userID\", col(\"result\").alias(\"user_profile\"))\n",
    "    return user_w2v_cp\n",
    "\n",
    "user_w2v_cp = getUserProfile(user_df, conservative_df)\n",
    "user_w2v_cp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "| paperID|              tokens|\n",
      "+--------+--------------------+\n",
      "|12832332|[deepening, democ...|\n",
      "| 1305474|[the, tesla, broa...|\n",
      "| 1001231|[anonymous, commu...|\n",
      "|  352713|[survey, sensor, ...|\n",
      "|  956315|[wormhole, attack...|\n",
      "|  945604|[packet, leashes,...|\n",
      "|10294999|[distance, recons...|\n",
      "|  967275|[the, survivabili...|\n",
      "|  115945|[twelvestep, prog...|\n",
      "|11733005|[evidence, for, w...|\n",
      "| 9045137|[nuclear, dna, co...|\n",
      "| 3728173|[evolution, indiv...|\n",
      "| 8310458|[evolution, compl...|\n",
      "|   80546|[the, arbitrarine...|\n",
      "| 5842862|[how, choose, goo...|\n",
      "| 1242600|[how, write, cons...|\n",
      "| 3467077|[defrosting, the,...|\n",
      "|  309395|[why, most, publi...|\n",
      "|  305755|[the, structure, ...|\n",
      "| 6603134|[how, build, moti...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())\n",
    "\n",
    "def getUnrated(user_df):\n",
    "    # Add a new column of unrated papers for each user\n",
    "    unrated_df = user_df.withColumn(\"unrated\", get_unrated(user_df.paperID))\n",
    "    unrated_df = unrated_df.drop(\"paperID\")\n",
    "    unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "    unrated_df = unrated_df.drop(\"unrated\")\n",
    "    \n",
    "    return unrated_df\n",
    "\n",
    "unrated_df = getUnrated(user_df_pre)\n",
    "\n",
    "conservative_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                        |\n",
      "+--------------------------------+----------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[3281478, 6603134, 816066, 3614773, 822253, 965334, 249, 4302361, 115945, 5394760]|\n",
      "+--------------------------------+----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a recommender system based on word2vector\n",
    "def w2vRS(user_df, unrated_df, preprocessed_df, k, passUserID=False, userID=None):\n",
    "    \n",
    "    # compute the user profile and paper prfiles\n",
    "    model = word2Vec.fit(preprocessed_df)\n",
    "    paper_w2v_cp = model.transform(preprocessed_df)\n",
    "    paper_w2v_cp = paper_w2v_cp.select(\"paperID\", col(\"result\").alias(\"paper_profile\"))\n",
    "    user_w2v_cp = getUserProfile(user_df, preprocessed_df)\n",
    "    \n",
    "    df = unrated_df.join(user_w2v_cp, [\"userID\"]).join(paper_w2v_cp, [\"paperID\"])\n",
    "        \n",
    "    # if the userID passed, filter the dataframe\n",
    "    if passUserID:\n",
    "        df = df.where(df.userID==userID)\n",
    "        \n",
    "    # Apply similarity metric to the user_profile and paper_profile\n",
    "    sim_df = df.withColumn('Similarity', compute_sim(df.user_profile, df.paper_profile))\n",
    "    # Partition by userID and order by the similarity in descending order\n",
    "\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    # Add row numbers to the rows and get the top-k rows\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "\n",
    "    # Renaming\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    w2vRS_df = get_r.select(\"userID\", \"paperID\")\n",
    "    # un-explode, concatenate the recommended papers for each user\n",
    "    w2vRS_df = unExplode(w2vRS_df, 'userID', 'paperID', 'integer')\n",
    "    w2vRS_df = w2vRS_df.select('userID', col('paperID').alias('top_papers'))\n",
    "    \n",
    "    return w2vRS_df\n",
    "\n",
    "# sample user\n",
    "user = \"1eac022a97d683eace8815545ce3153f\"\n",
    "\n",
    "# get the recommendations for the sample user\n",
    "user_rec = w2vRS(user_df, unrated_df, conservative_df, 10, passUserID=True, userID=user)\n",
    "\n",
    "user_rec.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "### Exercise 5. 4 (Evaluation of Recommender System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lib_size_udf = udf(lambda x:len(x), IntegerType())\n",
    "\n",
    "def sampling(num_users, df, trainingSize=0.8):\n",
    "    # first we sample users and then we sample each user's library\n",
    "    \n",
    "    get_training_size_udf = udf(lambda x:int(x*trainingSize), IntegerType())\n",
    "    sampled_users = df.orderBy(rand()).limit(num_users)\n",
    "    sampled_users = sampled_users.withColumn('libSize', get_lib_size_udf('paperID'))\n",
    "    sampled_users = sampled_users.withColumn('trainingSize', get_training_size_udf('libSize'))\n",
    "    \n",
    "    # explode the paperIDs for each user\n",
    "    sampled_exploded = sampled_users.withColumn('paperID', explode(col('paperID')))\n",
    "\n",
    "    # Partion by userID and order them randomly\n",
    "    window = Window.partitionBy(col('userID')).orderBy(rand())\n",
    "\n",
    "    # Get row numbers\n",
    "    sampled_exploded = sampled_exploded.select(col('*'), row_number().over(window).alias('row_number'))\n",
    "\n",
    "    # Get the rows less than or equal to the training set size\n",
    "    # The rows will be different each time because of .orderBy(rand()) in the window function\n",
    "    training_df = sampled_exploded.where(col('row_number') <= col('trainingSize'))\n",
    "    training_df = training_df.select('userID', 'paperID').orderBy('userID')\n",
    "    #training_df.show()\n",
    "\n",
    "    # Get the test set by selecting the rows greater than the training size\n",
    "    test_df = sampled_exploded.where(col('row_number') > col('trainingSize'))\n",
    "    test_df = test_df.select('userID', 'paperID').orderBy('userID')\n",
    "    \n",
    "    return (training_df, test_df)\n",
    "\n",
    "(training_df, test_df) = sampling(50, user_df_pre, trainingSize=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_collected = unExplode(training_df, \"userID\", \"paperID\", \"integer\")\n",
    "\n",
    "train_unrated_df = getUnrated(training_df_collected)\n",
    "\n",
    "k = 10\n",
    "\n",
    "# Get recommendations for the sampled users\n",
    "user_rec = w2vRS(training_df, train_unrated_df, conservative_df, k)\n",
    "#user_rec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the column to array\n",
    "def castToArray(df, colName):\n",
    "    dff = df.withColumn(colName, split(col(colName), \", \").cast(ArrayType(IntegerType())))\n",
    "    return dff\n",
    "\n",
    "\n",
    "def getHits(train, test):\n",
    "    return list(set(train).intersection(test))\n",
    "\n",
    "getHits_udf = udf(getHits, ArrayType(IntegerType()))\n",
    "\n",
    "num_user = 50\n",
    "\n",
    "def hitSize_k(hits):\n",
    "    if len(hits) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(hits)/k\n",
    "\n",
    "hitSize_k_udf = udf(lambda x: hitSize_k(x), FloatType())\n",
    "\n",
    "def precisionK(df):\n",
    "    df = df.withColumn(\"hitSize_k\", hitSize_k_udf(\"Hits\"))\n",
    "    sumHits_k = df.select(f.sum(\"hitSize_k\")).collect()[0][0]\n",
    "    precision = (1/num_user)*sumHits_k\n",
    "    return precision\n",
    "\n",
    "def hitSize_testSize(hits, testSize):\n",
    "    return len(hits)/len(testSize)\n",
    "\n",
    "hitSize_testSize_udf = udf(hitSize_testSize, FloatType())\n",
    "\n",
    "def recallK(df):\n",
    "    df = df.withColumn(\"hitSize_testSize\", hitSize_testSize_udf(df.Hits, df.paperID))\n",
    "    sumHits_test = df.select(f.sum(\"hitSize_testSize\")).collect()[0][0]\n",
    "    recall = (1/num_user)*sumHits_test\n",
    "    return recall\n",
    "\n",
    "# Get the positions of the hits in the test set\n",
    "# find the minimum hit index that appeared in the test set\n",
    "def getPositionU(hits, test):\n",
    "    if not hits:\n",
    "        return 0.0\n",
    "    else:\n",
    "        hit_index = []\n",
    "        for i in range(0, len(hits)):\n",
    "            # Index starting at 1 to avoid division by 0\n",
    "            hit_index.append((test.index(hits[i]))+1)\n",
    "    \n",
    "        return 1/int((np.min(hit_index)))\n",
    "\n",
    "getPositionU_udf = udf(getPositionU, FloatType())\n",
    "\n",
    "def mrrK(df):\n",
    "    df = df.withColumn(\"P_u\", getPositionU_udf(df.Hits, df.paperID))\n",
    "    sumP_u = df.select(f.sum(\"P_u\")).collect()[0][0]\n",
    "    mrr = (1/num_user)*sumP_u\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates the precision@k, recall@k, mrr@k\n",
    "def  evaluateK(k, rec_df, test_df):\n",
    "    \n",
    "    # Concatenate the papers for test_df\n",
    "    test_df_collected = unExplode(test_df, 'userID', 'paperID', 'integer')\n",
    "    \n",
    "    # Join recommended papers with test_df\n",
    "    joined_test = test_df_collected.join(rec_df, \"userID\")\n",
    "    joined_test = joined_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "    \n",
    "    # find the hits by calling udf to make the intersection of test set and hits\n",
    "    hits = joined_test.withColumn('Hits', getHits_udf(joined_test.train_set, joined_test.test_set))\n",
    "    hits = hits.select(\"userID\", \"Hits\")\n",
    "    \n",
    "    # Join the test set and the hits\n",
    "    test_hits =  test_df_collected.join(hits, \"userID\")\n",
    "    \n",
    "    return (precisionK(hits), recallK(test_hits), mrrK(test_hits))\n",
    "    \n",
    "K = [10, 100]\n",
    "\n",
    "for i in range(0, len(K)):\n",
    "    prec, recall, mrr = evaluateK(K[i], user_rec, test_df)  \n",
    "    print(\"The precision@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % prec))\n",
    "    print(\"The Recall@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % recall))\n",
    "    print(\"The MRR@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % mrr) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Sample users with library size greater than 20 ######################################\n",
    "\n",
    "sampled_20 = user_df_pre.withColumn(\"libSize\", get_lib_size_udf(\"paperID\")).filter(col(\"libSize\") > 20)\n",
    "\n",
    "(training20_df, test20_df) = sampling(50, sampled_20, trainingSize=0.8)\n",
    "\n",
    "training20_df_collected = unExplode(training20_df, \"userID\", \"paperID\", \"integer\")\n",
    "\n",
    "train20_unrated_df = getUnrated(training20_df_collected)\n",
    "\n",
    "k = 10\n",
    "\n",
    "# Get recommendations for the sampled users with more than 20 papers in their libraries\n",
    "user_rec20 = w2vRS(training20_df, train20_unrated_df, conservative_df, k)\n",
    "#user_rec20.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [10, 100]\n",
    "\n",
    "for i in range(0, len(k)):\n",
    "    prec, recall, mrr = evaluateK(k[i], user_rec20, test20_df)  \n",
    "    print(\"The precision@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % prec))\n",
    "    print(\"The Recall@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % recall))\n",
    "    print(\"The MRR@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % mrr) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After repeating the experiment by picking only users which have more than 20 papers in their libraries, we noticed that the performance of the RS improves. This makes sense because there are more papers in the users' libraries, so the similarity scores from the RS will be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "### Exercise 5. 5 (Improving the Recommender System with TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "ip_df = intensive_df.select(\"paperID\", f.explode(\"tokens\").alias(\"tokens\")).distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "ip_df = ip_df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = ip_df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "\n",
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| − 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")\n",
    "#terms_index_hash.show()\n",
    "\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "#print(num_terms)\n",
    "\n",
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"tokens\").alias(\"terms\"))\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "# tf_df is now a df with a column of paperID and a column of lists of the terms (unexploded)\n",
    "#tf_df.show()\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "vector_df = model.transform(tf_df)\n",
    "vector_df = vector_df.select(\"paperID\", col(\"vectors\").alias(\"term_frequency_sparse\"))\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|paperID|index|\n",
      "+-------+-----+\n",
      "| 159967|    0|\n",
      "| 159967|   18|\n",
      "| 159967|   19|\n",
      "| 159967|   80|\n",
      "| 159967|   62|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTop10Terms(sparsevector):\n",
    "    \n",
    "    # Sort indices of SparseVector by the scores in reverse order\n",
    "    # Get top 10 indices of the score\n",
    "    return sorted(range(len(sparsevector)), key=lambda k: sparsevector[k], reverse=True)[:10]\n",
    "\n",
    "getTop10TermsUdf = udf(lambda x: getTop10Terms(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Get a column of the top10 words of each paper represented by the hashed index\n",
    "tf_idf_filtered = tf_idf_built_in.withColumn(\"index\", getTop10TermsUdf(col(\"features\")))\n",
    "tf_idf_filtered = tf_idf_filtered.drop(\"features\")\n",
    "\n",
    "# Explode top10 terms for each paper\n",
    "tf_idf_filtered = tf_idf_filtered.withColumn(\"index\", explode(tf_idf_filtered.index))\n",
    "\n",
    "# Papers now have only the 10 most important words\n",
    "tf_idf_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes to get the terms from the hash\n",
    "hash_joined = terms_index_hash.join(tf_idf_filtered, [\"index\"])\n",
    "hash_joined = hash_joined.drop(\"index\")\n",
    "\n",
    "# The dataframe now has a column of paperID and their top10 important terms\n",
    "hash_joined_collected = unExplode(hash_joined, \"paperID\", \"terms\", \"string\")\n",
    "hash_joined_collected = hash_joined_collected.select(\"paperID\", col(\"terms\").alias(\"tokens\"))\n",
    "\n",
    "#hash_joined_collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                            |\n",
      "+--------------------------------+--------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[5394760, 3281478, 967275, 469428, 166220, 1326856, 820297, 3614773, 11191048, 740681]|\n",
      "+--------------------------------+--------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "user = \"1eac022a97d683eace8815545ce3153f\"\n",
    "\n",
    "# Get recommendations using tf_idf\n",
    "user_rec_tf = w2vRS(user_df, unrated_df, hash_joined_collected, k, passUserID=True, userID=user)\n",
    "\n",
    "user_rec_tf.show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user profile for the sampled users using intensive preprocessing\n",
    "sampled_user_w2v_ip = getUserProfile(training_df, intensive_df)\n",
    "\n",
    "k = 10\n",
    "\n",
    "# Get recommendations for the sampled users\n",
    "user_rec_tf = w2vRS(training_df, train_unrated_df, hash_joined_collected, k)\n",
    "\n",
    "K = [10, 100]\n",
    "\n",
    "for i in range(0, len(K)):\n",
    "    prec, recall, mrr = evaluateK(K[i], user_rec_tf, test_df)  \n",
    "    print(\"The precision@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % prec))\n",
    "    print(\"The Recall@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % recall))\n",
    "    print(\"The MRR@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % mrr) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user profile for the sampled users with libraries greater than 20\n",
    "sampled20_user_w2v_ip = getUserProfile(training20_df, intensive_df)\n",
    "\n",
    "k = 10\n",
    "\n",
    "# Get recommendations for the sampled users\n",
    "user_rec20_tf = w2vRS(training20_df, train20_unrated_df, hash_joined_collected, k)\n",
    "\n",
    "k = [10, 100]\n",
    "\n",
    "for i in range(0, len(k)):\n",
    "    prec, recall, mrr = evaluateK(k[i], user_rec20_tf, test20_df)  \n",
    "    print(\"The precision@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % prec))\n",
    "    print(\"The Recall@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % recall))\n",
    "    print(\"The MRR@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % mrr) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that the tf-idf version of the recommender system is better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
