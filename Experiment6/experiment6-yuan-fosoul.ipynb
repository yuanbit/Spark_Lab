{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-06\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark.sql.functions as f\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, DoubleType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import concat, col, lit, size\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.appName('ex6').getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unexplode a column to an array of given datatype\n",
    "def unExplode(df, groupByColName, collectColName, colType):\n",
    "    types = {'string': StringType(), 'integer': IntegerType()}\n",
    "    df_collected = df.groupby(groupByColName).agg(f.concat_ws(\", \", f.collect_list(df[collectColName])).alias(collectColName))\n",
    "    result = df_collected.withColumn(collectColName, split(col(collectColName), \",\\s*\").cast(ArrayType(types[colType])).alias(collectColName)).orderBy(groupByColName)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|              userID|             paperID|libSize|\n",
      "+--------------------+--------------------+-------+\n",
      "|589b870a611c25fa9...|[12832332, 130547...|      5|\n",
      "|90f1a3e6fcdbf9bc5...|[115945, 11733005...|      5|\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read user ratings into Dataframe\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./example.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "user_pre = user_df\n",
    "\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Count the number of distinct users\n",
    "numUsers = user_df.select(\"userID\").distinct().count()\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "userLib = user_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType()))).orderBy(\"userID\")\n",
    "\n",
    "get_len_udf = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "userLib = userLib.withColumn(\"libSize\", get_len_udf(col(\"paperID\")))\n",
    "\n",
    "userLib.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+----+--------------------+\n",
      "| paperID|         type|pages|year|               words|\n",
      "+--------+-------------+-----+----+--------------------+\n",
      "|12832332|       inbook|   29|2013|deepening democra...|\n",
      "| 1305474|         misc| null|2002|the {tesla} broad...|\n",
      "| 1001231|  proceedings| null|2005|anonymous communi...|\n",
      "|  352713|         misc|   12|2002|a survey on senso...|\n",
      "|  956315|      article|   10|2006|wormhole attacks ...|\n",
      "|  945604|inproceedings| null|2003|packet leashes: a...|\n",
      "|10294999|      article| null|2012|a 2\\% distance to...|\n",
      "|  967275|inproceedings|   11|2005|on the survivabil...|\n",
      "|  115945|      article|   11|2005|a twelve-step pro...|\n",
      "|11733005|      article|    7|2012|evidence for wide...|\n",
      "| 9045137|      article|   24|2007|nuclear {dna} con...|\n",
      "| 3728173|      article|    5|2007|evolution of indi...|\n",
      "| 8310458|      article|   15|2008|evolution of comp...|\n",
      "|   80546|      article|   17|2004|the arbitrariness...|\n",
      "| 5842862|      article|    2|2009|how to choose a g...|\n",
      "| 1242600|      article|    4|2007|how to write cons...|\n",
      "| 3467077|      article| null|2008|defrosting the di...|\n",
      "|  309395|      article| null|2005|why most publishe...|\n",
      "|  305755|      article| null|2005|the structure of ...|\n",
      "| 6603134|      article|    1|2010|how to build a mo...|\n",
      "+--------+-------------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "    \n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./paper0.csv')\n",
    "#w_df = w_df.select(\"_c0\", \"_c01\" \"_c06\", \"_c09\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(\"_c0\", \"_c1\", \"_c6\", \"_c9\", \"_c13\", \"_c14\")\n",
    "#w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c01\").alias(\"type\"), col(\"_c06\").alias(\"pages\"), col(\"_c09\").alias(\"year\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c1\").alias(\"type\"), col(\"_c6\").alias(\"pages\"), col(\"_c9\").alias(\"year\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), \"type\", \"pages\", \"year\", concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "w_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6. 1 (Problem Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----+----+-----------+-------+--------+\n",
      "|              userID| paperID|rating|pages|year|       type|libSize|avgPages|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+\n",
      "|589b870a611c25fa9...|11733005|     0|    7|2012|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 9045137|     0|   24|2007|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...|  115945|     0|   11|2005|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 8310458|     0|   15|2008|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 3728173|     0|    5|2007|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 1001231|     1| 17.0|2005|proceedings|      5|    17.0|\n",
      "|589b870a611c25fa9...|12832332|     1|   29|2013|     inbook|      5|    17.0|\n",
      "|589b870a611c25fa9...|  352713|     1|   12|2002|       misc|      5|    17.0|\n",
      "|589b870a611c25fa9...| 1305474|     1| 17.0|2002|       misc|      5|    17.0|\n",
      "|589b870a611c25fa9...|  956315|     1|   10|2006|    article|      5|    17.0|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|     0|   29|2013|     inbook|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|     0| 12.4|2002|       misc|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|  956315|     0|   10|2006|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|     0|   12|2002|       misc|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|     0| 12.4|2005|proceedings|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|     1|   11|2005|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|     1|   24|2007|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|     1|    7|2012|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 3728173|     1|    5|2007|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|     1|   15|2008|    article|      5|    12.4|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a user hash map dataframe by adding a column with descending row numbers \n",
    "# to the pre-explode user dataframe\n",
    "# user_hash_map = user_pre.withColumn(\"user_idx\", F.monotonically_increasing_id().cast(IntegerType()))\n",
    "\n",
    "# Select the user indices and paperID\n",
    "#rating = user_hash_map.select('user_idx', 'paperID')\n",
    "rating = userLib.select('userID', 'paperID', 'libSize')\n",
    "\n",
    "# # Split the string if all paperIDs into individual paperIDs and cast the datatype to int\n",
    "# rating = rating.withColumn(\"papers\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"papers\"))\n",
    "# # Drop the column with the string of paperIDs\n",
    "# rating = rating.drop(\"paperID\")\n",
    "\n",
    "# Add a column of 1's called ratings for the rated papers w.r.t to the user index\n",
    "rating = rating.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))\n",
    "\n",
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "\n",
    "    # Select n random papers from the unrated papers where n is the \n",
    "    # length of the rated papers list of each user\n",
    "    unrated_papers = random.sample(unrated, len(papers))\n",
    "            \n",
    "    return unrated_papers\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "rating = rating.withColumn(\"unrated\", get_unrated(rating.paperID))\n",
    "\n",
    "#Create a dataframe with the rated papers and their ratings of 1's\n",
    "rated_df = rating.select(\"userID\", f.explode(\"paperID\").alias(\"paperID\"), \"rating\", \"libSize\")\n",
    "\n",
    "# Create a dataframe with the unrated papers\n",
    "unrated_df = rating.select(\"userID\", f.explode(\"unrated\").alias(\"unrated\"), \"libSize\")\n",
    "# Add a column of ratings of 0's to the unrated papers\n",
    "unrated_df = unrated_df.withColumn(\"rating\", lit(0))\n",
    "unrated_df = unrated_df.select(\"userID\", col(\"unrated\").alias(\"paperID\"), \"rating\", \"libSize\")\n",
    "\n",
    "# Union the rated and unrated dataframes and order by the user indices\n",
    "rating_matrix = rated_df.union(unrated_df).orderBy(\"userID\")\n",
    "\n",
    "feature_df = rating_matrix.join(w_df, [\"paperID\"]).orderBy(\"userID\")\n",
    "\n",
    "window = Window.partitionBy(col(\"userID\"))\n",
    "\n",
    "feature_df_rated = feature_df.filter(feature_df.rating == 1).groupBy(\"userID\", \"paperID\", \"rating\", \"pages\", \"year\", \"type\", \"libSize\").agg(f.avg(\"pages\").over(window).alias(\"avgPages\")).orderBy(\"userID\")\n",
    "\n",
    "feature_df_unrated = feature_df.filter(feature_df.rating == 0)\n",
    "feature_df_unrated = feature_df_unrated.withColumn('avgPages', lit(\" \").cast(StringType()))\n",
    "feature_df_unrated = feature_df_unrated.select(\"userID\", \"paperID\", \"rating\", \"pages\", \"year\", \"type\", \"libSize\", \"avgPages\")\n",
    "\n",
    "y = feature_df_rated.select('userID', 'avgPages').distinct()\n",
    "\n",
    "feature_df_unrated =  feature_df_unrated.drop(\"avgPages\")\n",
    "\n",
    "x = feature_df_unrated.join(y, 'userID')\n",
    "\n",
    "\n",
    "model_df = feature_df_rated.union(x).orderBy(\"userID\", \"rating\")\n",
    "model_df1 = model_df.withColumn(\"pages\", coalesce(model_df.pages, model_df.avgPages))\n",
    "\n",
    "model_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----+----+-----------+-------+--------+---------+-------------+\n",
      "|              userID| paperID|rating|pages|year|       type|libSize|avgPages|typeIndex|  encodedType|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+---------+-------------+\n",
      "|589b870a611c25fa9...| 8310458|     0|   15|2008|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...| 9045137|     0|   24|2007|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...| 3728173|     0|    5|2007|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...|  115945|     0|   11|2005|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...|11733005|     0|    7|2012|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...| 1001231|     1| 17.0|2005|proceedings|      5|    17.0|      2.0|(3,[2],[1.0])|\n",
      "|589b870a611c25fa9...|  352713|     1|   12|2002|       misc|      5|    17.0|      1.0|(3,[1],[1.0])|\n",
      "|589b870a611c25fa9...|  956315|     1|   10|2006|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...|12832332|     1|   29|2013|     inbook|      5|    17.0|      3.0|    (3,[],[])|\n",
      "|589b870a611c25fa9...| 1305474|     1| 17.0|2002|       misc|      5|    17.0|      1.0|(3,[1],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|     0| 12.4|2005|proceedings|      5|    12.4|      2.0|(3,[2],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|     0| 12.4|2002|       misc|      5|    12.4|      1.0|(3,[1],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|  956315|     0|   10|2006|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|     0|   12|2002|       misc|      5|    12.4|      1.0|(3,[1],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|     0|   29|2013|     inbook|      5|    12.4|      3.0|    (3,[],[])|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|     1|   11|2005|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|     1|    7|2012|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|     1|   24|2007|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 3728173|     1|    5|2007|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|     1|   15|2008|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
    "model = stringIndexer.fit(model_df1)\n",
    "indexed = model.transform(model_df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"typeIndex\", outputCol=\"encodedType\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df1 = encoded.withColumn(\"pages\", encoded[\"pages\"].cast(DoubleType()))\n",
    "model_df1 = model_df1.withColumn(\"year\", model_df1[\"year\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+\n",
      "|              userID| paperID|label|            features|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|589b870a611c25fa9...| 8310458|    0|[15.0,2008.0,5.0,...|\n",
      "|589b870a611c25fa9...|11733005|    0|[7.0,2012.0,5.0,1...|\n",
      "|589b870a611c25fa9...| 3728173|    0|[5.0,2007.0,5.0,1...|\n",
      "|589b870a611c25fa9...| 9045137|    0|[24.0,2007.0,5.0,...|\n",
      "|589b870a611c25fa9...|  115945|    0|[11.0,2005.0,5.0,...|\n",
      "|589b870a611c25fa9...|12832332|    1|[29.0,2013.0,5.0,...|\n",
      "|589b870a611c25fa9...| 1305474|    1|[17.0,2002.0,5.0,...|\n",
      "|589b870a611c25fa9...| 1001231|    1|[17.0,2005.0,5.0,...|\n",
      "|589b870a611c25fa9...|  352713|    1|[12.0,2002.0,5.0,...|\n",
      "|589b870a611c25fa9...|  956315|    1|[10.0,2006.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|  956315|    0|[10.0,2006.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|    0|[12.4,2002.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|    0|[12.4,2005.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|    0|[29.0,2013.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|    0|[12.0,2002.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|    1|[11.0,2005.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|    1|[7.0,2012.0,5.0,1...|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|    1|[24.0,2007.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 3728173|    1|[5.0,2007.0,5.0,1...|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|    1|[15.0,2008.0,5.0,...|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"pages\", \"year\", \"libSize\", \"avgPages\", \"encodedType\"], outputCol=\"features\")\n",
    "\n",
    "preprocessed_df = assembler.transform(model_df1)\n",
    "preprocessed_df = preprocessed_df.select(\"userID\", \"paperID\", col(\"rating\").alias(\"label\"),\"features\")\n",
    "\n",
    "preprocessed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6. 3 (Classificatoin Algorithms) & Exercise 6. 4 (Model Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|    1|[17.0,2002.0,5.0,...|\n",
      "|       1.0|    0|[12.0,2002.0,5.0,...|\n",
      "|       0.0|    1|[5.0,2007.0,5.0,1...|\n",
      "+----------+-----+--------------------+\n",
      "\n",
      "Root-mean-square error = 0.8164965809277261\n"
     ]
    }
   ],
   "source": [
    "def svm(df, features):\n",
    "    # Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "    #Automatically identify categorical features, and index them.\n",
    "    #We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    featureIndexer = VectorIndexer(inputCol=features, outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    (trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # Train a DecisionTree model.\n",
    "    dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "    # Chain indexers and tree in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"prediction\", \"label\", features).show()\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator =  RegressionEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root-mean-square error = \" + str(rmse))\n",
    "    \n",
    "svm(preprocessed_df, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             0|    0|[5.0,2007.0,5.0,1...|\n",
      "|             1|    0|[15.0,2008.0,5.0,...|\n",
      "|             1|    0|[24.0,2007.0,5.0,...|\n",
      "|             1|    0|[12.4,2005.0,5.0,...|\n",
      "|             1|    1|[5.0,2007.0,5.0,1...|\n",
      "+--------------+-----+--------------------+\n",
      "\n",
      "Root-mean-square error = 0.7745966692414833\n"
     ]
    }
   ],
   "source": [
    "def randomForest(df, features):\n",
    "    # Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "    # Automatically identify categorical features, and index them.\n",
    "    # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    featureIndexer = VectorIndexer(inputCol=features, outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    (trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # Train a RandomForest model.\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "    # Convert indexed labels back to original labels.\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "\n",
    "    # Chain indexers and forest in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"predictedLabel\", \"label\", features).show()\n",
    "\n",
    "    evaluator =  RegressionEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root-mean-square error = \" + str(rmse))\n",
    "    \n",
    "randomForest(preprocessed_df, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6. 5 (Additional Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Vector representation for the papers ###########################################\n",
    "\n",
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('stemmed', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"stemmed\")\n",
    "\n",
    "# Create a new df to store the paperID and stemmed tokens\n",
    "paper_terms = df\n",
    "\n",
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "df = df.select(\"paperID\", f.explode(\"stemmed\").alias(\"tokens\")).distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "df = df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "#important_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| − 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")\n",
    "#terms_index_hash.show()\n",
    "\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "#print(num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|paperID|term_frequency_sparse|\n",
      "+-------+---------------------+\n",
      "| 159967| (82,[0,7,18,21,25...|\n",
      "|2212959| (82,[1,4,21,40,64...|\n",
      "| 333353| (82,[6,8,10,35,36...|\n",
      "| 438129| (82,[30,59],[1.0,...|\n",
      "| 166220| (82,[11,16,25,28,...|\n",
      "|2883810| (82,[8,51,80],[1....|\n",
      "|1288940| (82,[2,4,13,19,44...|\n",
      "|5251453| (82,[0,6,13,14,19...|\n",
      "|7515828| (82,[5,8,51,80],[...|\n",
      "|2739852| (82,[0,2,3,17,21,...|\n",
      "|5961524| (82,[10,20,46,51,...|\n",
      "|    272| (82,[9,12,13,24,3...|\n",
      "|6573750| (82,[9,23],[1.0,1...|\n",
      "|  77265| (82,[6,10,20,67],...|\n",
      "| 820297| (82,[15,43,57,68,...|\n",
      "|1042553| (82,[7,24,39,41,4...|\n",
      "|2883820| (82,[22,35,74],[1...|\n",
      "|5434882| (82,[10,17,28,35,...|\n",
      "|1332540|      (82,[68],[1.0])|\n",
      "|1777140| (82,[0,1,7,18,32,...|\n",
      "+-------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\"))\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "# tf_df is now a df with a column of paperID and a column of lists of the terms (unexploded)\n",
    "#tf_df.show()\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "vector_df = model.transform(tf_df)\n",
    "vector_df = vector_df.select(\"paperID\", col(\"vectors\").alias(\"term_frequency_sparse\"))\n",
    "\n",
    "# Term frequency vector df\n",
    "vector_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## TF-IDF with built-in function ##########################################\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "#tf_idf_built_in.show()\n",
    "\n",
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "user_profile = tf_idf_built_in.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "user_profile = user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "user_profile = user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+-----------+\n",
      "|              userID| paperID|label|            features| Similarity|\n",
      "+--------------------+--------+-----+--------------------+-----------+\n",
      "|90f1a3e6fcdbf9bc5...|  956315|    0|[10.0,2006.0,5.0,...| 0.04896834|\n",
      "|589b870a611c25fa9...| 3728173|    0|[5.0,2007.0,5.0,1...|  0.0930714|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|    0|[12.0,2002.0,5.0,...|0.045270264|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|    0|[12.4,2002.0,5.0,...|0.021175466|\n",
      "|589b870a611c25fa9...|11733005|    0|[7.0,2012.0,5.0,1...| 0.08924326|\n",
      "|589b870a611c25fa9...| 8310458|    0|[15.0,2008.0,5.0,...| 0.17026126|\n",
      "|589b870a611c25fa9...|  115945|    0|[11.0,2005.0,5.0,...| 0.18160164|\n",
      "|589b870a611c25fa9...| 9045137|    0|[24.0,2007.0,5.0,...|0.118237026|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|    0|[12.4,2005.0,5.0,...|  0.0958606|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|    0|[29.0,2013.0,5.0,...| 0.18497433|\n",
      "+--------------------+--------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert dense vectors to sparse\n",
    "def to_sparse(x):        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(x)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in x if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "\n",
    "to_sparse_udf = udf(lambda x: to_sparse(x), VectorUDT())\n",
    "\n",
    "# Conver the feature vectors to sparse vectors\n",
    "user_profile = user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "#user_profile.show()\n",
    "\n",
    "# Join with the user profile with their unrated papers from unrated_df\n",
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "# df is now a dataframe with userID, paperID, user_profile, and paper_profile\n",
    "\n",
    "# df.show()\n",
    "\n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())\n",
    "\n",
    "sim_df = df.withColumn('Similarity', compute_sim(df.user_profile, df.paper_profile))\n",
    "sim_df = sim_df.drop(\"user_profile\", \"paper_profile\")\n",
    "\n",
    "preprocessed_df_joined = preprocessed_df.join(sim_df, [\"userID\", \"paperID\"])\n",
    "\n",
    "preprocessed_df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+\n",
      "|              userID| paperID|label|       featuresAdded|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|90f1a3e6fcdbf9bc5...|  956315|    0|[10.0,2006.0,5.0,...|\n",
      "|589b870a611c25fa9...| 3728173|    0|[5.0,2007.0,5.0,1...|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|    0|[12.0,2002.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|    0|[12.4,2002.0,5.0,...|\n",
      "|589b870a611c25fa9...|11733005|    0|[7.0,2012.0,5.0,1...|\n",
      "|589b870a611c25fa9...| 8310458|    0|[15.0,2008.0,5.0,...|\n",
      "|589b870a611c25fa9...|  115945|    0|[11.0,2005.0,5.0,...|\n",
      "|589b870a611c25fa9...| 9045137|    0|[24.0,2007.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|    0|[12.4,2005.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|    0|[29.0,2013.0,5.0,...|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"features\", \"Similarity\"], outputCol=\"featuresAdded\")\n",
    "\n",
    "preprocessed_df_joined = assembler.transform(preprocessed_df_joined)\n",
    "preprocessed_df_joined = preprocessed_df_joined.select(\"userID\", \"paperID\", \"label\",\"featuresAdded\")\n",
    "\n",
    "preprocessed_df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(preprocessed_df_joined, \"featuresAdded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest(preprocessed_df_joined, \"featuresAdded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
