{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-06\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark.sql.functions as f\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, DoubleType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import concat, col, lit, size\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "spark = SparkSession.builder.appName('ex6').getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unexplode a column to an array of given datatype\n",
    "def unExplode(df, groupByColName, collectColName, colType):\n",
    "    types = {'string': StringType(), 'integer': IntegerType()}\n",
    "    df_collected = df.groupby(groupByColName).agg(f.concat_ws(\", \", f.collect_list(df[collectColName])).alias(collectColName))\n",
    "    result = df_collected.withColumn(collectColName, split(col(collectColName), \",\\s*\").cast(ArrayType(types[colType])).alias(collectColName)).orderBy(groupByColName)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|              userID|             paperID|libSize|\n",
      "+--------------------+--------------------+-------+\n",
      "|589b870a611c25fa9...|[12832332, 130547...|      5|\n",
      "|90f1a3e6fcdbf9bc5...|[115945, 11733005...|      5|\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read user ratings into Dataframe\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./example.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "user_pre = user_df\n",
    "\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Count the number of distinct users\n",
    "numUsers = user_df.select(\"userID\").distinct().count()\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "userLib = user_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType()))).orderBy(\"userID\")\n",
    "\n",
    "get_len_udf = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "userLib = userLib.withColumn(\"libSize\", get_len_udf(col(\"paperID\")))\n",
    "\n",
    "userLib.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./paper0.csv')\n",
    "#w_df = w_df.select(\"_c0\", \"_c01\" \"_c06\", \"_c09\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(\"_c0\", \"_c1\", \"_c6\", \"_c9\")\n",
    "#w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c01\").alias(\"type\"), col(\"_c06\").alias(\"pages\"), col(\"_c09\").alias(\"year\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c1\").alias(\"type\"), col(\"_c6\").alias(\"pages\"), col(\"_c9\").alias(\"year\"))\n",
    "\n",
    "#w_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6. 1 (Problem Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----+----+-----------+-------+--------+\n",
      "|              userID| paperID|rating|pages|year|       type|libSize|avgPages|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+\n",
      "|589b870a611c25fa9...|11733005|     0|    7|2012|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 9045137|     0|   24|2007|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...|  115945|     0|   11|2005|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 8310458|     0|   15|2008|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 3728173|     0|    5|2007|    article|      5|    17.0|\n",
      "|589b870a611c25fa9...| 1001231|     1| 17.0|2005|proceedings|      5|    17.0|\n",
      "|589b870a611c25fa9...|12832332|     1|   29|2013|     inbook|      5|    17.0|\n",
      "|589b870a611c25fa9...|  352713|     1|   12|2002|       misc|      5|    17.0|\n",
      "|589b870a611c25fa9...| 1305474|     1| 17.0|2002|       misc|      5|    17.0|\n",
      "|589b870a611c25fa9...|  956315|     1|   10|2006|    article|      5|    17.0|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|     0|   29|2013|     inbook|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|     0| 12.4|2002|       misc|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|  956315|     0|   10|2006|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|     0|   12|2002|       misc|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|     0| 12.4|2005|proceedings|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|     1|   11|2005|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|     1|   24|2007|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|     1|    7|2012|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 3728173|     1|    5|2007|    article|      5|    12.4|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|     1|   15|2008|    article|      5|    12.4|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a user hash map dataframe by adding a column with descending row numbers \n",
    "# to the pre-explode user dataframe\n",
    "# user_hash_map = user_pre.withColumn(\"user_idx\", F.monotonically_increasing_id().cast(IntegerType()))\n",
    "\n",
    "# Select the user indices and paperID\n",
    "#rating = user_hash_map.select('user_idx', 'paperID')\n",
    "rating = userLib.select('userID', 'paperID', 'libSize')\n",
    "\n",
    "# # Split the string if all paperIDs into individual paperIDs and cast the datatype to int\n",
    "# rating = rating.withColumn(\"papers\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"papers\"))\n",
    "# # Drop the column with the string of paperIDs\n",
    "# rating = rating.drop(\"paperID\")\n",
    "\n",
    "# Add a column of 1's called ratings for the rated papers w.r.t to the user index\n",
    "rating = rating.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))\n",
    "\n",
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "\n",
    "    # Select n random papers from the unrated papers where n is the \n",
    "    # length of the rated papers list of each user\n",
    "    unrated_papers = random.sample(unrated, len(papers))\n",
    "            \n",
    "    return unrated_papers\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "rating = rating.withColumn(\"unrated\", get_unrated(rating.paperID))\n",
    "\n",
    "#Create a dataframe with the rated papers and their ratings of 1's\n",
    "rated_df = rating.select(\"userID\", f.explode(\"paperID\").alias(\"paperID\"), \"rating\", \"libSize\")\n",
    "\n",
    "# Create a dataframe with the unrated papers\n",
    "unrated_df = rating.select(\"userID\", f.explode(\"unrated\").alias(\"unrated\"), \"libSize\")\n",
    "# Add a column of ratings of 0's to the unrated papers\n",
    "unrated_df = unrated_df.withColumn(\"rating\", lit(0))\n",
    "unrated_df = unrated_df.select(\"userID\", col(\"unrated\").alias(\"paperID\"), \"rating\", \"libSize\")\n",
    "\n",
    "# Union the rated and unrated dataframes and order by the user indices\n",
    "rating_matrix = rated_df.union(unrated_df).orderBy(\"userID\")\n",
    "\n",
    "feature_df = rating_matrix.join(w_df, [\"paperID\"]).orderBy(\"userID\")\n",
    "\n",
    "window = Window.partitionBy(col(\"userID\"))\n",
    "\n",
    "feature_df_rated = feature_df.filter(feature_df.rating == 1).groupBy(\"userID\", \"paperID\", \"rating\", \"pages\", \"year\", \"type\", \"libSize\").agg(f.avg(\"pages\").over(window).alias(\"avgPages\")).orderBy(\"userID\")\n",
    "\n",
    "feature_df_unrated = feature_df.filter(feature_df.rating == 0)\n",
    "feature_df_unrated = feature_df_unrated.withColumn('avgPages', lit(\" \").cast(StringType()))\n",
    "feature_df_unrated = feature_df_unrated.select(\"userID\", \"paperID\", \"rating\", \"pages\", \"year\", \"type\", \"libSize\", \"avgPages\")\n",
    "\n",
    "y = feature_df_rated.select('userID', 'avgPages').distinct()\n",
    "\n",
    "feature_df_unrated =  feature_df_unrated.drop(\"avgPages\")\n",
    "\n",
    "x = feature_df_unrated.join(y, 'userID')\n",
    "\n",
    "\n",
    "model_df = feature_df_rated.union(x).orderBy(\"userID\", \"rating\")\n",
    "model_df1 = model_df.withColumn(\"pages\", coalesce(model_df.pages, model_df.avgPages))\n",
    "\n",
    "model_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----+----+-----------+-------+--------+---------+-------------+\n",
      "|              userID| paperID|rating|pages|year|       type|libSize|avgPages|typeIndex|  encodedType|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+---------+-------------+\n",
      "|589b870a611c25fa9...| 8310458|     0|   15|2008|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...| 9045137|     0|   24|2007|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...| 3728173|     0|    5|2007|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...|  115945|     0|   11|2005|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...|11733005|     0|    7|2012|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...| 1001231|     1| 17.0|2005|proceedings|      5|    17.0|      2.0|(3,[2],[1.0])|\n",
      "|589b870a611c25fa9...|  352713|     1|   12|2002|       misc|      5|    17.0|      1.0|(3,[1],[1.0])|\n",
      "|589b870a611c25fa9...|  956315|     1|   10|2006|    article|      5|    17.0|      0.0|(3,[0],[1.0])|\n",
      "|589b870a611c25fa9...|12832332|     1|   29|2013|     inbook|      5|    17.0|      3.0|    (3,[],[])|\n",
      "|589b870a611c25fa9...| 1305474|     1| 17.0|2002|       misc|      5|    17.0|      1.0|(3,[1],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|     0| 12.4|2005|proceedings|      5|    12.4|      2.0|(3,[2],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|     0| 12.4|2002|       misc|      5|    12.4|      1.0|(3,[1],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|  956315|     0|   10|2006|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|     0|   12|2002|       misc|      5|    12.4|      1.0|(3,[1],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|     0|   29|2013|     inbook|      5|    12.4|      3.0|    (3,[],[])|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|     1|   11|2005|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|     1|    7|2012|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|     1|   24|2007|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 3728173|     1|    5|2007|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|     1|   15|2008|    article|      5|    12.4|      0.0|(3,[0],[1.0])|\n",
      "+--------------------+--------+------+-----+----+-----------+-------+--------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
    "model = stringIndexer.fit(model_df1)\n",
    "indexed = model.transform(model_df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"typeIndex\", outputCol=\"encodedType\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df1 = encoded.withColumn(\"pages\", encoded[\"pages\"].cast(DoubleType()))\n",
    "model_df1 = model_df1.withColumn(\"year\", model_df1[\"year\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+\n",
      "|              userID| paperID|label|            features|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|589b870a611c25fa9...| 8310458|    0|[15.0,2008.0,5.0,...|\n",
      "|589b870a611c25fa9...|11733005|    0|[7.0,2012.0,5.0,1...|\n",
      "|589b870a611c25fa9...| 3728173|    0|[5.0,2007.0,5.0,1...|\n",
      "|589b870a611c25fa9...| 9045137|    0|[24.0,2007.0,5.0,...|\n",
      "|589b870a611c25fa9...|  115945|    0|[11.0,2005.0,5.0,...|\n",
      "|589b870a611c25fa9...|12832332|    1|[29.0,2013.0,5.0,...|\n",
      "|589b870a611c25fa9...| 1305474|    1|[17.0,2002.0,5.0,...|\n",
      "|589b870a611c25fa9...| 1001231|    1|[17.0,2005.0,5.0,...|\n",
      "|589b870a611c25fa9...|  352713|    1|[12.0,2002.0,5.0,...|\n",
      "|589b870a611c25fa9...|  956315|    1|[10.0,2006.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|  956315|    0|[10.0,2006.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 1305474|    0|[12.4,2002.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 1001231|    0|[12.4,2005.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|12832332|    0|[29.0,2013.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|  352713|    0|[12.0,2002.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|    1|[11.0,2005.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|    1|[7.0,2012.0,5.0,1...|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|    1|[24.0,2007.0,5.0,...|\n",
      "|90f1a3e6fcdbf9bc5...| 3728173|    1|[5.0,2007.0,5.0,1...|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|    1|[15.0,2008.0,5.0,...|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"pages\", \"year\", \"libSize\", \"avgPages\", \"encodedType\"], outputCol=\"features\")\n",
    "\n",
    "preprocessed_df = assembler.transform(model_df1)\n",
    "preprocessed_df = preprocessed_df.select(\"userID\", \"paperID\", col(\"rating\").alias(\"label\"),\"features\")\n",
    "\n",
    "preprocessed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6. 3 (Classificatoin Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         0.0|[7.0,2012.0,5.0,1...|\n",
      "|       0.0|         0.0|[12.4,2005.0,5.0,...|\n",
      "|       0.0|         1.0|[5.0,2007.0,5.0,1...|\n",
      "+----------+------------+--------------------+\n",
      "\n",
      "Root-mean-square error = 0.8164965809277261\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(preprocessed_df)\n",
    "\n",
    "#Automatically identify categorical features, and index them.\n",
    "#We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(preprocessed_df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = preprocessed_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show()\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator =  RegressionEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             1|    0|[24.0,2007.0,5.0,...|\n",
      "|             0|    1|[10.0,2006.0,5.0,...|\n",
      "|             1|    0|[29.0,2013.0,5.0,...|\n",
      "+--------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(preprocessed_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(preprocessed_df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = preprocessed_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show()\n",
    "\n",
    "# # Select (prediction, true label) and compute test error\n",
    "# evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "# rfModel = model.stages[2]\n",
    "# print(rfModel)  # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
