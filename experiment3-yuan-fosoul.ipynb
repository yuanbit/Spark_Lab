{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-03\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We reduced our dataset for both the user_libraries and papers due to out of memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example2.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|               words|\n",
      "+-------+--------------------+\n",
      "|  80546|the arbitrariness...|\n",
      "|5842862|how to choose a g...|\n",
      "|1242600|how to write cons...|\n",
      "|3467077|defrosting the di...|\n",
      "| 309395|why most publishe...|\n",
      "| 305755|the structure of ...|\n",
      "|6603134|how to build a mo...|\n",
      "|     99|collective dynami...|\n",
      "| 105595|linked: how every...|\n",
      "| 212874|gene ontology: to...|\n",
      "| 740681|usage patterns of...|\n",
      "|    101|network motifs: s...|\n",
      "|  99857|the strength of w...|\n",
      "|3614773|{rna}-seq: a revo...|\n",
      "| 873540|pattern recogniti...|\n",
      "|6434100|a quick guide for...|\n",
      "| 100088|basic local align...|\n",
      "|1387765|power-law distrib...|\n",
      "| 161814|the elements of s...|\n",
      "| 117535|maximum likelihoo...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./example_paper.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "w_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 1 Vector representation for the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|    terms|count|\n",
      "+---------+-----+\n",
      "|   compar|   29|\n",
      "|   measur|   29|\n",
      "|   featur|   28|\n",
      "|  problem|   28|\n",
      "|   requir|   28|\n",
      "|     cell|   27|\n",
      "|  pattern|   27|\n",
      "|technolog|   27|\n",
      "|      web|   27|\n",
      "|      map|   26|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('stemmed', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"stemmed\")\n",
    "\n",
    "# Create a new df to store the paperID and stemmed tokens\n",
    "paper_terms = df\n",
    "\n",
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "df = df.select(\"paperID\", f.explode(\"stemmed\").alias(\"tokens\")).distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "df = df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "important_words.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| âˆ’ 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of distinct terms\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "# Split (explode) the list of words into a column of tokens and\n",
    "# count the number of occurences of the tokens per paper\n",
    "# p_terms is a df with paperID, terms, and count\n",
    "# p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\")).groupBy(\"paperID\", \"terms\").count()\n",
    "# p_terms = p_terms.orderBy(\"paperID\", \"count\")\n",
    "\n",
    "# # Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "# joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "# # Drop the terms because now they are represented by the indices\n",
    "# joined_df = joined_df.drop(\"terms\")\n",
    "# joined_df  = joined_df.orderBy(\"paperID\")\n",
    "\n",
    "# # Create a new df to compute the term frequency vectors\n",
    "# tf_df = joined_df\n",
    "\n",
    "# # Concatenate the indices and the count (occurences in papers)\n",
    "# tf_df = tf_df.withColumn(\"index_count\", f.concat(col(\"index\"), lit(\", \"), col(\"count\")))\n",
    "# tf_df = tf_df.drop(\"index\", \"count\")\n",
    "\n",
    "# # Concatenate the index_counts per paperID (\"un-explode\")\n",
    "# tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.index_count)).alias(\"index_count\"))\n",
    "\n",
    "# tf_df.show()\n",
    "# # Create a new column and casting the index_count into an array with integer type\n",
    "# # The terms_count of the tf_df column is now a list where the odd positions are the terms indices and the even positions\n",
    "# tf_df = tf_df.withColumn(\"terms_count\", split(col(\"index_count\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"terms_count\"))\n",
    "# tf_df = tf_df.drop(\"index_count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|              terms_|\n",
      "+-------+--------------------+\n",
      "| 159967|[scale, predict, ...|\n",
      "|2212959|[map, cluster, cl...|\n",
      "| 333353|[pattern, current...|\n",
      "| 438129|  [collect, resourc]|\n",
      "| 166220|[challeng, curren...|\n",
      "|2883810|     [offer, featur]|\n",
      "|1288940|[web, web, web, w...|\n",
      "|5251453|[pattern, effect,...|\n",
      "|7515828|[dna, offer, featur]|\n",
      "|2739852|[web, emerg, scal...|\n",
      "|5961524|[offer, addit, ev...|\n",
      "|    272|[current, dynam, ...|\n",
      "|6573750|       [cell, genet]|\n",
      "|  77265|   [pattern, evolut]|\n",
      "| 820297|[simpl, perform, ...|\n",
      "|1042553|[power, power, po...|\n",
      "|2883820|[technolog, curre...|\n",
      "|5434882|[current, emerg, ...|\n",
      "|1332540|             [simpl]|\n",
      "|1777140|[map, challeng, p...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\"))\n",
    "#.groupBy(\"paperID\", \"terms\").count()\n",
    "#p_terms = p_terms.orderBy(\"paperID\", \"count\")\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "# Drop the terms because now they are represented by the indices\n",
    "#joined_df = joined_df.drop(\"terms\")\n",
    "#joined_df  = joined_df.orderBy(\"paperID\")\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "tf_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paperID|terms_                                                                                                                                                                                                                               |vectors                                                                                                                            |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|159967 |[scale, predict, protein, protein, protein, protein, protein, protein, protein, protein, protein, protein, protein, protein, similar, cluster, cluster, cluster, detect, detect, detect, detect, call, test, effici, compar, problem]|(64,[0,5,15,23,25,35,38,39,44,52,55],[12.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0])                                               |\n",
      "|2212959|[map, cluster, cluster, cluster, effici]                                                                                                                                                                                             |(64,[1,15,55],[1.0,3.0,1.0])                                                                                                       |\n",
      "|333353 |[pattern, current, improv, suggest, offer, perform, evolut, evolut, evolut, evolut, evolut, highli, properti, featur, requir]                                                                                                        |(64,[3,4,13,28,36,37,40,41,43,46,61],[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                |\n",
      "|438129 |[collect, resourc]                                                                                                                                                                                                                   |(64,[21,48],[1.0,1.0])                                                                                                             |\n",
      "|166220 |[challeng, current, softwar, softwar, softwar, work, scientif, collabor, collabor, compar]                                                                                                                                           |(64,[8,12,18,23,28,31,49],[2.0,3.0,1.0,1.0,1.0,1.0,1.0])                                                                           |\n",
      "|2883810|[offer, featur]                                                                                                                                                                                                                      |(64,[4,46],[1.0,1.0])                                                                                                              |\n",
      "|1288940|[web, web, web, web, web, improv, paper, similar, experiment, measur]                                                                                                                                                                |(64,[2,10,32,39,40,59],[5.0,1.0,1.0,1.0,1.0,1.0])                                                                                  |\n",
      "|5251453|[pattern, effect, power, paper, paper, paper, paper, paper, protein, theori, theori, scientif, support, specif, specif, measur]                                                                                                      |(64,[0,3,10,11,17,18,32,34,53,63],[1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,2.0])                                                       |\n",
      "|7515828|[dna, offer, featur]                                                                                                                                                                                                                 |(64,[4,6,46],[1.0,1.0,1.0])                                                                                                        |\n",
      "|2739852|[web, emerg, scale, link, predict, predict, multipl, protein, connect, suggest, suggest, offer, cluster, short, principl, distribut, genet, group, group, group, group, found, properti]                                             |(64,[0,2,9,14,15,19,22,26,30,33,35,41,43,44,46,47,54,62],[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|5961524|[offer, addit, evolut, evolut, evolut, evolut, evolut, highli, experiment]                                                                                                                                                           |(64,[13,46,57,59,61],[5.0,1.0,1.0,1.0,1.0])                                                                                        |\n",
      "|272    |[current, dynam, dynam, dynam, scale, power, multipl, multipl, wide, short, call, highli, measur, cell]                                                                                                                              |(64,[7,10,17,20,26,28,33,44,52,60,61],[3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])                                               |\n",
      "|6573750|[cell, genet]                                                                                                                                                                                                                        |(64,[20,22],[1.0,1.0])                                                                                                             |\n",
      "|77265  |[pattern, evolut]                                                                                                                                                                                                                    |(64,[3,13],[1.0,1.0])                                                                                                              |\n",
      "|820297 |[simpl, perform, learn]                                                                                                                                                                                                              |(64,[27,36,56],[1.0,1.0,1.0])                                                                                                      |\n",
      "|1042553|[power, power, power, multipl, multipl, multipl, simpl, signific, call, call, test, test, test, problem, problem, problem]                                                                                                           |(64,[5,17,33,38,45,52,56],[3.0,3.0,3.0,3.0,1.0,2.0,1.0])                                                                           |\n",
      "|2883820|[technolog, current, current, experiment]                                                                                                                                                                                            |(64,[16,28,59],[1.0,2.0,1.0])                                                                                                      |\n",
      "|5434882|[current, emerg, link, work, scientif, scientif, scientif, scientif, evolut, resourc]                                                                                                                                                |(64,[13,14,18,28,47,48,49],[1.0,1.0,4.0,1.0,1.0,1.0,1.0])                                                                          |\n",
      "|1332540|[simpl]                                                                                                                                                                                                                              |(64,[56],[1.0])                                                                                                                    |\n",
      "|1777140|[map, challeng, protein, protein, detect, problem]                                                                                                                                                                                   |(64,[0,1,5,25,31],[2.0,1.0,1.0,1.0,1.0])                                                                                           |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "# Get the number of distinct terms\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "x = model.transform(tf_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paperID|term_frequency_sparse                                                                                                               |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|159967 |(64,[6,16,17,21,25,26,41,43,47,60,62],[1.0,4.0,1.0,1.0,1.0,3.0,12.0,1.0,1.0,1.0,1.0])                                               |\n",
      "|2212959|(64,[6,26,48],[1.0,3.0,1.0])                                                                                                        |\n",
      "|333353 |(64,[2,7,10,19,24,33,46,50,56,59,61],[5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |\n",
      "|438129 |(64,[4,30],[1.0,1.0])                                                                                                               |\n",
      "|166220 |(64,[20,29,37,38,49,50,62],[2.0,1.0,1.0,3.0,1.0,1.0,1.0])                                                                           |\n",
      "|2883810|(64,[24,59],[1.0,1.0])                                                                                                              |\n",
      "|1288940|(64,[8,25,39,46,58,63],[1.0,1.0,1.0,1.0,5.0,1.0])                                                                                   |\n",
      "|5251453|(64,[5,15,22,29,39,41,44,54,56,63],[2.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0])                                                       |\n",
      "|7515828|(64,[24,34,59],[1.0,1.0,1.0])                                                                                                       |\n",
      "|2739852|(64,[3,9,10,12,14,18,24,26,28,33,40,41,42,43,45,47,51,58],[4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])|\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# udf to get the sparse vector of term frequency\n",
    "def vector_list_map(x):\n",
    "    # a list with size of number of terms and fill it with zeroes \n",
    "\n",
    "    mylist = [0] * num_terms\n",
    "    for i in range(0, len(x), 2):\n",
    "        mylist[int(x[i])] = x[i+1]\n",
    "        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(mylist)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in mylist if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "           \n",
    "vector_map = udf(lambda x: vector_list_map(x), VectorUDT())\n",
    "\n",
    "# applying the udf to the terms_count column to create the term_frequency_sparse column\n",
    "vector_df = tf_df.withColumn('term_frequency_sparse', vector_map(col('terms_count')))\n",
    "\n",
    "vector_df = vector_df.drop(\"terms_count\")\n",
    "\n",
    "vector_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 2 (TF-IDF representation for the papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|            features|\n",
      "+-------+--------------------+\n",
      "| 159967|(64,[6,16,17,21,2...|\n",
      "|2212959|(64,[6,26,48],[2....|\n",
      "| 333353|(64,[2,7,10,19,24...|\n",
      "| 438129|(64,[4,30],[2.542...|\n",
      "| 166220|(64,[20,29,37,38,...|\n",
      "|2883810|(64,[24,59],[2.45...|\n",
      "|1288940|(64,[8,25,39,46,5...|\n",
      "|5251453|(64,[5,15,22,29,3...|\n",
      "|7515828|(64,[24,34,59],[2...|\n",
      "|2739852|(64,[3,9,10,12,14...|\n",
      "|5961524|(64,[1,2,7,8,24],...|\n",
      "|    272|(64,[7,17,23,28,4...|\n",
      "|6573750|(64,[18,55],[2.49...|\n",
      "|  77265|(64,[2,56],[2.542...|\n",
      "| 820297|(64,[11,19,35],[2...|\n",
      "|1042553|(64,[17,21,27,35,...|\n",
      "|2883820|(64,[8,50,57],[2....|\n",
      "|5434882|(64,[2,4,29,37,45...|\n",
      "|1332540|(64,[35],[2.40919...|\n",
      "|1777140|(64,[16,41,48,49,...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TF-IDF with built-in function ###\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "tf_idf_built_in.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a udf to compute the idf without using the built-in functions\n",
    "def idf(x):\n",
    "    return math.log((num_papers + 1)/(x+1))\n",
    "\n",
    "idf_udf = udf(lambda x: idf(x), DoubleType())\n",
    "\n",
    "# apply the udf to the dataframe\n",
    "df = df.withColumn('idf', idf_udf(col('count')))\n",
    "\n",
    "# select the term-index and the idf\n",
    "idf_df = df.select(col(\"row_num\").alias(\"index\"), \"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paperID|tf-idf_sparse                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|159967 |(64,[6,16,17,21,25,26,41,43,47,60,62],[2.662587827025453,10.464271245562239,2.6160678113905598,2.6160678113905598,2.571616048819726,7.714848146459179,29.8588132785681,2.4490137267273937,2.4490137267273937,2.3398144347624017,2.3059128830867204])                                                                                                                                                   |\n",
      "|2212959|(64,[6,26,48],[2.662587827025453,7.714848146459179,2.4112733987445467])                                                                                                                                                                                                                                                                                                                                |\n",
      "|333353 |(64,[2,7,10,19,24,33,46,50,56,59,61],[13.312939135127264,2.662587827025453,2.662587827025453,2.6160678113905598,2.571616048819726,2.52905643440093,2.4490137267273937,2.4112733987445467,2.3749057545736716,2.3398144347624017,2.3398144347624017])                                                                                                                                                    |\n",
      "|438129 |(64,[4,30],[2.662587827025453,2.571616048819726])                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|166220 |(64,[20,29,37,38,49,50,62],[5.2321356227811195,2.571616048819726,2.52905643440093,7.464703319642025,2.4112733987445467,2.4112733987445467,2.3059128830867204])                                                                                                                                                                                                                                         |\n",
      "|2883810|(64,[24,59],[2.571616048819726,2.3398144347624017])                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|1288940|(64,[8,25,39,46,58,63],[2.662587827025453,2.571616048819726,2.488234439880675,2.4490137267273937,11.874528772868358,2.3059128830867204])                                                                                                                                                                                                                                                               |\n",
      "|5251453|(64,[5,15,22,29,39,41,44,54,56,63],[5.325175654050906,2.6160678113905598,5.143232097639452,2.571616048819726,12.441172199403374,2.488234439880675,2.4490137267273937,2.4112733987445467,2.3749057545736716,2.3059128830867204])                                                                                                                                                                        |\n",
      "|7515828|(64,[24,34,59],[2.571616048819726,2.52905643440093,2.3398144347624017])                                                                                                                                                                                                                                                                                                                                |\n",
      "|2739852|(64,[3,9,10,12,14,18,24,26,28,33,40,41,42,43,45,47,51,58],[10.650351308101811,2.662587827025453,2.662587827025453,2.6160678113905598,2.6160678113905598,2.6160678113905598,2.571616048819726,2.571616048819726,2.571616048819726,5.05811286880186,2.488234439880675,2.488234439880675,2.488234439880675,2.4490137267273937,2.4490137267273937,4.898027453454787,2.4112733987445467,2.3749057545736716])|\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### TF-IDF without built-in function ####\n",
    "\n",
    "# join dataframes to get the count of the terms for each paper\n",
    "tf_idf_df = joined_df.join(idf_df, [\"index\"]).select(\"index\", \"paperID\", (col(\"count\") * col(\"idf\")).alias(\"tf-idf\"))\n",
    "\n",
    "# concatenate the index and the tf-idf columns \n",
    "tf_idf_df = tf_idf_df.withColumn(\"index_tf_idf\", f.concat(col(\"index\"), lit(\", \"), col(\"tf-idf\")))\n",
    "tf_idf_df = tf_idf_df.drop(\"index\", \"tf-idf\")\n",
    "\n",
    "# Concatenate the index_tf-idf per paperID \"un-explode\"\n",
    "tf_idf_df = tf_idf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_idf_df.index_tf_idf)).alias(\"index_tf_idf\"))\n",
    "\n",
    "# cast the index_tf-idf into the array of doubles\n",
    "tf_idf_df = tf_idf_df.withColumn(\"tf-idf\", split(col(\"index_tf_idf\"), \",\\s*\").cast(ArrayType(DoubleType())).alias(\"terms_tf-idf\"))\n",
    "tf_idf_df = tf_idf_df.drop(\"index_tf_idf\")\n",
    "\n",
    "#tf_idf_df.show(truncate=False)\n",
    "\n",
    "# compute the sparse vector for tf-idf\n",
    "tf_idf_vector = tf_idf_df.withColumn('tf-idf_sparse', vector_map(col('tf-idf')))\n",
    "\n",
    "tf_idf_vector = tf_idf_vector.drop(\"tf-idf\")\n",
    "\n",
    "tf_idf_vector.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 3 (Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userId|            features|\n",
      "+--------------------+--------------------+\n",
      "|ffa86c7343f5ce8a7...|[0.0,0.0,0.0,0.0,...|\n",
      "|1006f001bb7ea1023...|[0.0,0.0,0.0,0.0,...|\n",
      "|42b59f0ee03fa51f3...|[0.0,0.0,0.0,2.54...|\n",
      "|6a96998090c72b724...|[0.0,0.0,2.542726...|\n",
      "|6d35c61471e1803bc...|[0.0,0.0,2.542726...|\n",
      "|b953afb03918cb8fa...|[0.0,0.0,7.628178...|\n",
      "|d85f7d83f27b3f533...|[2.54272622067682...|\n",
      "|a06e5e3519a8d90a8...|[5.08545244135365...|\n",
      "|5fd7b7de422c5b11d...|[5.08545244135365...|\n",
      "|f1e1cd4ff25018273...|[0.0,0.0,0.0,2.54...|\n",
      "|b656009a6efdc8b1a...|[2.54272622067682...|\n",
      "|3df36dc3cdc7e9086...|[0.0,0.0,0.0,0.0,...|\n",
      "|32930342b4a8ca975...|[0.0,0.0,0.0,0.0,...|\n",
      "|0530b8ed834fa63f2...|[0.0,0.0,0.0,0.0,...|\n",
      "|7d378131aeffcab66...|[0.0,0.0,2.542726...|\n",
      "|bd8345b325c5cf7a9...|[0.0,0.0,0.0,0.0,...|\n",
      "|b36c3189bb1457cd0...|[0.0,0.0,0.0,0.0,...|\n",
      "|bbcd9dae3160ddcb9...|[0.0,0.0,0.0,7.62...|\n",
      "|ce250fe59a9a7d698...|[10.1709048827073...|\n",
      "|5c7cfd33003a6a2c3...|[0.0,0.0,7.628178...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "user_profile = tf_idf_built_in.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "user_profile = user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "user_profile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 3 (Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|              userId|            features|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|ffa86c7343f5ce8a7...|[0.0,0.0,0.0,0.0,...|        35|\n",
      "|1006f001bb7ea1023...|[0.0,0.0,0.0,0.0,...|        29|\n",
      "|42b59f0ee03fa51f3...|[0.0,0.0,0.0,2.54...|        45|\n",
      "|6a96998090c72b724...|[0.0,0.0,2.542726...|        28|\n",
      "|6d35c61471e1803bc...|[0.0,0.0,2.542726...|        13|\n",
      "|b953afb03918cb8fa...|[0.0,0.0,7.628178...|        16|\n",
      "|d85f7d83f27b3f533...|[2.54272622067682...|        41|\n",
      "|a06e5e3519a8d90a8...|[5.08545244135365...|         1|\n",
      "|5fd7b7de422c5b11d...|[5.08545244135365...|        11|\n",
      "|f1e1cd4ff25018273...|[0.0,0.0,0.0,2.54...|        46|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### 50 Clusters #####\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans_50 = KMeans().setK(50).setSeed(1)\n",
    "model_50 = kmeans_50.fit(user_profile)\n",
    "\n",
    "# Make predictions\n",
    "predictions_50 = model_50.transform(user_profile)\n",
    "predictions_50.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00289257345792\n"
     ]
    }
   ],
   "source": [
    "### DBI for 50 Clusters ###\n",
    "\n",
    "# compute the centroid of the clusters\n",
    "centers_50 = model_50.clusterCenters()\n",
    "\n",
    "# add the column of centroids of the respective clusters of the features\n",
    "user_profile_50 = predictions_50.rdd.map(lambda profile: (profile[0], profile[1], profile[2], DenseVector(centers_50[profile[2]]))).toDF(['userID', 'features', 'prediction', 'center'])\n",
    "\n",
    "# compute the Euclidean distance of the users' features from their centroid\n",
    "user_profile_50 = user_profile_50.rdd.map(lambda x: (x[2], x[1], float(distance.euclidean(x[1], x[3])))).toDF(['cluster', 'features', 'distance'])\n",
    "\n",
    "# compute the average distance\n",
    "average_50 = user_profile_50.select('*').groupBy('cluster').agg(mean('distance'))\n",
    "#average_50.show()\n",
    "\n",
    "# convert average distances to a list\n",
    "variances_50 = list(average_50.select('avg(distance)').toPandas()['avg(distance)'])\n",
    "\n",
    "# a function to compute the Davies-Bouldin-Index\n",
    "def dbi(variances, centers, num_cluster):\n",
    "    db = []\n",
    "    max_result = []\n",
    "    \n",
    "    for i in range(0, num_cluster):\n",
    "        for j in range(i+1, num_cluster):\n",
    "            db.append((variances[i] + variances[j]) / distance.euclidean(centers[i], centers[j]))\n",
    "        if len(db):\n",
    "            # pick the maximum of the similarity measures for each cluster\n",
    "            max_result.append(np.max(db))\n",
    "        db = []\n",
    "\n",
    "    result = np.sum(max_result) / num_cluster\n",
    "    return result\n",
    "\n",
    "print(dbi(variances_50, centers_50, 50))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|cluster|     avg(distance)|\n",
      "+-------+------------------+\n",
      "|      0|18.012801404202207|\n",
      "|      7|               0.0|\n",
      "|      6|               0.0|\n",
      "|      9|12.680302167100209|\n",
      "|      5| 8.953256295372702|\n",
      "|      1| 16.65302558839553|\n",
      "|      3|  29.4149877077502|\n",
      "|      8|13.069494169768761|\n",
      "|      2|               0.0|\n",
      "|      4|               0.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 10 Clusters ###\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans_10 = KMeans().setK(10).setSeed(1)\n",
    "model_10 = kmeans_10.fit(user_profile)\n",
    "\n",
    "# Make predictions\n",
    "predictions_10 = model_10.transform(user_profile)\n",
    "\n",
    "centers_10 = model_10.clusterCenters()\n",
    "\n",
    "user_profile_10 = predictions_10.rdd.map(lambda profile: (profile[0], profile[1], profile[2], DenseVector(centers_10[profile[2]]))).toDF(['userID', 'features', 'prediction', 'center'])\n",
    "user_profile_10 = user_profile_10.rdd.map(lambda x: (x[2], x[1], float(distance.euclidean(x[1], x[3])))).toDF(['cluster', 'features', 'distance'])\n",
    "average_10 = user_profile_10.select('*').groupBy('cluster').agg(mean('distance'))\n",
    "average_10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0778643099259\n"
     ]
    }
   ],
   "source": [
    "### DBI for 10 Clusters ###\n",
    "\n",
    "variances_10 = list(average_10.select('avg(distance)').toPandas()['avg(distance)'])\n",
    "\n",
    "print(dbi(variances_10, centers_10, 10))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case 50 clusters is better because it has a lower DBI of  0.00289257345792 compared to 0.0778643099259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 4 Latent Direchlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|paperId|            features|   topicDistribution|\n",
      "+-------+--------------------+--------------------+\n",
      "| 159967|(64,[6,16,17,21,2...|[8.62388518649721...|\n",
      "|2212959|(64,[6,26,48],[1....|[0.00403231936109...|\n",
      "| 333353|(64,[2,7,10,19,24...|[0.00150978031081...|\n",
      "| 438129|(64,[4,30],[1.0,1...|[0.00808468265300...|\n",
      "| 166220|(64,[20,29,37,38,...|[0.00219697108991...|\n",
      "|2883810|(64,[24,59],[1.0,...|[0.00808468265300...|\n",
      "|1288940|(64,[8,25,39,46,5...|[0.00219697108991...|\n",
      "|5251453|(64,[5,15,22,29,3...|[0.00142089211694...|\n",
      "|7515828|(64,[24,34,59],[1...|[0.00605598618653...|\n",
      "|2739852|(64,[3,9,10,12,14...|[0.00100620885952...|\n",
      "|5961524|(64,[1,2,7,8,24],...|[0.00241699468525...|\n",
      "|    272|(64,[7,17,23,28,4...|[0.00161053199979...|\n",
      "|6573750|(64,[18,55],[1.0,...|[0.00808468265300...|\n",
      "|  77265|(64,[2,56],[1.0,1...|[0.00808468265300...|\n",
      "| 820297|(64,[11,19,35],[1...|[0.00605598618653...|\n",
      "|1042553|(64,[17,21,27,35,...|[0.00142089211694...|\n",
      "|2883820|(64,[8,50,57],[1....|[0.00484118373203...|\n",
      "|5434882|(64,[2,4,29,37,45...|[0.00219697108991...|\n",
      "|1332540|     (64,[35],[1.0])|[0.01215723970024...|\n",
      "|1777140|(64,[16,41,48,49,...|[0.00345505003453...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### LDA ###\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# just rename the column to use it with the built-in methods\n",
    "termFrequencyVector = vector_df.select('paperId', col('term_frequency_sparse').alias('features'))\n",
    "# Trains a LDA model\n",
    "# set k=40 to have 40 different topics\n",
    "lda = LDA(k=40)\n",
    "model = lda.fit(termFrequencyVector)\n",
    "\n",
    "# each topic is described by 5 terms\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "#topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# it shows the probabilty of each topic for each paper\n",
    "transformed = model.transform(termFrequencyVector)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userId|            features|\n",
      "+--------------------+--------------------+\n",
      "|ffa86c7343f5ce8a7...|[0.00302236622220...|\n",
      "|1006f001bb7ea1023...|[0.00201366349526...|\n",
      "|42b59f0ee03fa51f3...|[0.00808468265300...|\n",
      "|6a96998090c72b724...|[0.02135010739927...|\n",
      "|6d35c61471e1803bc...|[0.00172569205872...|\n",
      "|b953afb03918cb8fa...|[0.00293329210930...|\n",
      "|d85f7d83f27b3f533...|[0.00829623376657...|\n",
      "|a06e5e3519a8d90a8...|[0.09146511925821...|\n",
      "|5fd7b7de422c5b11d...|[0.04219386992193...|\n",
      "|f1e1cd4ff25018273...|[0.02427424171435...|\n",
      "|b656009a6efdc8b1a...|[0.06709806077504...|\n",
      "|3df36dc3cdc7e9086...|[0.01215723970024...|\n",
      "|32930342b4a8ca975...|[0.01215723970024...|\n",
      "|0530b8ed834fa63f2...|[0.00605598618653...|\n",
      "|7d378131aeffcab66...|[0.00172569205872...|\n",
      "|bd8345b325c5cf7a9...|[0.00201366349526...|\n",
      "|b36c3189bb1457cd0...|[0.01215723970024...|\n",
      "|bbcd9dae3160ddcb9...|[0.01921192528354...|\n",
      "|ce250fe59a9a7d698...|[0.06465767495926...|\n",
      "|5c7cfd33003a6a2c3...|[0.05165255079668...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "\n",
    "lda_user_profile = transformed.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "lda_user_profile = lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "lda_user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|              userId|            features|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|ffa86c7343f5ce8a7...|[0.00302236622220...|        35|\n",
      "|1006f001bb7ea1023...|[0.00201366349526...|        19|\n",
      "|42b59f0ee03fa51f3...|[0.00808468265300...|        47|\n",
      "|6a96998090c72b724...|[0.02135010739927...|        29|\n",
      "|6d35c61471e1803bc...|[0.00172569205872...|        13|\n",
      "|b953afb03918cb8fa...|[0.00293329210930...|         8|\n",
      "|d85f7d83f27b3f533...|[0.00829623376657...|         0|\n",
      "|a06e5e3519a8d90a8...|[0.09146511925821...|        11|\n",
      "|5fd7b7de422c5b11d...|[0.04219386992193...|        16|\n",
      "|f1e1cd4ff25018273...|[0.02427424171435...|         1|\n",
      "|b656009a6efdc8b1a...|[0.06709806077504...|         5|\n",
      "|3df36dc3cdc7e9086...|[0.01215723970024...|        46|\n",
      "|32930342b4a8ca975...|[0.01215723970024...|        41|\n",
      "|0530b8ed834fa63f2...|[0.00605598618653...|        25|\n",
      "|7d378131aeffcab66...|[0.00172569205872...|        13|\n",
      "|bd8345b325c5cf7a9...|[0.00201366349526...|        19|\n",
      "|b36c3189bb1457cd0...|[0.01215723970024...|        48|\n",
      "|bbcd9dae3160ddcb9...|[0.01921192528354...|        12|\n",
      "|ce250fe59a9a7d698...|[0.06465767495926...|        10|\n",
      "|5c7cfd33003a6a2c3...|[0.05165255079668...|         6|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans_lda = KMeans().setK(50).setSeed(1)\n",
    "model_lda = kmeans_lda.fit(lda_user_profile)\n",
    "\n",
    "# Make predictions\n",
    "predictions_lda = model_lda.transform(lda_user_profile)\n",
    "\n",
    "predictions_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|cluster|       avg(distance)|\n",
      "+-------+--------------------+\n",
      "|     29|                 0.0|\n",
      "|     26| 0.14103847428113697|\n",
      "|     19|8.338292235946308E-5|\n",
      "|      0|                 0.0|\n",
      "|     22|                 0.0|\n",
      "|      7|                 0.0|\n",
      "|     34|                 0.0|\n",
      "|     43|                 0.0|\n",
      "|     32|                 0.0|\n",
      "|     31|                 0.0|\n",
      "|     39|                 0.0|\n",
      "|     25| 0.24649043624563108|\n",
      "|      6|                 0.0|\n",
      "|      9|                 0.0|\n",
      "|     27|                 0.0|\n",
      "|     17|                 0.0|\n",
      "|     41|                 0.0|\n",
      "|     28|                 0.0|\n",
      "|     33| 0.13129754752687012|\n",
      "|      5|                 0.0|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DBI for LDA ###\n",
    "\n",
    "centers_lda = model_lda.clusterCenters()\n",
    "\n",
    "#print(len(centers_lda[0]))\n",
    "\n",
    "user_profile_lda = predictions_lda.rdd.map(lambda profile: (profile[0], profile[1], profile[2], DenseVector(centers_lda[profile[2]]))).toDF(['userID', 'features', 'prediction', 'center'])\n",
    "user_profile_lda = user_profile_lda.rdd.map(lambda x: (x[2], x[1], float(distance.euclidean(x[1], x[3])))).toDF(['cluster', 'features', 'distance'])\n",
    "average_lda = user_profile_lda.select('*').groupBy('cluster').agg(mean('distance'))\n",
    "average_lda.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00293010711337\n"
     ]
    }
   ],
   "source": [
    "### DBI for LDA Clusters ###\n",
    "\n",
    "variances_lda = list(average_lda.select('avg(distance)').toPandas()['avg(distance)'])\n",
    "\n",
    "print(dbi(variances_lda, centers_lda, 50))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .............\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /opt/conda:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    blinker:           1.4-py_1             conda-forge\n",
      "    boto:              2.49.0-py36_0        defaults   \n",
      "    boto3:             1.9.47-py_0          conda-forge\n",
      "    botocore:          1.12.48-py_0         conda-forge\n",
      "    bz2file:           0.98-py_0            conda-forge\n",
      "    docutils:          0.14-py36_1001       conda-forge\n",
      "    gensim:            3.5.0-py36_0         conda-forge\n",
      "    jmespath:          0.9.3-py_1           conda-forge\n",
      "    nltk:              3.2.5-py_0           conda-forge\n",
      "    oauthlib:          2.1.0-py_0           conda-forge\n",
      "    pyjwt:             1.6.4-py_0           conda-forge\n",
      "    python-crfsuite:   0.9.6-py36h2d50403_0 conda-forge\n",
      "    requests-oauthlib: 1.0.0-py_1           conda-forge\n",
      "    s3transfer:        0.1.13-py36_1001     conda-forge\n",
      "    smart_open:        1.7.1-py_0           conda-forge\n",
      "    twython:           3.7.0-py_0           conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    r-pbdzmq:          0.2_6-r342h934a24f_0 defaults    --> 0.3_2-r3.3.2_0 conda-forge\n",
      "\n",
      "blinker-1.4-py 100% |################################| Time: 0:00:00 138.43 kB/s\n",
      "boto-2.49.0-py 100% |################################| Time: 0:00:00  14.93 MB/s\n",
      "bz2file-0.98-p 100% |################################| Time: 0:00:00   4.92 MB/s\n",
      "docutils-0.14- 100% |################################| Time: 0:00:00   1.42 MB/s\n",
      "jmespath-0.9.3 100% |################################| Time: 0:00:00   6.38 MB/s\n",
      "python-crfsuit 100% |################################| Time: 0:00:00   1.58 MB/s\n",
      "pyjwt-1.6.4-py 100% |################################| Time: 0:00:00   1.41 MB/s\n",
      "oauthlib-2.1.0 100% |################################| Time: 0:00:00  12.42 MB/s\n",
      "botocore-1.12. 100% |################################| Time: 0:00:06 495.30 kB/s\n",
      "r-pbdzmq-0.3_2 100% |################################| Time: 0:00:00 398.28 kB/s\n",
      "requests-oauth 100% |################################| Time: 0:00:00  29.61 kB/s\n",
      "s3transfer-0.1 100% |################################| Time: 0:00:00 400.41 kB/s\n",
      "boto3-1.9.47-p 100% |################################| Time: 0:00:00 325.16 kB/s\n",
      "twython-3.7.0- 100% |################################| Time: 0:00:00   7.31 MB/s\n",
      "smart_open-1.7 100% |################################| Time: 0:00:00 397.37 kB/s\n",
      "gensim-3.5.0-p 100% |################################| Time: 0:01:08 349.92 kB/s\n",
      "nltk-3.2.5-py_ 100% |################################| Time: 0:00:01 533.51 kB/s\n"
     ]
    }
   ],
   "source": [
    "# needed if you want to install the nltk on your docker image\n",
    "\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
