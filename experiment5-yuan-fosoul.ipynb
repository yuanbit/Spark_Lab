{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-05\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex5').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "#from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "sampled = user_df\n",
    "\n",
    "user_df_pre = user_df\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./paper0.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "#w_df.show()\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 1 (Pre-processing Text for word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10 most similar words to “science” using conservative pre-processing\n",
      "\n",
      "+------------+------------------+\n",
      "|        word|        similarity|\n",
      "+------------+------------------+\n",
      "|       among| 0.929947018623352|\n",
      "|        they|0.9288403391838074|\n",
      "|     analyze|     0.91845703125|\n",
      "|  principles|0.9139120578765869|\n",
      "|    elements|0.9135629534721375|\n",
      "|  integrated|0.9131134748458862|\n",
      "|organization|0.9103100299835205|\n",
      "| recommender|0.9093801379203796|\n",
      "|   represent| 0.907145082950592|\n",
      "|  processing|0.9065096974372864|\n",
      "+------------+------------------+\n",
      "\n",
      "top10 most similar words to “science” using intensive pre-processing\n",
      "\n",
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "| qualiti|0.9525512456893921|\n",
      "|   evolv|0.9520372152328491|\n",
      "| practic|0.9485098123550415|\n",
      "| overlap|0.9436403512954712|\n",
      "|research|0.9434059262275696|\n",
      "|  reveal|0.9401988983154297|\n",
      "|    idea|0.9399848580360413|\n",
      "|   motiv|0.9399530291557312|\n",
      "|  absenc|0.9376935362815857|\n",
      "| topolog|0.9373517036437988|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################### Conservative pre-processing ###################################################\n",
    "\n",
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "conservative_df = df\n",
    "conservative_df = conservative_df.withColumn(\"paperID\", conservative_df[\"paperID\"].cast(IntegerType()))\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "model = word2Vec.fit(conservative_df)\n",
    "\n",
    "\n",
    "print(\"top10 most similar words to “science” using conservative pre-processing\")\n",
    "print()\n",
    "synonyms = model.findSynonyms('science', 10)\n",
    "synonyms.show()\n",
    "\n",
    "#################################### Intensive pre-processing #####################################################\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('tokens', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"tokens\")\n",
    "\n",
    "intensive_df = df\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "model2 = word2Vec.fit(intensive_df)\n",
    "\n",
    "print(\"top10 most similar words to “science” using intensive pre-processing\")\n",
    "print()\n",
    "synonyms2 = model2.findSynonyms(ps.stem('science'), 10)\n",
    "synonyms2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results for intensive pre-processing are better (the similarity scores are higher) because unimportant words are removed and words with the same stems are shown in the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 2 (Analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analogy for the conservative pre-processing model:\n",
      "\n",
      "+--------------+------------------+\n",
      "|          word|        similarity|\n",
      "+--------------+------------------+\n",
      "|           dna|0.9437289834022522|\n",
      "|           rna|0.9341936111450195|\n",
      "|highthroughput|0.9332094788551331|\n",
      "|      projects|0.9273788928985596|\n",
      "|       protein|0.9262586236000061|\n",
      "+--------------+------------------+\n",
      "\n",
      "analogy for the intensive pre-processing model:\n",
      "\n",
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|contrast|0.9072569608688354|\n",
      "|  materi|0.8987579941749573|\n",
      "|    mass|0.8907632827758789|\n",
      "|    wiki|0.8852062821388245|\n",
      "| sequenc|0.8777000308036804|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getAverageVector(query, model):\n",
    "    df = sqlContext.createDataFrame([[query]], ['tokens'])\n",
    "    return model.transform(df)\n",
    "\n",
    "def analogy(word1, word2, word3, model, stemming=False):        \n",
    "    vec = model.getVectors()\n",
    "    words = [word1, word2, word3]\n",
    "    keywords = []\n",
    "    vectors = []\n",
    "    for i in words:\n",
    "        keywords.append([x.lower().strip() for x in re.split(\"[^A-Za-z]+\", i)])\n",
    "        \n",
    "    if stemming:\n",
    "        for i, query in enumerate(keywords):\n",
    "            for j, word in enumerate(query):\n",
    "                keywords[i][j] = ps.stem(word)\n",
    "    \n",
    "    for query in keywords:\n",
    "        if len(query) > 1:\n",
    "            vectors.append(getAverageVector(query, model).head()[1])\n",
    "        else:\n",
    "            vectors.append(vec.where(vec.word==query[0]).head()[1])\n",
    "            \n",
    "    w = vectors[0] - vectors[1] + vectors[2]\n",
    "    result = model.findSynonyms((-1)*w,5)\n",
    "    return result\n",
    "\n",
    "print(\"analogy for the conservative pre-processing model:\")\n",
    "print()\n",
    "analogy(\"machine learning\", \"prediction\", \"recommender system\", model, stemming=False).show()\n",
    "\n",
    "print(\"analogy for the intensive pre-processing model:\")\n",
    "print()\n",
    "analogy(\"machine learning\", \"prediction\", \"recommender systems\", model2, stemming=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 3 (From Embeddings to Paper Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "| paperID|       paper_profile|\n",
      "+--------+--------------------+\n",
      "|12832332|[-0.0047933138315...|\n",
      "| 1305474|[-0.0058180570508...|\n",
      "| 1001231|[-0.0077452716323...|\n",
      "|  352713|[-0.0148051936151...|\n",
      "|  956315|[-0.0140641083614...|\n",
      "|  945604|[-0.0126434465753...|\n",
      "|10294999|[-0.0168046080972...|\n",
      "|  967275|[-0.0096054069311...|\n",
      "|  115945|[-0.0151569036622...|\n",
      "|11733005|[-0.0146136129983...|\n",
      "| 9045137|[-0.0162341385197...|\n",
      "| 3728173|[-0.0073623910933...|\n",
      "| 8310458|[-0.0058715931459...|\n",
      "|   80546|[-0.0089703239850...|\n",
      "| 5842862|[-0.0147599245198...|\n",
      "| 1242600|[-0.0181218879103...|\n",
      "| 3467077|[-0.0114230531488...|\n",
      "|  309395|[-0.0178380076933...|\n",
      "|  305755|[0.00962374638766...|\n",
      "| 6603134|[-0.0135982233764...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_w2v_cp = model.transform(conservative_df)\n",
    "paper_w2v_cp = paper_w2v_cp.select(\"paperID\", col(\"result\").alias(\"paper_profile\"))\n",
    "\n",
    "paper_w2v_cp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|        user_profile|\n",
      "+--------------------+--------------------+\n",
      "|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|\n",
      "|bbcd9dae3160ddcb9...|[-0.0028135964860...|\n",
      "|9cb1df6a39b70d88f...|[-0.0023582438061...|\n",
      "|22ede8ed38ebcbaf8...|[-0.0042628118548...|\n",
      "|1eac022a97d683eac...|[-0.0032774288954...|\n",
      "|589b870a611c25fa9...|[-0.0034229244691...|\n",
      "|a0bbf6bb9b1c818f3...|[-0.0031705740899...|\n",
      "|8d898a2171f552b3d...|[-0.0010895416652...|\n",
      "|d1d41a15201915503...|[-0.0022795030562...|\n",
      "|f1e1cd4ff25018273...|[-0.0045282421986...|\n",
      "|3b715ebaf1f8f81a1...|[-0.0036120140960...|\n",
      "|cbd4a69e4b3ed3472...|[-0.0038574325744...|\n",
      "|f05bcffe7951de9e5...|[-0.0032565576942...|\n",
      "|cf9c7f356092c34be...|[-0.0028873409683...|\n",
      "|ee1dfee93ebeadade...|[-0.0034400954917...|\n",
      "|b36c3189bb1457cd0...|[-0.0040417211316...|\n",
      "|3c9a5b5e6448a0119...|[-0.0010189504721...|\n",
      "|a7fe408c548c6112d...|[-0.0047462778076...|\n",
      "|b656009a6efdc8b1a...|[-0.0036004181851...|\n",
      "|4c8912d1b04471cf5...|[-0.0035911134656...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_doc = user_df.join(conservative_df, ['paperID']).orderBy(\"userID\")\n",
    "user_doc = user_doc.drop(\"paperID\")\n",
    "\n",
    "# Concatenate the words of the user library\n",
    "user_doc = user_doc.rdd.map(lambda user_doc: (user_doc.userID, user_doc.tokens)).reduceByKey(lambda x,y: x + y).toDF(['userID','tokens'])\n",
    "\n",
    "model_user = word2Vec.fit(user_doc)\n",
    "\n",
    "user_w2v_cp = model_user.transform(user_doc)\n",
    "\n",
    "user_w2v_cp = user_w2v_cp.select(\"userID\", col(\"result\").alias(\"user_profile\"))\n",
    "\n",
    "user_w2v_cp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "unrated_df = user_df_pre.withColumn(\"unrated\", get_unrated(user_df_pre.paperID))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"paperID\")\n",
    "\n",
    "unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"unrated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|paperID|              userID|        user_profile|       paper_profile|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "| 212874|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0131514674861...|\n",
      "|1326856|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0102063186265...|\n",
      "|  81501|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0232182141199...|\n",
      "|  65083|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0306622119581...|\n",
      "| 523772|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0243687142307...|\n",
      "| 105906|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0094669061247...|\n",
      "|1121661|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0072813053467...|\n",
      "|7355647|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0194716824508...|\n",
      "|  72879|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0139008522373...|\n",
      "| 244827|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0163729196105...|\n",
      "| 244827|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0163729196105...|\n",
      "|5307378|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0119814829279...|\n",
      "| 531300|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0101011594570...|\n",
      "| 211804|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0147707040001...|\n",
      "| 965334|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0194895810218...|\n",
      "| 965334|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0194895810218...|\n",
      "|  80546|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0089703239850...|\n",
      "| 849862|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0058897257153...|\n",
      "| 956315|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0140641083614...|\n",
      "|1242600|90f1a3e6fcdbf9bc5...|[-0.0025672873174...|[-0.0181218879103...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = unrated_df.join(user_w2v_cp, [\"userID\"]).join(paper_w2v_cp, [\"paperID\"])\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                 |\n",
      "+--------------------------------+---------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|361498, 154, 5307378, 6434100, 4511, 197260, 484851, 820297, 5662136, 90413|\n",
      "+--------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def w2vRS(userID, df, k):\n",
    "    df = df.where(df.userID==userID)\n",
    "    # Apply similarity metric to the user_profile and paper_profile\n",
    "    sim_df = df.withColumn('Similarity', compute_sim(df.user_profile, df.paper_profile))\n",
    "    # Partition by userID and order by the similarity in descending order\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    # Add row numbers to the rows and get the top-k rows\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "    # Renaming\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    w2vRS_df = get_r.select(\"userID\", \"paperID\")\n",
    "    # un-explode, concatenate the recommended papers for each user\n",
    "    w2vRS_df = w2vRS_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(w2vRS_df.paperID)).alias(\"top_papers\"))\n",
    "    \n",
    "    return w2vRS_df\n",
    "\n",
    "k = 10\n",
    "\n",
    "user = \"1eac022a97d683eace8815545ce3153f\"\n",
    "\n",
    "user_rec = w2vRS(user, new_df, k)\n",
    "\n",
    "user_rec.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 3 (Content-based recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------------------------------+\n",
      "|userID                          |top_papers                                 |\n",
      "+--------------------------------+-------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|11733005, 8336239, 7010764, 2887105, 115945|\n",
      "+--------------------------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cbrs(u, k):\n",
    "    # Apply similarity metric to the user_profile and paper_profile\n",
    "    sim_df = u.withColumn('Similarity', compute_sim(u.user_profile, u.paper_profile))\n",
    "    # Partition by userID and order by the similarity in descending order\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    # Add row numbers to the rows and get the top-k rows\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "    # Renaming\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    cbrs_df = get_r.select(\"userID\", \"paperID\")\n",
    "    # un-explode, concatenate the recommended papers for each user\n",
    "    cbrs_df = cbrs_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(cbrs_df.paperID)).alias(\"top_papers\"))\n",
    "    \n",
    "    return cbrs_df\n",
    "\n",
    "k = 5\n",
    "\n",
    "user_rec = cbrs(df_selected, k).show(truncate=False)\n",
    "\n",
    "#print(\"The top \" + str(k) + \" papers for \" + str(user_rec.head()[0]) + \" are \" + str(user_rec.head()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA ###\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# just rename the column to use it with the built-in methods\n",
    "termFrequencyVector = vector_df.select('paperId', col('term_frequency_sparse').alias('features'))\n",
    "# Trains a LDA model\n",
    "# set k=40 to have 40 different topics\n",
    "lda = LDA(k=40)\n",
    "model = lda.fit(termFrequencyVector)\n",
    "\n",
    "# each topic is described by 5 terms\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "#topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# it shows the probabilty of each topic for each paper\n",
    "transformed = model.transform(termFrequencyVector)\n",
    "#transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "\n",
    "lda_user_profile = transformed.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "lda_user_profile = lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "lda_user_profile = lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "lda_user_profile = lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "#lda_user_profile.show()\n",
    "\n",
    "df_lda = unrated_df.join(transformed, [\"paperID\"]).join(lda_user_profile, [\"userID\"])\n",
    "\n",
    "df_lda = df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "#df_lda.show()\n",
    "\n",
    "df_selected_lda = df_lda.where(df_lda.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "#df_selected_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------------------+\n",
      "|userID                          |top_papers                                  |\n",
      "+--------------------------------+--------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|7164691, 9563857, 1363828, 11733005, 8336239|\n",
      "+--------------------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbrs(df_selected_lda, 5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 4 Sampling and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+---------+\n",
      "|              userID|             paperID|libSize|trainSize|\n",
      "+--------------------+--------------------+-------+---------+\n",
      "|1eac022a97d683eac...|[3973229, 322433,...|    321|      256|\n",
      "|589b870a611c25fa9...|[1283233, 1305474...|      8|        6|\n",
      "+--------------------+--------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "\n",
    "num_user = 2\n",
    "\n",
    "# Order the users randomly and get n sampled users\n",
    "sampled_users = user_df_pre.orderBy(rand()).limit(num_user)\n",
    "\n",
    "# Get length of each list in a column\n",
    "get_len_udf = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "# Get library size for each user\n",
    "sampled_users = sampled_users.withColumn(\"libSize\", get_len_udf(\"paperID\"))\n",
    "\n",
    "# Get the size of the training set\n",
    "get_train = udf(lambda x: int(x*0.8), IntegerType())\n",
    "\n",
    "# Get size of the training set for each user\n",
    "sampled_users = sampled_users.withColumn(\"trainSize\", get_train(\"libSize\"))\n",
    "\n",
    "sampled_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the paperIDs for each user\n",
    "sampled_exploded = sampled_users.withColumn(\"paperID\", explode(col(\"paperID\")))\n",
    "\n",
    "# Partion by userID and order them randomly\n",
    "window = Window.partitionBy(col(\"userID\")).orderBy(rand())\n",
    "\n",
    "# Get row numbers\n",
    "sampled_exploded = sampled_exploded.select(col('*'), row_number().over(window).alias('row_number'))\n",
    "\n",
    "# Get the rows less than or equal to the training set size\n",
    "# The rows will be different each time because of .orderBy(rand()) in the window function\n",
    "training_df = sampled_exploded.where(col('row_number') <= col(\"trainSize\"))\n",
    "training_df = training_df.select(\"userID\", \"paperID\").orderBy(\"userID\")\n",
    "#training_df.show()\n",
    "\n",
    "# Get the test set by selecting the rows greater than the training size\n",
    "test_df = sampled_exploded.where(col('row_number') > col(\"trainSize\"))\n",
    "test_df = test_df.select(\"userID\", \"paperID\").orderBy(\"userID\")\n",
    "#test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "training_user_profile = tf_idf_built_in.join(training_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "training_user_profile = training_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "# Same steps as above\n",
    "training_user_profile = training_user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n",
    "\n",
    "training_user_profile = training_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "train_df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(training_user_profile, [\"userID\"])\n",
    "\n",
    "train_df = train_df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "train_df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(training_user_profile, [\"userID\"])\n",
    "\n",
    "train_df = train_df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\")).orderBy(\"userID\")\n",
    "\n",
    "#train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "training_lda_user_profile = transformed.join(training_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "training_lda_user_profile = training_lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "training_lda_user_profile = training_lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "training_lda_user_profile = training_lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "train_df_lda = unrated_df.join(transformed, [\"paperID\"]).join(training_lda_user_profile, [\"userID\"])\n",
    "\n",
    "train_df_lda = train_df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\")).orderBy(\"userID\")\n",
    "\n",
    "#train_df_lda.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 5 (Off-line evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF recommender\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                               |\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[11733005, 7010764, 8336239, 115945, 7496675, 2887105, 8310458, 945604, 1305474, 9045137]|\n",
      "|589b870a611c25fa99bd3d7295ac0622|[2887105, 7496675, 8336239, 9563857, 7010764, 1042553, 7164691, 9045137, 255030, 3010240]|\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "LDA recommender\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                                |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[7164691, 9563857, 1363828, 8336239, 11733005, 3728173, 115945, 2887105, 9045137, 8310458]|\n",
      "|589b870a611c25fa99bd3d7295ac0622|[255030, 7164691, 238188, 9563857, 1363828, 7010764, 3728173, 11733005, 8336239, 115945]  |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a) Generate 10 recommendations\n",
    "\n",
    "def castToArray(df, colName):\n",
    "    dff = df.withColumn(colName, split(col(colName), \", \").cast(ArrayType(IntegerType())))\n",
    "    return dff\n",
    "\n",
    "k = 10\n",
    "\n",
    "# TF-IDF\n",
    "print(\"TF-IDF recommender\")\n",
    "tf_rec = cbrs(train_df, k).orderBy(\"userID\")\n",
    "# Cast the recommendations to a list of integers\n",
    "tf_rec  = castToArray(tf_rec, \"top_papers\")\n",
    "tf_rec.show(truncate=False)\n",
    "\n",
    "print(\"LDA recommender\")\n",
    "# LDA\n",
    "lda_rec  = cbrs(train_df_lda, k).orderBy(\"userID\")\n",
    "# Cast the recommendations to a list of integers\n",
    "lda_rec  = castToArray(lda_rec, \"top_papers\")\n",
    "lda_rec.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|             paperID|\n",
      "+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|[3469193, 600359,...|\n",
      "|589b870a611c25fa9...|   [1283233, 956315]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the test set into a list of integers\n",
    "test_df_collected = test_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(test_df.paperID)).alias(\"paperID\"))\n",
    "test_df_collected = test_df_collected.withColumn(\"paperID\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"paperID\")).orderBy(\"userID\")\n",
    "\n",
    "test_df_collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tf_test = test_df_collected.join(tf_rec, \"userID\")\n",
    "joined_tf_test = joined_tf_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "\n",
    "joined_lda_test = test_df_collected.join(lda_rec, \"userID\")\n",
    "joined_lda_test = joined_lda_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF recommender\n",
      "+--------------------------------+----+\n",
      "|userID                          |Hits|\n",
      "+--------------------------------+----+\n",
      "|1eac022a97d683eace8815545ce3153f|[]  |\n",
      "|589b870a611c25fa99bd3d7295ac0622|[]  |\n",
      "+--------------------------------+----+\n",
      "\n",
      "LDA recommender\n",
      "+--------------------------------+----+\n",
      "|userID                          |Hits|\n",
      "+--------------------------------+----+\n",
      "|1eac022a97d683eace8815545ce3153f|[]  |\n",
      "|589b870a611c25fa99bd3d7295ac0622|[]  |\n",
      "+--------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getHits(train, test):\n",
    "    return list(set(train).intersection(test))\n",
    "\n",
    "getHits_udf = udf(getHits, ArrayType(IntegerType()))\n",
    "# TF-IDF\n",
    "print(\"TF-IDF recommender\")\n",
    "tf_hits = joined_tf_test.withColumn('Hits', getHits_udf(joined_tf_test.train_set, joined_tf_test.test_set))\n",
    "tf_hits = tf_hits.select(\"userID\", \"Hits\")\n",
    "tf_hits.show(truncate=False)\n",
    "\n",
    "print(\"LDA recommender\")\n",
    "# LDA\n",
    "lda_hits = joined_lda_test.withColumn('Hits', getHits_udf(joined_lda_test.train_set, joined_lda_test.test_set))\n",
    "lda_hits = lda_hits.select(\"userID\", \"Hits\")\n",
    "lda_hits.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we could not get results for the hits, we modified the results to show our computations for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              userID|     Hits|\n",
      "+--------------------+---------+\n",
      "|               user0|[1, 2, 3]|\n",
      "|               user1|[4, 5, 6]|\n",
      "|1eac022a97d683eac...|       []|\n",
      "|589b870a611c25fa9...|       []|\n",
      "+--------------------+---------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              userID|             paperID|\n",
      "+--------------------+--------------------+\n",
      "|               user0|     [2, 3, 4, 1, 5]|\n",
      "|               user1|[5, 4, 6, 7, 8, 9...|\n",
      "|1eac022a97d683eac...|[3469193, 600359,...|\n",
      "|589b870a611c25fa9...|   [1283233, 956315]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########## example #####################\n",
    "\n",
    "columns = ['userID', 'Hits']\n",
    "vals = [(\"user0\", \"1, 2, 3\"), (\"user1\", \"4, 5, 6\")]\n",
    "\n",
    "ex = sqlContext.createDataFrame(vals, columns)\n",
    "ex = castToArray(ex, \"Hits\")\n",
    "\n",
    "ex_hits = ex.union(tf_hits)\n",
    "ex_hits.show()\n",
    "\n",
    "columns2 = ['userID', 'paperID']\n",
    "vals2 = [(\"user0\", \"2, 3, 4, 1, 5\"), (\"user1\", \"5, 4, 6, 7, 8, 9, 10\")]\n",
    "ex2 = sqlContext.createDataFrame(vals2, columns2)\n",
    "ex2 = castToArray(ex2, \"paperID\")\n",
    "\n",
    "ex_test = ex2.union(test_df_collected)\n",
    "ex_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|              userID|             paperID|     Hits|\n",
      "+--------------------+--------------------+---------+\n",
      "|               user1|[5, 4, 6, 7, 8, 9...|[4, 5, 6]|\n",
      "|1eac022a97d683eac...|[3469193, 600359,...|       []|\n",
      "|589b870a611c25fa9...|   [1283233, 956315]|       []|\n",
      "|               user0|     [2, 3, 4, 1, 5]|[1, 2, 3]|\n",
      "+--------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_test_hits = ex_test.join(ex_hits, \"userID\")\n",
    "joined_test_hits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision@10 for TF-IDF is: 0.15\n"
     ]
    }
   ],
   "source": [
    "num_user = 4\n",
    "\n",
    "def hitSize_k(hits):\n",
    "    return len(hits)/k\n",
    "\n",
    "hitSize_k_udf = udf(lambda x: hitSize_k(x), FloatType())\n",
    "\n",
    "def precisionK(df):\n",
    "    df = df.withColumn(\"hitSize_k\", hitSize_k_udf(\"Hits\"))\n",
    "    sumHits_k = df.select(f.sum(\"hitSize_k\")).collect()[0][0]\n",
    "    precision = (1/num_user)*sumHits_k\n",
    "    return precision\n",
    "\n",
    "print(\"The precision@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % precisionK(ex_hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Recall@10 for TF-IDF is: 0.26\n"
     ]
    }
   ],
   "source": [
    "def hitSize_testSize(hits, testSize):\n",
    "    return len(hits)/len(testSize)\n",
    "\n",
    "hitSize_testSize_udf = udf(hitSize_testSize, FloatType())\n",
    "\n",
    "def recallK(df):\n",
    "    df = df.withColumn(\"hitSize_testSize\", hitSize_testSize_udf(df.Hits, df.paperID))\n",
    "    sumHits_test = df.select(f.sum(\"hitSize_testSize\")).collect()[0][0]\n",
    "    recall = (1/num_user)*sumHits_test\n",
    "    return recall\n",
    "\n",
    "print(\"The Recall@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % recallK(joined_test_hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MRR@10 for TF-IDF is: 0.33\n"
     ]
    }
   ],
   "source": [
    "def getPositionU(hits, test):\n",
    "    if not hits:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1/test.index(hits[0])\n",
    "\n",
    "getPositionU_udf = udf(getPositionU, FloatType())\n",
    "\n",
    "def mrrK(df):\n",
    "    df = df.withColumn(\"P_u\", getPositionU_udf(df.Hits, df.paperID))\n",
    "    sumP_u = df.select(f.sum(\"P_u\")).collect()[0][0]\n",
    "    mrr = (1/num_user)*sumP_u\n",
    "    return mrr\n",
    "\n",
    "print(\"The MRR@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % mrrK(joined_test_hits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we could not get good results for the hits (empty), we computed the precision for k in {10, 20}\n",
    "#### We could change the 21 in the range to 101 to compute the precision up till k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision@10 for TF-IDF is: 0.00\n",
      "The Recall@10 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@11 for TF-IDF is: 0.00\n",
      "The Recall@11 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@12 for TF-IDF is: 0.00\n",
      "The Recall@12 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@13 for TF-IDF is: 0.00\n",
      "The Recall@13 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@14 for TF-IDF is: 0.00\n",
      "The Recall@14 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@15 for TF-IDF is: 0.00\n",
      "The Recall@15 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@16 for TF-IDF is: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e0f967061c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The precision@\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" for TF-IDF is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprecisionK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_hits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Recall@\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" for TF-IDF is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrecallK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_hits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-761038ac5ddc>\u001b[0m in \u001b[0;36mrecallK\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecallK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hitSize_testSize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhitSize_testSize_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaperID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msumHits_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hitSize_testSize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msumHits_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_user = 2\n",
    "\n",
    "for k in range(10, 21):\n",
    "    # TF-IDF\n",
    "    tf_rec = cbrs(train_df, k).orderBy(\"userID\")\n",
    "    tf_rec  = castToArray(tf_rec, \"top_papers\")\n",
    "    joined_tf_test = test_df_collected.join(tf_rec, \"userID\")\n",
    "    joined_tf_test = joined_tf_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "    tf_hits = joined_tf_test.withColumn('Hits', getHits_udf(joined_tf_test.train_set, joined_tf_test.test_set))\n",
    "    tf_hits = tf_hits.select(\"userID\", \"Hits\")\n",
    "    test_hits =  test_df_collected.join(tf_hits, \"userID\")\n",
    "    \n",
    "    print(\"The precision@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % precisionK(tf_hits)))\n",
    "    print(\"The Recall@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % recallK(test_hits)) + \"\\n\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10, 21):\n",
    "    # LDA\n",
    "    lda_rec  = cbrs(train_df_lda, k).orderBy(\"userID\")\n",
    "    lda_rec  = castToArray(lda_rec, \"top_papers\")\n",
    "    joined_lda_test = test_df_collected.join(lda_rec, \"userID\")\n",
    "    joined_lda_test = joined_lda_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "    lda_hits = joined_lda_test.withColumn('Hits', getHits_udf(joined_lda_test.train_set, joined_lda_test.test_set))\n",
    "    lda_hits = lda_hits.select(\"userID\", \"Hits\")\n",
    "    test_hits_lda =  test_df_collected.join(lda_hits, \"userID\")\n",
    "    \n",
    "    print(\"The precision@\" + str(k) + \" for LDA is: \" + (\"%.2f\" % precisionK(tf_hits)))\n",
    "    print(\"The Recall@\" + str(k) + \" for LDA is: \" + (\"%.2f\" % recallK(test_hits)) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
