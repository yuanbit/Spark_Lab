{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-05\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex5').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unExplode(df, groupByColName, collectColName, colType):\n",
    "    types = {'string': StringType(), 'integer': IntegerType()}\n",
    "    df_collected = df.groupby(col(groupByColName)).agg(f.concat_ws(\", \", f.collect_list(df[collectColName])).alias(collectColName))\n",
    "    result = df_collected.withColumn(col(collectColName), split(col(collectColName), \",\\s*\").cast(ArrayType(types[colType])).alias(collectColName).orderBy(groupByColName))\n",
    "                                     \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "user_df_pre = user_df\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./paper0.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "#w_df.show()\n",
    "\n",
    "paper_df = w_df\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 1 (Pre-processing Text for word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Conservative pre-processing ###################################################\n",
    "\n",
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "conservative_df = df\n",
    "conservative_df = conservative_df.withColumn(\"paperID\", conservative_df[\"paperID\"].cast(IntegerType()))\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "model = word2Vec.fit(conservative_df)\n",
    "\n",
    "\n",
    "# print(\"top10 most similar words to “science” using conservative pre-processing\")\n",
    "# print()\n",
    "# synonyms = model.findSynonyms('science', 10)\n",
    "# synonyms.show()\n",
    "\n",
    "# #################################### Intensive pre-processing #####################################################\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('tokens', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"tokens\")\n",
    "\n",
    "intensive_df = df\n",
    "paper_terms = df\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "model2 = word2Vec.fit(intensive_df)\n",
    "\n",
    "# print(\"top10 most similar words to “science” using intensive pre-processing\")\n",
    "# print()\n",
    "# synonyms2 = model2.findSynonyms(ps.stem('science'), 10)\n",
    "#synonyms2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results for intensive pre-processing are better (the similarity scores are higher) because unimportant words are removed and words with the same stems are shown in the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 2 (Analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageVector(query, model):\n",
    "    df = sqlContext.createDataFrame([[query]], ['tokens'])\n",
    "    return model.transform(df)\n",
    "\n",
    "def analogy(word1, word2, word3, model, stemming=False):        \n",
    "    vec = model.getVectors()\n",
    "    words = [word1, word2, word3]\n",
    "    keywords = []\n",
    "    vectors = []\n",
    "    for i in words:\n",
    "        keywords.append([x.lower().strip() for x in re.split(\"[^A-Za-z]+\", i)])\n",
    "        \n",
    "    if stemming:\n",
    "        for i, query in enumerate(keywords):\n",
    "            for j, word in enumerate(query):\n",
    "                keywords[i][j] = ps.stem(word)\n",
    "    \n",
    "    for query in keywords:\n",
    "        if len(query) > 1:\n",
    "            vectors.append(getAverageVector(query, model).head()[1])\n",
    "        else:\n",
    "            vectors.append(vec.where(vec.word==query[0]).head()[1])\n",
    "            \n",
    "    w = vectors[0] - vectors[1] + vectors[2]\n",
    "    result = model.findSynonyms((-1)*w,5)\n",
    "    return result\n",
    "\n",
    "# print(\"analogy for the conservative pre-processing model:\")\n",
    "# print()\n",
    "# analogy(\"machine learning\", \"prediction\", \"recommender system\", model, stemming=False).show()\n",
    "\n",
    "# print(\"analogy for the intensive pre-processing model:\")\n",
    "# print()\n",
    "# analogy(\"machine learning\", \"prediction\", \"recommender systems\", model2, stemming=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 3 (From Embeddings to Paper Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_w2v_cp = model.transform(conservative_df)\n",
    "paper_w2v_cp = paper_w2v_cp.select(\"paperID\", col(\"result\").alias(\"paper_profile\"))\n",
    "\n",
    "def getUserProfile(user_df, preprocessed_df):\n",
    "    user_doc = user_df.join(preprocessed_df, ['paperID']).orderBy(\"userID\")\n",
    "    user_doc = user_doc.drop(\"paperID\")\n",
    "    # Concatenate the words of the user library\n",
    "    user_doc = user_doc.rdd.map(lambda user_doc: (user_doc.userID, user_doc.tokens)).reduceByKey(lambda x,y: x + y).toDF(['userID','tokens'])\n",
    "    \n",
    "    # Use word2Vec model fitting\n",
    "    model_user = word2Vec.fit(user_doc)\n",
    "    user_w2v_cp = model_user.transform(user_doc)\n",
    "    user_w2v_cp = user_w2v_cp.select(\"userID\", col(\"result\").alias(\"user_profile\"))\n",
    "    return user_w2v_cp\n",
    "\n",
    "user_w2v_cp = getUserProfile(user_df, conservative_df)\n",
    "#user_w2v_cp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())\n",
    "\n",
    "def getUnrated(user_df):\n",
    "    # Add a new column of unrated papers for each user\n",
    "    unrated_df = user_df.withColumn(\"unrated\", get_unrated(user_df.paperID))\n",
    "    unrated_df = unrated_df.drop(\"paperID\")\n",
    "    unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "    unrated_df = unrated_df.drop(\"unrated\")\n",
    "    \n",
    "    return unrated_df\n",
    "\n",
    "#unrated_df = getUnrated(user_df_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vRS(user_df, unrated_df, user_profile, paper_profile, k, passUserID=False, userID=None):\n",
    "\n",
    "    df = unrated_df.join(user_profile, [\"userID\"]).join(paper_profile, [\"paperID\"])\n",
    "        \n",
    "    if passUserID:\n",
    "        df = df.where(df.userID==userID)\n",
    "        \n",
    "    # Apply similarity metric to the user_profile and paper_profile\n",
    "    sim_df = df.withColumn('Similarity', compute_sim(df.user_profile, df.paper_profile))\n",
    "    # Partition by userID and order by the similarity in descending order\n",
    "\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    # Add row numbers to the rows and get the top-k rows\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "\n",
    "    # Renaming\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    w2vRS_df = get_r.select(\"userID\", \"paperID\")\n",
    "    # un-explode, concatenate the recommended papers for each user\n",
    "    w2vRS_df = w2vRS_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(w2vRS_df.paperID)).alias(\"top_papers\"))\n",
    "    w2vRS_df = w2vRS_df.orderBy(\"userID\")\n",
    "    \n",
    "    return w2vRS_df\n",
    "\n",
    "# k = 10\n",
    "\n",
    "# user = \"1eac022a97d683eace8815545ce3153f\"\n",
    "\n",
    "# user_rec = w2vRS(user_df_pre, unrated_df, user_w2v_cp, paper_w2v_cp, k)\n",
    "\n",
    "# user_rec.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 4 (Evaluation of Recommender System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lib_size_udf = udf(lambda x:len(x), IntegerType())\n",
    "\n",
    "def sampling(num_users, df, trainingSize=0.8):\n",
    "    # first we sample users and then we sample each user's library\n",
    "    \n",
    "    get_training_size_udf = udf(lambda x:int(x*trainingSize), IntegerType())\n",
    "    sampled_users = df.orderBy(rand()).limit(num_users)\n",
    "    sampled_users = sampled_users.withColumn('libSize', get_lib_size_udf('paperID'))\n",
    "    sampled_users = sampled_users.withColumn('trainingSize', get_training_size_udf('libSize'))\n",
    "    \n",
    "    # explode the paperIDs for each user\n",
    "    sampled_exploded = sampled_users.withColumn('paperID', explode(col('paperID')))\n",
    "\n",
    "    # Partion by userID and order them randomly\n",
    "    window = Window.partitionBy(col('userID')).orderBy(rand())\n",
    "\n",
    "    # Get row numbers\n",
    "    sampled_exploded = sampled_exploded.select(col('*'), row_number().over(window).alias('row_number'))\n",
    "\n",
    "    # Get the rows less than or equal to the training set size\n",
    "    # The rows will be different each time because of .orderBy(rand()) in the window function\n",
    "    training_df = sampled_exploded.where(col('row_number') <= col('trainingSize'))\n",
    "    training_df = training_df.select('userID', 'paperID').orderBy('userID')\n",
    "    #training_df.show()\n",
    "\n",
    "    # Get the test set by selecting the rows greater than the training size\n",
    "    test_df = sampled_exploded.where(col('row_number') > col('trainingSize'))\n",
    "    test_df = test_df.select('userID', 'paperID').orderBy('userID')\n",
    "    \n",
    "    return (training_df, test_df)\n",
    "\n",
    "(training_df, test_df) = sampling(50, user_df_pre, trainingSize=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_collected = training_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(training_df.paperID)).alias(\"paperID\"))\n",
    "training_df_collected = training_df_collected.withColumn(\"paperID\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"paperID\")).orderBy(\"userID\")\n",
    "\n",
    "#training_df_collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unrated_df = getUnrated(training_df_collected)\n",
    "\n",
    "sampled_user_w2v_cp = getUserProfile(training_df, conservative_df)\n",
    "#sampled_user_w2v_cp.show()\n",
    "\n",
    "k = 10\n",
    "\n",
    "user_rec = w2vRS(training_df_collected, train_unrated_df, sampled_user_w2v_cp, paper_w2v_cp, k)\n",
    "\n",
    "#user_rec.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def castToArray(df, colName):\n",
    "    dff = df.withColumn(colName, split(col(colName), \", \").cast(ArrayType(IntegerType())))\n",
    "    return dff\n",
    "\n",
    "\n",
    "def getHits(train, test):\n",
    "    return list(set(train).intersection(test))\n",
    "\n",
    "getHits_udf = udf(getHits, ArrayType(IntegerType()))\n",
    "\n",
    "num_user = 50\n",
    "\n",
    "def hitSize_k(hits):\n",
    "    if len(hits) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(hits)/k\n",
    "\n",
    "hitSize_k_udf = udf(lambda x: hitSize_k(x), FloatType())\n",
    "\n",
    "def precisionK(df):\n",
    "    df = df.withColumn(\"hitSize_k\", hitSize_k_udf(\"Hits\"))\n",
    "    sumHits_k = df.select(f.sum(\"hitSize_k\")).collect()[0][0]\n",
    "    precision = (1/num_user)*sumHits_k\n",
    "    return precision\n",
    "\n",
    "def hitSize_testSize(hits, testSize):\n",
    "    return len(hits)/len(testSize)\n",
    "\n",
    "hitSize_testSize_udf = udf(hitSize_testSize, FloatType())\n",
    "\n",
    "def recallK(df):\n",
    "    df = df.withColumn(\"hitSize_testSize\", hitSize_testSize_udf(df.Hits, df.paperID))\n",
    "    sumHits_test = df.select(f.sum(\"hitSize_testSize\")).collect()[0][0]\n",
    "    recall = (1/num_user)*sumHits_test\n",
    "    return recall\n",
    "\n",
    "def getPositionU(hits, test):\n",
    "    if not hits:\n",
    "        return 0.0\n",
    "    else:\n",
    "        hit_index = []\n",
    "        for i in range(0, len(hits)):\n",
    "            hit_index.append(test.index(hits[i]))\n",
    "    \n",
    "        return 1/int((np.min(hit_index)))\n",
    "\n",
    "getPositionU_udf = udf(getPositionU, FloatType())\n",
    "\n",
    "def mrrK(df):\n",
    "    df = df.withColumn(\"P_u\", getPositionU_udf(df.Hits, df.paperID))\n",
    "    sumP_u = df.select(f.sum(\"P_u\")).collect()[0][0]\n",
    "    mrr = (1/num_user)*sumP_u\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision@10 for TF-IDF is: 0.000\n",
      "The Recall@10 for TF-IDF is: 0.000\n",
      "The MRR@10 for TF-IDF is: 0.000\n",
      "\n",
      "The precision@100 for TF-IDF is: 0.000\n",
      "The Recall@100 for TF-IDF is: 0.000\n",
      "The MRR@100 for TF-IDF is: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def  evaluateK(k, rec_df, test_df):\n",
    "    # Cast recommended papers to array\n",
    "    rec_df  = castToArray(rec_df, \"top_papers\")\n",
    "    \n",
    "    # Concatenate the papers for test_df\n",
    "    test_df_collected = test_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(test_df.paperID)).alias(\"paperID\"))\n",
    "    test_df_collected = test_df_collected.withColumn(\"paperID\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"paperID\")).orderBy(\"userID\")\n",
    "    \n",
    "    # Join recommended papers with test_df\n",
    "    joined_test = test_df_collected.join(rec_df, \"userID\")\n",
    "    joined_test = joined_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "    \n",
    "    hits = joined_test.withColumn('Hits', getHits_udf(joined_test.train_set, joined_test.test_set))\n",
    "    hits = hits.select(\"userID\", \"Hits\")\n",
    "    \n",
    "    test_hits =  test_df_collected.join(hits, \"userID\")\n",
    "    \n",
    "    return (precisionK(hits), recallK(test_hits), mrrK(test_hits))\n",
    "    \n",
    "K = [10, 100]\n",
    "\n",
    "for i in range(0, len(K)):\n",
    "    prec, recall, mrr = evaluateK(K[i], user_rec, test_df)  \n",
    "    print(\"The precision@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % prec))\n",
    "    print(\"The Recall@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % recall))\n",
    "    print(\"The MRR@\" + str(K[i]) + \" for TF-IDF is: \" + (\"%.3f\" % mrr) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Sample users with library size greater than 20 ######################################\n",
    "\n",
    "sampled_20 = user_df_pre.withColumn(\"libSize\", get_lib_size_udf(\"paperID\")).filter(col(\"libSize\") > 20)\n",
    "\n",
    "(training20_df, test20_df) = sampling(50, sampled_20, trainingSize=0.8)\n",
    "\n",
    "training20_df_collected = training20_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(training20_df.paperID)).alias(\"paperID\"))\n",
    "training20_df_collected = training20_df_collected.withColumn(\"paperID\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"paperID\")).orderBy(\"userID\")\n",
    "\n",
    "train20_unrated_df = getUnrated(training20_df_collected)\n",
    "\n",
    "sampled20_user_w2v_cp = getUserProfile(training20_df, conservative_df)\n",
    "\n",
    "k = 10\n",
    "\n",
    "user_rec20 = w2vRS(training20_df_collected, train20_unrated_df, sampled20_user_w2v_cp, paper_w2v_cp, k)\n",
    "\n",
    "#user_rec20.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [10, 100]\n",
    "\n",
    "for i in range(0, len(k)):\n",
    "    prec, recall, mrr = evaluateK(k[i], user_rec20, test20_df)  \n",
    "    print(\"The precision@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % prec))\n",
    "    print(\"The Recall@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % recall))\n",
    "    print(\"The MRR@\" + str(k[i]) + \" for TF-IDF is: \" + (\"%.3f\" % mrr) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After repeating the experiment by picking only users which have more than 20 papers in their libraries, we noticed that the performance of the RS improves. This makes sense because there are more papers in the users' libraries, so the similarity scores from the RS will be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 5 (Improving the Recommender System with TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "ip_df = intensive_df.select(\"paperID\", f.explode(\"tokens\").alias(\"tokens\")).distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "ip_df = ip_df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = ip_df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "#important_words.show()\n",
    "\n",
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| − 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")\n",
    "#terms_index_hash.show()\n",
    "\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "#print(num_terms)\n",
    "\n",
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"tokens\").alias(\"terms\"))\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "# tf_df is now a df with a column of paperID and a column of lists of the terms (unexploded)\n",
    "#tf_df.show()\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "vector_df = model.transform(tf_df)\n",
    "vector_df = vector_df.select(\"paperID\", col(\"vectors\").alias(\"term_frequency_sparse\"))\n",
    "\n",
    "# Term frequency vector df\n",
    "#vector_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "#tf_idf_built_in.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop10Terms(sparsevector):\n",
    "    \n",
    "    # Sort indices of SparseVector by the scores in reverse order\n",
    "    # Get top 10 indices of the score\n",
    "    return sorted(range(len(sparsevector)), key=lambda k: sparsevector[k], reverse=True)[:10]\n",
    "\n",
    "getTop10TermsUdf = udf(lambda x: getTop10Terms(x), ArrayType(IntegerType()))\n",
    "\n",
    "tf_idf_filtered = tf_idf_built_in.withColumn(\"index\", getTop10TermsUdf(col(\"features\")))\n",
    "tf_idf_filtered = tf_idf_filtered.drop(\"features\")\n",
    "\n",
    "# Explode top10 terms for each paper\n",
    "tf_idf_filtered = tf_idf_filtered.withColumn(\"index\", explode(tf_idf_filtered.index))\n",
    "\n",
    "#tf_idf_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_joined = terms_index_hash.join(tf_idf_filtered, [\"index\"])\n",
    "hash_joined = hash_joined.drop(\"index\")\n",
    "\n",
    "hash_joined_collected = hash_joined.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(hash_joined[\"terms\"])).alias(\"terms\"))\n",
    "hash_joined_collected = hash_joined_collected.withColumn(\"terms\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"term\")).orderBy(\"paperID\")\n",
    "\n",
    "hash_joined_collected = hash_joined_collected.select(\"paperID\", col(\"terms\").alias(\"tokens\"))\n",
    "\n",
    "# Fit word2vec\n",
    "\n",
    "model_tf = word2Vec.fit(hash_joined_collected)\n",
    "\n",
    "paper_w2v_ip = model_tf.transform(hash_joined_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`result`' given input columns: [paperID, paper_profile];;\\n'Project [paperID#127, 'result AS paper_profile#46283]\\n+- Project [paperID#127, result#231 AS paper_profile#236]\\n   +- Project [paperID#127, tokens#123, UDF(tokens#123) AS result#231]\\n      +- Project [cast(paperID#87 as int) AS paperID#127, tokens#123]\\n         +- Project [paperID#87, <lambda>(tokens#119) AS tokens#123]\\n            +- Project [paperID#87, <lambda>(tokens#111) AS tokens#119]\\n               +- Project [paperID#87, tokens#111]\\n                  +- Project [paperID#87, words#104, UDF(words#104) AS tokens#111]\\n                     +- Project [paperID#87, concat(title#98,  , abstract#99) AS words#104]\\n                        +- Project [paperID#87, coalesce(title#88, cast( as string)) AS title#98, coalesce(abstract#89, cast( as string)) AS abstract#99]\\n                           +- Project [_c0#52 AS paperID#87, _c13#65 AS title#88, _c14#66 AS abstract#89]\\n                              +- Project [_c0#52, _c13#65, _c14#66]\\n                                 +- Relation[_c0#52,_c1#53,_c2#54,_c3#55,_c4#56,_c5#57,_c6#58,_c7#59,_c8#60,_c9#61,_c10#62,_c11#63,_c12#64,_c13#65,_c14#66] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o399.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`result`' given input columns: [paperID, paper_profile];;\n'Project [paperID#127, 'result AS paper_profile#46283]\n+- Project [paperID#127, result#231 AS paper_profile#236]\n   +- Project [paperID#127, tokens#123, UDF(tokens#123) AS result#231]\n      +- Project [cast(paperID#87 as int) AS paperID#127, tokens#123]\n         +- Project [paperID#87, <lambda>(tokens#119) AS tokens#123]\n            +- Project [paperID#87, <lambda>(tokens#111) AS tokens#119]\n               +- Project [paperID#87, tokens#111]\n                  +- Project [paperID#87, words#104, UDF(words#104) AS tokens#111]\n                     +- Project [paperID#87, concat(title#98,  , abstract#99) AS words#104]\n                        +- Project [paperID#87, coalesce(title#88, cast( as string)) AS title#98, coalesce(abstract#89, cast( as string)) AS abstract#99]\n                           +- Project [_c0#52 AS paperID#87, _c13#65 AS title#88, _c14#66 AS abstract#89]\n                              +- Project [_c0#52, _c13#65, _c14#66]\n                                 +- Relation[_c0#52,_c1#53,_c2#54,_c3#55,_c4#56,_c5#57,_c6#58,_c7#59,_c8#60,_c9#61,_c10#62,_c11#63,_c12#64,_c13#65,_c14#66] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2872)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1153)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2188096300c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpaper_w2v_ip2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaper_w2v_cp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paperID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paper_profile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpaper_w2v_ip2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# def getUserProfile(user_df, preprocessed_df):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     user_doc = user_df.join(preprocessed_df, ['paperID']).orderBy(\"userID\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \"\"\"\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`result`' given input columns: [paperID, paper_profile];;\\n'Project [paperID#127, 'result AS paper_profile#46283]\\n+- Project [paperID#127, result#231 AS paper_profile#236]\\n   +- Project [paperID#127, tokens#123, UDF(tokens#123) AS result#231]\\n      +- Project [cast(paperID#87 as int) AS paperID#127, tokens#123]\\n         +- Project [paperID#87, <lambda>(tokens#119) AS tokens#123]\\n            +- Project [paperID#87, <lambda>(tokens#111) AS tokens#119]\\n               +- Project [paperID#87, tokens#111]\\n                  +- Project [paperID#87, words#104, UDF(words#104) AS tokens#111]\\n                     +- Project [paperID#87, concat(title#98,  , abstract#99) AS words#104]\\n                        +- Project [paperID#87, coalesce(title#88, cast( as string)) AS title#98, coalesce(abstract#89, cast( as string)) AS abstract#99]\\n                           +- Project [_c0#52 AS paperID#87, _c13#65 AS title#88, _c14#66 AS abstract#89]\\n                              +- Project [_c0#52, _c13#65, _c14#66]\\n                                 +- Relation[_c0#52,_c1#53,_c2#54,_c3#55,_c4#56,_c5#57,_c6#58,_c7#59,_c8#60,_c9#61,_c10#62,_c11#63,_c12#64,_c13#65,_c14#66] csv\\n\""
     ]
    }
   ],
   "source": [
    "paper_w2v_ip2 = paper_w2v_cp.select(\"paperID\", col(\"result\").alias(\"paper_profile\"))\n",
    "paper_w2v_ip2.show()\n",
    "\n",
    "# def getUserProfile(user_df, preprocessed_df):\n",
    "#     user_doc = user_df.join(preprocessed_df, ['paperID']).orderBy(\"userID\")\n",
    "#     user_doc = user_doc.drop(\"paperID\")\n",
    "#     # Concatenate the words of the user library\n",
    "#     user_doc = user_doc.rdd.map(lambda user_doc: (user_doc.userID, user_doc.tokens)).reduceByKey(lambda x,y: x + y).toDF(['userID','tokens'])\n",
    "    \n",
    "#     # Use word2Vec model fitting\n",
    "#     model_user = word2Vec.fit(user_doc)\n",
    "#     user_w2v_cp = model_user.transform(user_doc)\n",
    "#     user_w2v_cp = user_w2v_cp.select(\"userID\", col(\"result\").alias(\"user_profile\"))\n",
    "#     return user_w2v_cp\n",
    "\n",
    "# user_w2v_ip = getUserProfile(user_df, intensive_df)\n",
    "# user_w2v_cp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensive_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conservative_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
