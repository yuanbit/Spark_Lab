{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-05\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex5').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "#from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "# df to be used in 4.4\n",
    "sampled_users = user_df\n",
    "\n",
    "user_df_pre = user_df\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "w_df = spark.read.csv('./papers.csv')\n",
    "#w_df = spark.read.csv('./paper0.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "#w_df.show()\n",
    "\n",
    "# # Transform the distinct paperIDs dataframe to a list\n",
    "# paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# # Map each distinct paper into int\n",
    "# paper_list = list(map(int, paper_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 1 (Pre-processing Text for word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Conservative pre-processing ###################################################\n",
    "\n",
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "conservative_df = df\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "model = word2Vec.fit(conservative_df)\n",
    "\n",
    "\n",
    "print(\"top10 most similar words to “science” using conservative pre-processing\")\n",
    "print()\n",
    "synonyms = model.findSynonyms('science', 10)\n",
    "synonyms.show()\n",
    "\n",
    "#################################### Intensive pre-processing #####################################################\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('tokens', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"tokens\")\n",
    "\n",
    "intensive_df = df\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, inputCol=\"tokens\", outputCol=\"result\")\n",
    "model2 = word2Vec.fit(intensive_df)\n",
    "\n",
    "print(\"top10 most similar words to “science” using intensive pre-processing\")\n",
    "print()\n",
    "synonyms2 = model2.findSynonyms(ps.stem('scienc'), 10)\n",
    "synonyms2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results for intensive pre-processing are better (the similarity scores are higher) because unimportant words are removed and words with the same stems are shown in the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. 2 (Analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def analogy(word1, word2, word3, model):\n",
    "    vec = model.getVectors()\n",
    "    words = [word1, word2, word3]\n",
    "    keywords = []\n",
    "    vectors = []\n",
    "    for i in words:\n",
    "        i = ps.stem(i)\n",
    "        keywords.append([x.lower().strip() for x in re.split(\"[^A-Za-z]+\", i)]) \n",
    "    \n",
    "    for query in keywords:\n",
    "        if len(query) > 1:\n",
    "            tmp_vec = []\n",
    "            for word in query:\n",
    "                tmp_vec.append(vec.where(vec.word==word).head()[1])\n",
    "            vectors.append(DenseVector(np.average(tmp_vec, axis=0)))\n",
    "        else:\n",
    "            vectors.append(vec.where(vec.word==query[0]).head()[1])\n",
    "            \n",
    "    w = vectors[0] - vectors[1] + vectors[2]\n",
    "    result = model.findSynonyms((-1)*w,5)\n",
    "    return result\n",
    "\n",
    "print(\"analogy for the conservative pre-processing model:\")\n",
    "print()\n",
    "analogy(\"machine learning\", \"prediction\", \"recommender systems\", model).show()\n",
    "\n",
    "print(\"analogy for the intensive pre-processing model:\")\n",
    "print()\n",
    "analogy(\"machine learning\", \"prediction\", \"recommender systems\", model2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "df = df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "#important_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              userID| paperID|\n",
      "+--------------------+--------+\n",
      "|589b870a611c25fa9...| 9485312|\n",
      "|589b870a611c25fa9...|14120961|\n",
      "|589b870a611c25fa9...| 9676804|\n",
      "|589b870a611c25fa9...|  604166|\n",
      "|589b870a611c25fa9...| 2526216|\n",
      "|589b870a611c25fa9...| 6804489|\n",
      "|589b870a611c25fa9...| 1610761|\n",
      "|589b870a611c25fa9...|10049547|\n",
      "|589b870a611c25fa9...|11733005|\n",
      "|589b870a611c25fa9...| 4186128|\n",
      "|589b870a611c25fa9...|  970776|\n",
      "|589b870a611c25fa9...|  118812|\n",
      "|589b870a611c25fa9...|12306465|\n",
      "|589b870a611c25fa9...| 5317666|\n",
      "|589b870a611c25fa9...| 7152677|\n",
      "|589b870a611c25fa9...| 6392873|\n",
      "|589b870a611c25fa9...|13223978|\n",
      "|589b870a611c25fa9...| 6967343|\n",
      "|589b870a611c25fa9...|  255030|\n",
      "|589b870a611c25fa9...| 8020025|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "unrated_df = user_df_pre.withColumn(\"unrated\", get_unrated(user_df_pre.paperID))\n",
    "unrated_df = unrated_df.drop(\"paperID\")\n",
    "unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "unrated_df = unrated_df.drop(\"unrated\")\n",
    "\n",
    "unrated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| − 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")\n",
    "#terms_index_hash.show()\n",
    "\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "#print(num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\"))\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "# tf_df is now a df with a column of paperID and a column of lists of the terms (unexploded)\n",
    "#tf_df.show()\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "vector_df = model.transform(tf_df)\n",
    "vector_df = vector_df.select(\"paperID\", col(\"vectors\").alias(\"term_frequency_sparse\"))\n",
    "\n",
    "# Term frequency vector df\n",
    "#vector_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userId|        user_profile|\n",
      "+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|[12.3235197123524...|\n",
      "|589b870a611c25fa9...|[0.0,0.0,0.0,0.0,...|\n",
      "|90f1a3e6fcdbf9bc5...|[0.0,0.0,24.05779...|\n",
      "|0e3bcf22520d804fe...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################## TF-IDF with built-in function ##########################################\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "#tf_idf_built_in.show()\n",
    "\n",
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "user_profile = tf_idf_built_in.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "user_profile = user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "user_profile = user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n",
    "\n",
    "user_profile.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dense vectors to sparse\n",
    "def to_sparse(x):        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(x)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in x if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "\n",
    "to_sparse_udf = udf(lambda x: to_sparse(x), VectorUDT())\n",
    "\n",
    "# Conver the feature vectors to sparse vectors\n",
    "user_profile = user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "#user_profile.show()\n",
    "\n",
    "# Join with the user profile with their unrated papers from unrated_df\n",
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "# df is now a dataframe with userID, paperID, user_profile, and paper_profile\n",
    "\n",
    "#df.show()\n",
    "\n",
    "# Select a user\n",
    "df_selected = df.where(df.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "#df_selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 2 (Content-based recommendations: similarity metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 3 (Content-based recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------------------------------+\n",
      "|userID                          |top_papers                                 |\n",
      "+--------------------------------+-------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|11733005, 8336239, 7010764, 2887105, 115945|\n",
      "+--------------------------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cbrs(u, k):\n",
    "    # Apply similarity metric to the user_profile and paper_profile\n",
    "    sim_df = u.withColumn('Similarity', compute_sim(u.user_profile, u.paper_profile))\n",
    "    # Partition by userID and order by the similarity in descending order\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    # Add row numbers to the rows and get the top-k rows\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "    # Renaming\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    cbrs_df = get_r.select(\"userID\", \"paperID\")\n",
    "    # un-explode, concatenate the recommended papers for each user\n",
    "    cbrs_df = cbrs_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(cbrs_df.paperID)).alias(\"top_papers\"))\n",
    "    \n",
    "    return cbrs_df\n",
    "\n",
    "k = 5\n",
    "\n",
    "user_rec = cbrs(df_selected, k).show(truncate=False)\n",
    "\n",
    "#print(\"The top \" + str(k) + \" papers for \" + str(user_rec.head()[0]) + \" are \" + str(user_rec.head()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA ###\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# just rename the column to use it with the built-in methods\n",
    "termFrequencyVector = vector_df.select('paperId', col('term_frequency_sparse').alias('features'))\n",
    "# Trains a LDA model\n",
    "# set k=40 to have 40 different topics\n",
    "lda = LDA(k=40)\n",
    "model = lda.fit(termFrequencyVector)\n",
    "\n",
    "# each topic is described by 5 terms\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "#topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# it shows the probabilty of each topic for each paper\n",
    "transformed = model.transform(termFrequencyVector)\n",
    "#transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "\n",
    "lda_user_profile = transformed.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "lda_user_profile = lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "lda_user_profile = lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "lda_user_profile = lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "#lda_user_profile.show()\n",
    "\n",
    "df_lda = unrated_df.join(transformed, [\"paperID\"]).join(lda_user_profile, [\"userID\"])\n",
    "\n",
    "df_lda = df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "#df_lda.show()\n",
    "\n",
    "df_selected_lda = df_lda.where(df_lda.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "#df_selected_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------------------+\n",
      "|userID                          |top_papers                                  |\n",
      "+--------------------------------+--------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|7164691, 9563857, 1363828, 11733005, 8336239|\n",
      "+--------------------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbrs(df_selected_lda, 5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 4 Sampling and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+---------+\n",
      "|              userID|             paperID|libSize|trainSize|\n",
      "+--------------------+--------------------+-------+---------+\n",
      "|1eac022a97d683eac...|[3973229, 322433,...|    321|      256|\n",
      "|589b870a611c25fa9...|[1283233, 1305474...|      8|        6|\n",
      "+--------------------+--------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "\n",
    "num_user = 2\n",
    "\n",
    "# Order the users randomly and get n sampled users\n",
    "sampled_users = user_df_pre.orderBy(rand()).limit(num_user)\n",
    "\n",
    "# Get length of each list in a column\n",
    "get_len_udf = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "# Get library size for each user\n",
    "sampled_users = sampled_users.withColumn(\"libSize\", get_len_udf(\"paperID\"))\n",
    "\n",
    "# Get the size of the training set\n",
    "get_train = udf(lambda x: int(x*0.8), IntegerType())\n",
    "\n",
    "# Get size of the training set for each user\n",
    "sampled_users = sampled_users.withColumn(\"trainSize\", get_train(\"libSize\"))\n",
    "\n",
    "sampled_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the paperIDs for each user\n",
    "sampled_exploded = sampled_users.withColumn(\"paperID\", explode(col(\"paperID\")))\n",
    "\n",
    "# Partion by userID and order them randomly\n",
    "window = Window.partitionBy(col(\"userID\")).orderBy(rand())\n",
    "\n",
    "# Get row numbers\n",
    "sampled_exploded = sampled_exploded.select(col('*'), row_number().over(window).alias('row_number'))\n",
    "\n",
    "# Get the rows less than or equal to the training set size\n",
    "# The rows will be different each time because of .orderBy(rand()) in the window function\n",
    "training_df = sampled_exploded.where(col('row_number') <= col(\"trainSize\"))\n",
    "training_df = training_df.select(\"userID\", \"paperID\").orderBy(\"userID\")\n",
    "#training_df.show()\n",
    "\n",
    "# Get the test set by selecting the rows greater than the training size\n",
    "test_df = sampled_exploded.where(col('row_number') > col(\"trainSize\"))\n",
    "test_df = test_df.select(\"userID\", \"paperID\").orderBy(\"userID\")\n",
    "#test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "training_user_profile = tf_idf_built_in.join(training_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "training_user_profile = training_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "# Same steps as above\n",
    "training_user_profile = training_user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n",
    "\n",
    "training_user_profile = training_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "train_df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(training_user_profile, [\"userID\"])\n",
    "\n",
    "train_df = train_df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "train_df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(training_user_profile, [\"userID\"])\n",
    "\n",
    "train_df = train_df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\")).orderBy(\"userID\")\n",
    "\n",
    "#train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "training_lda_user_profile = transformed.join(training_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "training_lda_user_profile = training_lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "training_lda_user_profile = training_lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "training_lda_user_profile = training_lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "train_df_lda = unrated_df.join(transformed, [\"paperID\"]).join(training_lda_user_profile, [\"userID\"])\n",
    "\n",
    "train_df_lda = train_df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\")).orderBy(\"userID\")\n",
    "\n",
    "#train_df_lda.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 5 (Off-line evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF recommender\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                               |\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[11733005, 7010764, 8336239, 115945, 7496675, 2887105, 8310458, 945604, 1305474, 9045137]|\n",
      "|589b870a611c25fa99bd3d7295ac0622|[2887105, 7496675, 8336239, 9563857, 7010764, 1042553, 7164691, 9045137, 255030, 3010240]|\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "LDA recommender\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "|userID                          |top_papers                                                                                |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "|1eac022a97d683eace8815545ce3153f|[7164691, 9563857, 1363828, 8336239, 11733005, 3728173, 115945, 2887105, 9045137, 8310458]|\n",
      "|589b870a611c25fa99bd3d7295ac0622|[255030, 7164691, 238188, 9563857, 1363828, 7010764, 3728173, 11733005, 8336239, 115945]  |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a) Generate 10 recommendations\n",
    "\n",
    "def castToArray(df, colName):\n",
    "    dff = df.withColumn(colName, split(col(colName), \", \").cast(ArrayType(IntegerType())))\n",
    "    return dff\n",
    "\n",
    "k = 10\n",
    "\n",
    "# TF-IDF\n",
    "print(\"TF-IDF recommender\")\n",
    "tf_rec = cbrs(train_df, k).orderBy(\"userID\")\n",
    "# Cast the recommendations to a list of integers\n",
    "tf_rec  = castToArray(tf_rec, \"top_papers\")\n",
    "tf_rec.show(truncate=False)\n",
    "\n",
    "print(\"LDA recommender\")\n",
    "# LDA\n",
    "lda_rec  = cbrs(train_df_lda, k).orderBy(\"userID\")\n",
    "# Cast the recommendations to a list of integers\n",
    "lda_rec  = castToArray(lda_rec, \"top_papers\")\n",
    "lda_rec.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|             paperID|\n",
      "+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|[3469193, 600359,...|\n",
      "|589b870a611c25fa9...|   [1283233, 956315]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the test set into a list of integers\n",
    "test_df_collected = test_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(test_df.paperID)).alias(\"paperID\"))\n",
    "test_df_collected = test_df_collected.withColumn(\"paperID\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"paperID\")).orderBy(\"userID\")\n",
    "\n",
    "test_df_collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tf_test = test_df_collected.join(tf_rec, \"userID\")\n",
    "joined_tf_test = joined_tf_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "\n",
    "joined_lda_test = test_df_collected.join(lda_rec, \"userID\")\n",
    "joined_lda_test = joined_lda_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF recommender\n",
      "+--------------------------------+----+\n",
      "|userID                          |Hits|\n",
      "+--------------------------------+----+\n",
      "|1eac022a97d683eace8815545ce3153f|[]  |\n",
      "|589b870a611c25fa99bd3d7295ac0622|[]  |\n",
      "+--------------------------------+----+\n",
      "\n",
      "LDA recommender\n",
      "+--------------------------------+----+\n",
      "|userID                          |Hits|\n",
      "+--------------------------------+----+\n",
      "|1eac022a97d683eace8815545ce3153f|[]  |\n",
      "|589b870a611c25fa99bd3d7295ac0622|[]  |\n",
      "+--------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getHits(train, test):\n",
    "    return list(set(train).intersection(test))\n",
    "\n",
    "getHits_udf = udf(getHits, ArrayType(IntegerType()))\n",
    "# TF-IDF\n",
    "print(\"TF-IDF recommender\")\n",
    "tf_hits = joined_tf_test.withColumn('Hits', getHits_udf(joined_tf_test.train_set, joined_tf_test.test_set))\n",
    "tf_hits = tf_hits.select(\"userID\", \"Hits\")\n",
    "tf_hits.show(truncate=False)\n",
    "\n",
    "print(\"LDA recommender\")\n",
    "# LDA\n",
    "lda_hits = joined_lda_test.withColumn('Hits', getHits_udf(joined_lda_test.train_set, joined_lda_test.test_set))\n",
    "lda_hits = lda_hits.select(\"userID\", \"Hits\")\n",
    "lda_hits.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we could not get results for the hits, we modified the results to show our computations for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              userID|     Hits|\n",
      "+--------------------+---------+\n",
      "|               user0|[1, 2, 3]|\n",
      "|               user1|[4, 5, 6]|\n",
      "|1eac022a97d683eac...|       []|\n",
      "|589b870a611c25fa9...|       []|\n",
      "+--------------------+---------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              userID|             paperID|\n",
      "+--------------------+--------------------+\n",
      "|               user0|     [2, 3, 4, 1, 5]|\n",
      "|               user1|[5, 4, 6, 7, 8, 9...|\n",
      "|1eac022a97d683eac...|[3469193, 600359,...|\n",
      "|589b870a611c25fa9...|   [1283233, 956315]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########## example #####################\n",
    "\n",
    "columns = ['userID', 'Hits']\n",
    "vals = [(\"user0\", \"1, 2, 3\"), (\"user1\", \"4, 5, 6\")]\n",
    "\n",
    "ex = sqlContext.createDataFrame(vals, columns)\n",
    "ex = castToArray(ex, \"Hits\")\n",
    "\n",
    "ex_hits = ex.union(tf_hits)\n",
    "ex_hits.show()\n",
    "\n",
    "columns2 = ['userID', 'paperID']\n",
    "vals2 = [(\"user0\", \"2, 3, 4, 1, 5\"), (\"user1\", \"5, 4, 6, 7, 8, 9, 10\")]\n",
    "ex2 = sqlContext.createDataFrame(vals2, columns2)\n",
    "ex2 = castToArray(ex2, \"paperID\")\n",
    "\n",
    "ex_test = ex2.union(test_df_collected)\n",
    "ex_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|              userID|             paperID|     Hits|\n",
      "+--------------------+--------------------+---------+\n",
      "|               user1|[5, 4, 6, 7, 8, 9...|[4, 5, 6]|\n",
      "|1eac022a97d683eac...|[3469193, 600359,...|       []|\n",
      "|589b870a611c25fa9...|   [1283233, 956315]|       []|\n",
      "|               user0|     [2, 3, 4, 1, 5]|[1, 2, 3]|\n",
      "+--------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_test_hits = ex_test.join(ex_hits, \"userID\")\n",
    "joined_test_hits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision@10 for TF-IDF is: 0.15\n"
     ]
    }
   ],
   "source": [
    "num_user = 4\n",
    "\n",
    "def hitSize_k(hits):\n",
    "    return len(hits)/k\n",
    "\n",
    "hitSize_k_udf = udf(lambda x: hitSize_k(x), FloatType())\n",
    "\n",
    "def precisionK(df):\n",
    "    df = df.withColumn(\"hitSize_k\", hitSize_k_udf(\"Hits\"))\n",
    "    sumHits_k = df.select(f.sum(\"hitSize_k\")).collect()[0][0]\n",
    "    precision = (1/num_user)*sumHits_k\n",
    "    return precision\n",
    "\n",
    "print(\"The precision@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % precisionK(ex_hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Recall@10 for TF-IDF is: 0.26\n"
     ]
    }
   ],
   "source": [
    "def hitSize_testSize(hits, testSize):\n",
    "    return len(hits)/len(testSize)\n",
    "\n",
    "hitSize_testSize_udf = udf(hitSize_testSize, FloatType())\n",
    "\n",
    "def recallK(df):\n",
    "    df = df.withColumn(\"hitSize_testSize\", hitSize_testSize_udf(df.Hits, df.paperID))\n",
    "    sumHits_test = df.select(f.sum(\"hitSize_testSize\")).collect()[0][0]\n",
    "    recall = (1/num_user)*sumHits_test\n",
    "    return recall\n",
    "\n",
    "print(\"The Recall@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % recallK(joined_test_hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MRR@10 for TF-IDF is: 0.33\n"
     ]
    }
   ],
   "source": [
    "def getPositionU(hits, test):\n",
    "    if not hits:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1/test.index(hits[0])\n",
    "\n",
    "getPositionU_udf = udf(getPositionU, FloatType())\n",
    "\n",
    "def mrrK(df):\n",
    "    df = df.withColumn(\"P_u\", getPositionU_udf(df.Hits, df.paperID))\n",
    "    sumP_u = df.select(f.sum(\"P_u\")).collect()[0][0]\n",
    "    mrr = (1/num_user)*sumP_u\n",
    "    return mrr\n",
    "\n",
    "print(\"The MRR@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % mrrK(joined_test_hits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we could not get good results for the hits (empty), we computed the precision for k in {10, 20}\n",
    "#### We could change the 21 in the range to 101 to compute the precision up till k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision@10 for TF-IDF is: 0.00\n",
      "The Recall@10 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@11 for TF-IDF is: 0.00\n",
      "The Recall@11 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@12 for TF-IDF is: 0.00\n",
      "The Recall@12 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@13 for TF-IDF is: 0.00\n",
      "The Recall@13 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@14 for TF-IDF is: 0.00\n",
      "The Recall@14 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@15 for TF-IDF is: 0.00\n",
      "The Recall@15 for TF-IDF is: 0.00\n",
      "\n",
      "The precision@16 for TF-IDF is: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e0f967061c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The precision@\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" for TF-IDF is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprecisionK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_hits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Recall@\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" for TF-IDF is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrecallK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_hits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-761038ac5ddc>\u001b[0m in \u001b[0;36mrecallK\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecallK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hitSize_testSize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhitSize_testSize_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaperID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msumHits_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hitSize_testSize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msumHits_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_user = 2\n",
    "\n",
    "for k in range(10, 21):\n",
    "    # TF-IDF\n",
    "    tf_rec = cbrs(train_df, k).orderBy(\"userID\")\n",
    "    tf_rec  = castToArray(tf_rec, \"top_papers\")\n",
    "    joined_tf_test = test_df_collected.join(tf_rec, \"userID\")\n",
    "    joined_tf_test = joined_tf_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "    tf_hits = joined_tf_test.withColumn('Hits', getHits_udf(joined_tf_test.train_set, joined_tf_test.test_set))\n",
    "    tf_hits = tf_hits.select(\"userID\", \"Hits\")\n",
    "    test_hits =  test_df_collected.join(tf_hits, \"userID\")\n",
    "    \n",
    "    print(\"The precision@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % precisionK(tf_hits)))\n",
    "    print(\"The Recall@\" + str(k) + \" for TF-IDF is: \" + (\"%.2f\" % recallK(test_hits)) + \"\\n\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10, 21):\n",
    "    # LDA\n",
    "    lda_rec  = cbrs(train_df_lda, k).orderBy(\"userID\")\n",
    "    lda_rec  = castToArray(lda_rec, \"top_papers\")\n",
    "    joined_lda_test = test_df_collected.join(lda_rec, \"userID\")\n",
    "    joined_lda_test = joined_lda_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "    lda_hits = joined_lda_test.withColumn('Hits', getHits_udf(joined_lda_test.train_set, joined_lda_test.test_set))\n",
    "    lda_hits = lda_hits.select(\"userID\", \"Hits\")\n",
    "    test_hits_lda =  test_df_collected.join(lda_hits, \"userID\")\n",
    "    \n",
    "    print(\"The precision@\" + str(k) + \" for LDA is: \" + (\"%.2f\" % precisionK(tf_hits)))\n",
    "    print(\"The Recall@\" + str(k) + \" for LDA is: \" + (\"%.2f\" % recallK(test_hits)) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
