{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-04\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "import csv\n",
    "spark = SparkSession.builder.appName('ex4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|             paperID|\n",
      "+--------------------+--------------------+\n",
      "|28d3f81251d94b097...|[3929762, 503574,...|\n",
      "|d0c9aaa788153daea...|[2080631, 6343346...|\n",
      "|f05bcffe7951de9e5...|[1158654, 478707,...|\n",
      "|ca4f1ba4094011d9a...|            [278019]|\n",
      "|d1d41a15201915503...|[6610569, 6493797...|\n",
      "|f2f77383828ea6d39...|[943458, 238121, ...|\n",
      "|9c883d02115400f7b...|[3509971, 3509965...|\n",
      "|b656009a6efdc8b1a...|[771870, 181369, ...|\n",
      "|cf9c7f356092c34be...|             [90558]|\n",
      "|0f5cbb39410a9278f...|           [9344598]|\n",
      "|d85f7d83f27b3f533...|[7610843, 3633347...|\n",
      "|586c867a0688250ac...|[464760, 466011, ...|\n",
      "|10fdfaf945d5c27ad...|           [2010550]|\n",
      "|589b870a611c25fa9...|[1283233, 1305474...|\n",
      "|90f1a3e6fcdbf9bc5...|[115945, 11733005...|\n",
      "|7e070a9da96672e05...|           [1071959]|\n",
      "|3b715ebaf1f8f81a1...|[4119394, 3378798...|\n",
      "|488fb15e8c77f8054...|[1523301, 5281566...|\n",
      "|3fdf355e59949c79d...|[7077220, 1289842...|\n",
      "|c6b59086a0bbac141...|[2230995, 3050075...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example2.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "user_df_pre = user_df\n",
    "\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "user_df_pre.show()\n",
    "\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()\n",
    "\n",
    "# # Get the number of distinct papers\n",
    "# num_papers = user_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# # Get the number of distinct users\n",
    "# num_users = user_df.select(\"userID\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./example_paper.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|              userID|paperID|\n",
      "+--------------------+-------+\n",
      "|28d3f81251d94b097...|1277953|\n",
      "|28d3f81251d94b097...|3153930|\n",
      "|28d3f81251d94b097...|3153931|\n",
      "|28d3f81251d94b097...|1388555|\n",
      "|28d3f81251d94b097...|4186128|\n",
      "|28d3f81251d94b097...|1814546|\n",
      "|28d3f81251d94b097...|7499794|\n",
      "|28d3f81251d94b097...| 921623|\n",
      "|28d3f81251d94b097...| 970776|\n",
      "|28d3f81251d94b097...| 311321|\n",
      "|28d3f81251d94b097...|7499801|\n",
      "|28d3f81251d94b097...| 118812|\n",
      "|28d3f81251d94b097...| 290846|\n",
      "|28d3f81251d94b097...|4067359|\n",
      "|28d3f81251d94b097...| 688160|\n",
      "|28d3f81251d94b097...|1118240|\n",
      "|28d3f81251d94b097...|  81954|\n",
      "|28d3f81251d94b097...|4460578|\n",
      "|28d3f81251d94b097...| 340004|\n",
      "|28d3f81251d94b097...|2363430|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "unrated_df = user_df_pre.withColumn(\"unrated\", get_unrated(user_df_pre.paperID))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"paperID\")\n",
    "\n",
    "unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"unrated\")\n",
    "\n",
    "unrated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 1 Vector representation for the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17677\n",
      "+--------+-----+\n",
      "|   terms|count|\n",
      "+--------+-----+\n",
      "|   motif|   29|\n",
      "| discuss|   29|\n",
      "|  improv|   29|\n",
      "| similar|   29|\n",
      "|   error|   29|\n",
      "|properti|   29|\n",
      "|    test|   29|\n",
      "|   scale|   28|\n",
      "| suggest|   28|\n",
      "|    rate|   28|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('stemmed', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"stemmed\")\n",
    "\n",
    "# Create a new df to store the paperID and stemmed tokens\n",
    "paper_terms = df\n",
    "\n",
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "df = df.select(\"paperID\", f.explode(\"stemmed\").alias(\"tokens\"))\n",
    "\n",
    "#.distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "df = df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "important_words.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|index|      terms|\n",
      "+-----+-----------+\n",
      "|    0|       code|\n",
      "|    1|      major|\n",
      "|    2|     author|\n",
      "|    3|      defin|\n",
      "|    4|       year|\n",
      "|    5|    insight|\n",
      "|    6|bioinformat|\n",
      "|    7|     captur|\n",
      "|    8|       rang|\n",
      "|    9|      build|\n",
      "|   10|   demonstr|\n",
      "|   11|      singl|\n",
      "|   12|     analys|\n",
      "|   13|     observ|\n",
      "|   14|      local|\n",
      "|   15|    potenti|\n",
      "|   16|   interest|\n",
      "|   17|    control|\n",
      "|   18|     articl|\n",
      "|   19|  scientist|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| − 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")\n",
    "\n",
    "terms_index_hash.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# Get the number of distinct terms\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|         terms_index|\n",
      "+-------+--------------------+\n",
      "| 159967|[5, 44, 52, 52, 6...|\n",
      "|2212959|     [4, 44, 72, 72]|\n",
      "| 333353|[20, 33, 71, 71, ...|\n",
      "| 438129|[18, 29, 43, 43, ...|\n",
      "| 166220|       [6, 6, 6, 70]|\n",
      "|2883810|             [5, 76]|\n",
      "|1288940|[7, 13, 34, 46, 4...|\n",
      "|5251453|[2, 2, 2, 2, 12, ...|\n",
      "|7515828|             [5, 76]|\n",
      "|2739852|[4, 5, 13, 23, 26...|\n",
      "|5961524|[16, 24, 33, 38, ...|\n",
      "|    272|[8, 12, 14, 26, 2...|\n",
      "|  77265|            [45, 47]|\n",
      "| 820297|[4, 18, 30, 31, 3...|\n",
      "|1042553|[15, 17, 17, 17, ...|\n",
      "|    154|                [46]|\n",
      "|2883820|                [48]|\n",
      "|5434882|[13, 18, 19, 19, ...|\n",
      "|1332540|                [42]|\n",
      "|1777140|            [52, 54]|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split (explode) the list of words into a column of tokens and\n",
    "#count the number of occurences of the tokens per paper\n",
    "#p_terms is a df with paperID, terms, and count\n",
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\"))\n",
    "#.groupBy(\"paperID\", \"terms\").count()\n",
    "#p_terms = p_terms.orderBy(\"paperID\", \"count\")\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "# Drop the terms because now they are represented by the indices\n",
    "joined_df = joined_df.drop(\"terms\")\n",
    "joined_df  = joined_df.orderBy(\"paperID\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.index)).alias(\"index\"))\n",
    "tf_df = tf_df.withColumn(\"terms_index\", split(col(\"index\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"index\"))\n",
    "tf_df = tf_df.drop(\"index\")\n",
    "tf_df.show()\n",
    "\n",
    "\n",
    "# Concatenate the indices and the count (occurences in papers)\n",
    "# tf_df = tf_df.withColumn(\"index_count\", f.concat(col(\"index\"), lit(\", \"), col(\"count\")))\n",
    "# tf_df = tf_df.drop(\"index\", \"count\")\n",
    "\n",
    "# # Concatenate the index_counts per paperID (\"un-explode\")\n",
    "# tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.index_count)).alias(\"index_count\"))\n",
    "# # Create a new column and casting the index_count into an array with integer type\n",
    "# # The terms_count of the tf_df column is now a list where the odd positions are the terms indices and the even positions\n",
    "# tf_df = tf_df.withColumn(\"terms_count\", split(col(\"index_count\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"terms_count\"))\n",
    "# tf_df = tf_df.drop(\"index_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "+-------+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
      "|paperID|terms_index                                                                                                 |vectors                                                                                                |\n",
      "+-------+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
      "|159967 |[5, 44, 52, 52, 63, 79, 84, 87, 90]                                                                         |(91,[0,1,3,7,31,35,49,79],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])                                           |\n",
      "|2212959|[4, 44, 72, 72]                                                                                             |(91,[12,49,78],[2.0,1.0,1.0])                                                                          |\n",
      "|333353 |[20, 33, 71, 71, 71, 71, 71, 76, 80, 84, 84, 84, 84, 84, 86, 89]                                            |(91,[1,2,6,8,14,16,59,75],[5.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0])                                           |\n",
      "|438129 |[18, 29, 43, 43, 45, 68, 73]                                                                                |(91,[13,22,43,48,64,74],[1.0,1.0,2.0,1.0,1.0,1.0])                                                     |\n",
      "|166220 |[6, 6, 6, 70]                                                                                               |(91,[24,83],[1.0,3.0])                                                                                 |\n",
      "|2883810|[5, 76]                                                                                                     |(91,[16,79],[1.0,1.0])                                                                                 |\n",
      "|1288940|[7, 13, 34, 46, 48, 56, 86, 87]                                                                             |(91,[2,3,39,50,51,61,85,90],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                         |\n",
      "|5251453|[2, 2, 2, 2, 12, 21, 21, 31, 43, 53, 65, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83]                |(91,[11,33,37,43,65,70,82,89],[13.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0])                                      |\n",
      "|7515828|[5, 76]                                                                                                     |(91,[16,79],[1.0,1.0])                                                                                 |\n",
      "|2739852|[4, 5, 13, 23, 26, 27, 50, 55, 60, 60, 69, 76, 79, 80, 80, 89]                                              |(91,[6,7,8,16,23,28,34,40,58,60,71,78,79,85],[1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|5961524|[16, 24, 33, 38, 48, 66, 66, 76, 81, 81]                                                                    |(91,[9,16,21,46,51,59,67,73],[2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])                                        |\n",
      "|272    |[8, 12, 14, 26, 26, 26, 28, 32, 33, 47, 50, 50, 50, 61, 63, 79, 84]                                         |(91,[1,7,29,31,34,54,55,58,59,63,68,87,89],[1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0])      |\n",
      "|77265  |[45, 47]                                                                                                    |(91,[48,54],[1.0,1.0])                                                                                 |\n",
      "|820297 |[4, 18, 30, 31, 31, 31, 31, 42, 64]                                                                         |(91,[32,44,57,65,74,78],[1.0,1.0,1.0,4.0,1.0,1.0])                                                     |\n",
      "|1042553|[15, 17, 17, 17, 17, 17, 30, 37, 37, 37, 37, 42, 63, 63, 64, 77, 81, 81, 81, 81, 81, 81, 88, 88, 90, 90, 90]|(91,[0,5,9,17,31,32,41,44,57,69,72],[3.0,2.0,6.0,1.0,2.0,1.0,4.0,1.0,1.0,5.0,1.0])                     |\n",
      "|154    |[46]                                                                                                        |(91,[50],[1.0])                                                                                        |\n",
      "|2883820|[48]                                                                                                        |(91,[51],[1.0])                                                                                        |\n",
      "|5434882|[13, 18, 19, 19, 29, 30, 41, 68, 69, 70]                                                                    |(91,[22,23,24,42,57,64,74,76,85],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])                                |\n",
      "|1332540|[42]                                                                                                        |(91,[44],[1.0])                                                                                        |\n",
      "|1777140|[52, 54]                                                                                                    |(91,[35,38],[1.0,1.0])                                                                                 |\n",
      "+-------+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "# Get the number of distinct terms\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_index\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "x = model.transform(tf_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`terms_count`' given input columns: [index, paperID];;\\n'Project [index#1505, paperID#164, <lambda>('terms_count) AS term_frequency_sparse#1774]\\n+- Sort [paperID#164 ASC NULLS FIRST], true\\n   +- Project [index#1505, paperID#164]\\n      +- Project [terms#1361, index#1505, paperID#164]\\n         +- Join Inner, (terms#1361 = terms#1464)\\n            :- Project [row_num#1499 AS index#1505, terms#1361]\\n            :  +- Project [terms#1361, count#1349L, row_num#1499]\\n            :     +- Project [terms#1361, count#1349L, _we0#1500, (_we0#1500 - 1) AS row_num#1499]\\n            :        +- Window [row_number() windowspecdefinition(count#1349L ASC NULLS FIRST, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS _we0#1500], [count#1349L ASC NULLS FIRST]\\n            :           +- Project [terms#1361, count#1349L]\\n            :              +- Project [tokens#1313 AS terms#1361, count#1349L]\\n            :                 +- GlobalLimit 1000\\n            :                    +- LocalLimit 1000\\n            :                       +- Filter (count#1349L >= cast(20 as bigint))\\n            :                          +- Filter (count#1349L < cast(30 as bigint))\\n            :                             +- Sort [count#1349L DESC NULLS LAST], true\\n            :                                +- Project [tokens#1313, count#1349L]\\n            :                                   +- Aggregate [tokens#1313], [tokens#1313, collect_list(tokens#1313, 0, 0) AS duplicated_values#1347, sum(count#1323L) AS count#1349L]\\n            :                                      +- Project [tokens#1313, count#1323L]\\n            :                                         +- Aggregate [paperID#164, tokens#1313], [paperID#164, tokens#1313, count(1) AS count#1323L]\\n            :                                            +- Sort [paperID#164 ASC NULLS FIRST], true\\n            :                                               +- Deduplicate [paperID#164, tokens#1313], false\\n            :                                                  +- Project [paperID#164, tokens#1313]\\n            :                                                     +- Generate explode(stemmed#1304), true, false, [tokens#1313]\\n            :                                                        +- Project [paperID#164, stemmed#1304]\\n            :                                                           +- Project [paperID#164, filtered#1296, <lambda>(filtered#1296) AS stemmed#1304]\\n            :                                                              +- Project [paperID#164, filtered#1296]\\n            :                                                                 +- Project [paperID#164, tokens#1292, UDF(tokens#1292) AS filtered#1296]\\n            :                                                                    +- Project [paperID#164, <lambda>(tokens#1288) AS tokens#1292]\\n            :                                                                       +- Project [paperID#164, <lambda>(tokens#1280) AS tokens#1288]\\n            :                                                                          +- Project [paperID#164, tokens#1280]\\n            :                                                                             +- Project [paperID#164, words#181, UDF(words#181) AS tokens#1280]\\n            :                                                                                +- Project [paperID#164, concat(title#175,  , abstract#176) AS words#181]\\n            :                                                                                   +- Project [paperID#164, coalesce(title#165, cast( as string)) AS title#175, coalesce(abstract#166, cast( as string)) AS abstract#176]\\n            :                                                                                      +- Project [_c0#129 AS paperID#164, _c13#142 AS title#165, _c14#143 AS abstract#166]\\n            :                                                                                         +- Project [_c0#129, _c13#142, _c14#143]\\n            :                                                                                            +- Relation[_c0#129,_c1#130,_c2#131,_c3#132,_c4#133,_c5#134,_c6#135,_c7#136,_c8#137,_c9#138,_c10#139,_c11#140,_c12#141,_c13#142,_c14#143] csv\\n            +- Project [paperID#164, terms#1464]\\n               +- Generate explode(stemmed#1304), true, false, [terms#1464]\\n                  +- Project [paperID#164, stemmed#1304]\\n                     +- Project [paperID#164, filtered#1296, <lambda>(filtered#1296) AS stemmed#1304]\\n                        +- Project [paperID#164, filtered#1296]\\n                           +- Project [paperID#164, tokens#1292, UDF(tokens#1292) AS filtered#1296]\\n                              +- Project [paperID#164, <lambda>(tokens#1288) AS tokens#1292]\\n                                 +- Project [paperID#164, <lambda>(tokens#1280) AS tokens#1288]\\n                                    +- Project [paperID#164, tokens#1280]\\n                                       +- Project [paperID#164, words#181, UDF(words#181) AS tokens#1280]\\n                                          +- Project [paperID#164, concat(title#175,  , abstract#176) AS words#181]\\n                                             +- Project [paperID#164, coalesce(title#165, cast( as string)) AS title#175, coalesce(abstract#166, cast( as string)) AS abstract#176]\\n                                                +- Project [_c0#129 AS paperID#164, _c13#142 AS title#165, _c14#143 AS abstract#166]\\n                                                   +- Project [_c0#129, _c13#142, _c14#143]\\n                                                      +- Relation[_c0#129,_c1#130,_c2#131,_c3#132,_c4#133,_c5#134,_c6#135,_c7#136,_c8#137,_c9#138,_c10#139,_c11#140,_c12#141,_c13#142,_c14#143] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1459.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`terms_count`' given input columns: [index, paperID];;\n'Project [index#1505, paperID#164, <lambda>('terms_count) AS term_frequency_sparse#1774]\n+- Sort [paperID#164 ASC NULLS FIRST], true\n   +- Project [index#1505, paperID#164]\n      +- Project [terms#1361, index#1505, paperID#164]\n         +- Join Inner, (terms#1361 = terms#1464)\n            :- Project [row_num#1499 AS index#1505, terms#1361]\n            :  +- Project [terms#1361, count#1349L, row_num#1499]\n            :     +- Project [terms#1361, count#1349L, _we0#1500, (_we0#1500 - 1) AS row_num#1499]\n            :        +- Window [row_number() windowspecdefinition(count#1349L ASC NULLS FIRST, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS _we0#1500], [count#1349L ASC NULLS FIRST]\n            :           +- Project [terms#1361, count#1349L]\n            :              +- Project [tokens#1313 AS terms#1361, count#1349L]\n            :                 +- GlobalLimit 1000\n            :                    +- LocalLimit 1000\n            :                       +- Filter (count#1349L >= cast(20 as bigint))\n            :                          +- Filter (count#1349L < cast(30 as bigint))\n            :                             +- Sort [count#1349L DESC NULLS LAST], true\n            :                                +- Project [tokens#1313, count#1349L]\n            :                                   +- Aggregate [tokens#1313], [tokens#1313, collect_list(tokens#1313, 0, 0) AS duplicated_values#1347, sum(count#1323L) AS count#1349L]\n            :                                      +- Project [tokens#1313, count#1323L]\n            :                                         +- Aggregate [paperID#164, tokens#1313], [paperID#164, tokens#1313, count(1) AS count#1323L]\n            :                                            +- Sort [paperID#164 ASC NULLS FIRST], true\n            :                                               +- Deduplicate [paperID#164, tokens#1313], false\n            :                                                  +- Project [paperID#164, tokens#1313]\n            :                                                     +- Generate explode(stemmed#1304), true, false, [tokens#1313]\n            :                                                        +- Project [paperID#164, stemmed#1304]\n            :                                                           +- Project [paperID#164, filtered#1296, <lambda>(filtered#1296) AS stemmed#1304]\n            :                                                              +- Project [paperID#164, filtered#1296]\n            :                                                                 +- Project [paperID#164, tokens#1292, UDF(tokens#1292) AS filtered#1296]\n            :                                                                    +- Project [paperID#164, <lambda>(tokens#1288) AS tokens#1292]\n            :                                                                       +- Project [paperID#164, <lambda>(tokens#1280) AS tokens#1288]\n            :                                                                          +- Project [paperID#164, tokens#1280]\n            :                                                                             +- Project [paperID#164, words#181, UDF(words#181) AS tokens#1280]\n            :                                                                                +- Project [paperID#164, concat(title#175,  , abstract#176) AS words#181]\n            :                                                                                   +- Project [paperID#164, coalesce(title#165, cast( as string)) AS title#175, coalesce(abstract#166, cast( as string)) AS abstract#176]\n            :                                                                                      +- Project [_c0#129 AS paperID#164, _c13#142 AS title#165, _c14#143 AS abstract#166]\n            :                                                                                         +- Project [_c0#129, _c13#142, _c14#143]\n            :                                                                                            +- Relation[_c0#129,_c1#130,_c2#131,_c3#132,_c4#133,_c5#134,_c6#135,_c7#136,_c8#137,_c9#138,_c10#139,_c11#140,_c12#141,_c13#142,_c14#143] csv\n            +- Project [paperID#164, terms#1464]\n               +- Generate explode(stemmed#1304), true, false, [terms#1464]\n                  +- Project [paperID#164, stemmed#1304]\n                     +- Project [paperID#164, filtered#1296, <lambda>(filtered#1296) AS stemmed#1304]\n                        +- Project [paperID#164, filtered#1296]\n                           +- Project [paperID#164, tokens#1292, UDF(tokens#1292) AS filtered#1296]\n                              +- Project [paperID#164, <lambda>(tokens#1288) AS tokens#1292]\n                                 +- Project [paperID#164, <lambda>(tokens#1280) AS tokens#1288]\n                                    +- Project [paperID#164, tokens#1280]\n                                       +- Project [paperID#164, words#181, UDF(words#181) AS tokens#1280]\n                                          +- Project [paperID#164, concat(title#175,  , abstract#176) AS words#181]\n                                             +- Project [paperID#164, coalesce(title#165, cast( as string)) AS title#175, coalesce(abstract#166, cast( as string)) AS abstract#176]\n                                                +- Project [_c0#129 AS paperID#164, _c13#142 AS title#165, _c14#143 AS abstract#166]\n                                                   +- Project [_c0#129, _c13#142, _c14#143]\n                                                      +- Relation[_c0#129,_c1#130,_c2#131,_c3#132,_c4#133,_c5#134,_c6#135,_c7#136,_c8#137,_c9#138,_c10#139,_c11#140,_c12#141,_c13#142,_c14#143] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2872)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1153)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:1908)\n\tat sun.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-cc418da80651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# applying the udf to the terms_count column to create the term_frequency_sparse column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mvector_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'term_frequency_sparse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'terms_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mvector_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"terms_count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \"\"\"\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`terms_count`' given input columns: [index, paperID];;\\n'Project [index#1505, paperID#164, <lambda>('terms_count) AS term_frequency_sparse#1774]\\n+- Sort [paperID#164 ASC NULLS FIRST], true\\n   +- Project [index#1505, paperID#164]\\n      +- Project [terms#1361, index#1505, paperID#164]\\n         +- Join Inner, (terms#1361 = terms#1464)\\n            :- Project [row_num#1499 AS index#1505, terms#1361]\\n            :  +- Project [terms#1361, count#1349L, row_num#1499]\\n            :     +- Project [terms#1361, count#1349L, _we0#1500, (_we0#1500 - 1) AS row_num#1499]\\n            :        +- Window [row_number() windowspecdefinition(count#1349L ASC NULLS FIRST, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS _we0#1500], [count#1349L ASC NULLS FIRST]\\n            :           +- Project [terms#1361, count#1349L]\\n            :              +- Project [tokens#1313 AS terms#1361, count#1349L]\\n            :                 +- GlobalLimit 1000\\n            :                    +- LocalLimit 1000\\n            :                       +- Filter (count#1349L >= cast(20 as bigint))\\n            :                          +- Filter (count#1349L < cast(30 as bigint))\\n            :                             +- Sort [count#1349L DESC NULLS LAST], true\\n            :                                +- Project [tokens#1313, count#1349L]\\n            :                                   +- Aggregate [tokens#1313], [tokens#1313, collect_list(tokens#1313, 0, 0) AS duplicated_values#1347, sum(count#1323L) AS count#1349L]\\n            :                                      +- Project [tokens#1313, count#1323L]\\n            :                                         +- Aggregate [paperID#164, tokens#1313], [paperID#164, tokens#1313, count(1) AS count#1323L]\\n            :                                            +- Sort [paperID#164 ASC NULLS FIRST], true\\n            :                                               +- Deduplicate [paperID#164, tokens#1313], false\\n            :                                                  +- Project [paperID#164, tokens#1313]\\n            :                                                     +- Generate explode(stemmed#1304), true, false, [tokens#1313]\\n            :                                                        +- Project [paperID#164, stemmed#1304]\\n            :                                                           +- Project [paperID#164, filtered#1296, <lambda>(filtered#1296) AS stemmed#1304]\\n            :                                                              +- Project [paperID#164, filtered#1296]\\n            :                                                                 +- Project [paperID#164, tokens#1292, UDF(tokens#1292) AS filtered#1296]\\n            :                                                                    +- Project [paperID#164, <lambda>(tokens#1288) AS tokens#1292]\\n            :                                                                       +- Project [paperID#164, <lambda>(tokens#1280) AS tokens#1288]\\n            :                                                                          +- Project [paperID#164, tokens#1280]\\n            :                                                                             +- Project [paperID#164, words#181, UDF(words#181) AS tokens#1280]\\n            :                                                                                +- Project [paperID#164, concat(title#175,  , abstract#176) AS words#181]\\n            :                                                                                   +- Project [paperID#164, coalesce(title#165, cast( as string)) AS title#175, coalesce(abstract#166, cast( as string)) AS abstract#176]\\n            :                                                                                      +- Project [_c0#129 AS paperID#164, _c13#142 AS title#165, _c14#143 AS abstract#166]\\n            :                                                                                         +- Project [_c0#129, _c13#142, _c14#143]\\n            :                                                                                            +- Relation[_c0#129,_c1#130,_c2#131,_c3#132,_c4#133,_c5#134,_c6#135,_c7#136,_c8#137,_c9#138,_c10#139,_c11#140,_c12#141,_c13#142,_c14#143] csv\\n            +- Project [paperID#164, terms#1464]\\n               +- Generate explode(stemmed#1304), true, false, [terms#1464]\\n                  +- Project [paperID#164, stemmed#1304]\\n                     +- Project [paperID#164, filtered#1296, <lambda>(filtered#1296) AS stemmed#1304]\\n                        +- Project [paperID#164, filtered#1296]\\n                           +- Project [paperID#164, tokens#1292, UDF(tokens#1292) AS filtered#1296]\\n                              +- Project [paperID#164, <lambda>(tokens#1288) AS tokens#1292]\\n                                 +- Project [paperID#164, <lambda>(tokens#1280) AS tokens#1288]\\n                                    +- Project [paperID#164, tokens#1280]\\n                                       +- Project [paperID#164, words#181, UDF(words#181) AS tokens#1280]\\n                                          +- Project [paperID#164, concat(title#175,  , abstract#176) AS words#181]\\n                                             +- Project [paperID#164, coalesce(title#165, cast( as string)) AS title#175, coalesce(abstract#166, cast( as string)) AS abstract#176]\\n                                                +- Project [_c0#129 AS paperID#164, _c13#142 AS title#165, _c14#143 AS abstract#166]\\n                                                   +- Project [_c0#129, _c13#142, _c14#143]\\n                                                      +- Relation[_c0#129,_c1#130,_c2#131,_c3#132,_c4#133,_c5#134,_c6#135,_c7#136,_c8#137,_c9#138,_c10#139,_c11#140,_c12#141,_c13#142,_c14#143] csv\\n\""
     ]
    }
   ],
   "source": [
    "# udf to get the sparse vector of term frequency\n",
    "def vector_list_map(x):\n",
    "    # a list with size of number of terms and fill it with zeroes \n",
    "\n",
    "    mylist = [0] * num_terms\n",
    "    for i in range(0, len(x), 2):\n",
    "        mylist[int(x[i])] = x[i+1]\n",
    "        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(mylist)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in mylist if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "           \n",
    "vector_map = udf(lambda x: vector_list_map(x), VectorUDT())\n",
    "\n",
    "# applying the udf to the terms_count column to create the term_frequency_sparse column\n",
    "vector_df = tf_df.withColumn('term_frequency_sparse', vector_map(col('terms_count')))\n",
    "\n",
    "vector_df = vector_df.drop(\"terms_count\")\n",
    "\n",
    "#vector_df.show(10, truncate=False)\n",
    "\n",
    "vector_df.where(vector_df.paperID==\"80546\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|            features|\n",
      "+-------+--------------------+\n",
      "| 159967|(64,[6,16,17,21,2...|\n",
      "|2212959|(64,[6,26,48],[2....|\n",
      "| 333353|(64,[2,7,10,19,24...|\n",
      "| 438129|(64,[4,30],[2.542...|\n",
      "| 166220|(64,[20,29,37,38,...|\n",
      "|2883810|(64,[24,59],[2.45...|\n",
      "|1288940|(64,[8,25,39,46,5...|\n",
      "|5251453|(64,[5,15,22,29,3...|\n",
      "|7515828|(64,[24,34,59],[2...|\n",
      "|2739852|(64,[3,9,10,12,14...|\n",
      "|5961524|(64,[1,2,7,8,24],...|\n",
      "|    272|(64,[7,17,23,28,4...|\n",
      "|6573750|(64,[18,55],[2.49...|\n",
      "|  77265|(64,[2,56],[2.542...|\n",
      "| 820297|(64,[11,19,35],[2...|\n",
      "|1042553|(64,[17,21,27,35,...|\n",
      "|2883820|(64,[8,50,57],[2....|\n",
      "|5434882|(64,[2,4,29,37,45...|\n",
      "|1332540|(64,[35],[2.40919...|\n",
      "|1777140|(64,[16,41,48,49,...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TF-IDF with built-in function ###\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "tf_idf_built_in.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "user_profile = tf_idf_built_in.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "user_profile = user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "user_profile = user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userId|        user_profile|\n",
      "+--------------------+--------------------+\n",
      "|d503571e44a0373eb...|(64,[2,10,12,18,2...|\n",
      "|f1e1cd4ff25018273...|(64,[3,8,20,29,38...|\n",
      "|bbcd9dae3160ddcb9...|(64,[3,12,19,29,3...|\n",
      "|a0bbf6bb9b1c818f3...|(64,[29,32,60],[2...|\n",
      "|1eac022a97d683eac...|(64,[1,4,10,11,13...|\n",
      "|cf9c7f356092c34be...|(64,[1,9,10,14,18...|\n",
      "|3b715ebaf1f8f81a1...|(64,[3,6,9,10,14,...|\n",
      "|b36c3189bb1457cd0...|(64,[24],[2.45175...|\n",
      "|f3c28e50db4ce8ad8...|(64,[2,10,12,18,2...|\n",
      "|b656009a6efdc8b1a...|(64,[0,2,4,5,9,10...|\n",
      "|d85f7d83f27b3f533...|(64,[0,8,24,29,44...|\n",
      "|f05bcffe7951de9e5...|(64,[1,3,4,7,10,1...|\n",
      "|4c8912d1b04471cf5...|(64,[1,4,10,11,13...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_sparse(x):        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(x)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in x if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "\n",
    "to_sparse_udf = udf(lambda x: to_sparse(x), VectorUDT())\n",
    "\n",
    "user_profile = user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+\n",
      "|              userID|paperID|        user_profile|       paper_profile|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "|f1e1cd4ff25018273...| 166220|(64,[3,8,20,29,38...|(64,[20,29,37,38,...|\n",
      "|f1e1cd4ff25018273...| 820297|(64,[3,8,20,29,38...|(64,[11,19,35],[2...|\n",
      "|f1e1cd4ff25018273...|1042553|(64,[3,8,20,29,38...|(64,[17,21,27,35,...|\n",
      "|f1e1cd4ff25018273...|3010240|(64,[3,8,20,29,38...|(64,[29,31],[2.45...|\n",
      "|f1e1cd4ff25018273...| 523772|(64,[3,8,20,29,38...|(64,[35],[2.40919...|\n",
      "|f1e1cd4ff25018273...|1279898|(64,[3,8,20,29,38...|(64,[24,33,51,56]...|\n",
      "|f1e1cd4ff25018273...| 920055|(64,[3,8,20,29,38...|(64,[24,38,59],[2...|\n",
      "|f1e1cd4ff25018273...| 965334|(64,[3,8,20,29,38...|(64,[2,51,54,59],...|\n",
      "|f1e1cd4ff25018273...|     99|(64,[3,8,20,29,38...|(64,[7,17,18,20,2...|\n",
      "|f1e1cd4ff25018273...|7355647|(64,[3,8,20,29,38...|(64,[3,12,41,43,4...|\n",
      "|f1e1cd4ff25018273...|   4280|(64,[3,8,20,29,38...|(64,[24],[2.45175...|\n",
      "|f1e1cd4ff25018273...|  72879|(64,[3,8,20,29,38...|(64,[24,47,50,60]...|\n",
      "|f1e1cd4ff25018273...|  90558|(64,[3,8,20,29,38...|(64,[1,9,10,14,18...|\n",
      "|f1e1cd4ff25018273...|3614773|(64,[3,8,20,29,38...|(64,[49,57,63],[2...|\n",
      "|f1e1cd4ff25018273...|3281478|(64,[3,8,20,29,38...|(64,[14,28,46,47,...|\n",
      "|f1e1cd4ff25018273...| 849862|(64,[3,8,20,29,38...|(64,[21,52],[7.48...|\n",
      "|f1e1cd4ff25018273...|5394760|(64,[3,8,20,29,38...|(64,[6,15,37,38],...|\n",
      "|f1e1cd4ff25018273...|    268|(64,[3,8,20,29,38...|(64,[24,55],[2.45...|\n",
      "|f1e1cd4ff25018273...| 531300|(64,[3,8,20,29,38...|(64,[10,62],[2.54...|\n",
      "|f1e1cd4ff25018273...|1453145|(64,[3,8,20,29,38...|(64,[5,9,59,61],[...|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+\n",
      "|              userID| paperID|        user_profile|       paper_profile|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|  166220|(64,[1,4,10,11,13...|(64,[20,29,37,38,...|\n",
      "|1eac022a97d683eac...|  820297|(64,[1,4,10,11,13...|(64,[11,19,35],[2...|\n",
      "|1eac022a97d683eac...| 1279898|(64,[1,4,10,11,13...|(64,[24,33,51,56]...|\n",
      "|1eac022a97d683eac...|  920055|(64,[1,4,10,11,13...|(64,[24,38,59],[2...|\n",
      "|1eac022a97d683eac...|  965334|(64,[1,4,10,11,13...|(64,[2,51,54,59],...|\n",
      "|1eac022a97d683eac...|      99|(64,[1,4,10,11,13...|(64,[7,17,18,20,2...|\n",
      "|1eac022a97d683eac...| 7355647|(64,[1,4,10,11,13...|(64,[3,12,41,43,4...|\n",
      "|1eac022a97d683eac...|    4280|(64,[1,4,10,11,13...|(64,[24],[2.45175...|\n",
      "|1eac022a97d683eac...| 4302361|(64,[1,4,10,11,13...|(64,[29],[2.45175...|\n",
      "|1eac022a97d683eac...|   72879|(64,[1,4,10,11,13...|(64,[24,47,50,60]...|\n",
      "|1eac022a97d683eac...|   90558|(64,[1,4,10,11,13...|(64,[1,9,10,14,18...|\n",
      "|1eac022a97d683eac...| 3614773|(64,[1,4,10,11,13...|(64,[49,57,63],[2...|\n",
      "|1eac022a97d683eac...| 3281478|(64,[1,4,10,11,13...|(64,[14,28,46,47,...|\n",
      "|1eac022a97d683eac...|  849862|(64,[1,4,10,11,13...|(64,[21,52],[7.48...|\n",
      "|1eac022a97d683eac...| 5394760|(64,[1,4,10,11,13...|(64,[6,15,37,38],...|\n",
      "|1eac022a97d683eac...|     268|(64,[1,4,10,11,13...|(64,[24,55],[2.45...|\n",
      "|1eac022a97d683eac...| 1453145|(64,[1,4,10,11,13...|(64,[5,9,59,61],[...|\n",
      "|1eac022a97d683eac...|11191048|(64,[1,4,10,11,13...|(64,[4,34,45,48],...|\n",
      "|1eac022a97d683eac...|   80546|(64,[1,4,10,11,13...|(64,[2,10,12,18,2...|\n",
      "|1eac022a97d683eac...|  557229|(64,[1,4,10,11,13...|(64,[35,37,41,42,...|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "# def to_dense(x):\n",
    "#     return DenseVector(x.toArray())\n",
    "\n",
    "# to_dense_udf = udf(lambda x: to_dense(x), VectorUDT())\n",
    "\n",
    "# df = df.withColumn('paper_profile_dense', to_dense_udf(df.paper_profile))\n",
    "# df = df.drop(\"paper_profile\")\n",
    "\n",
    "df_selected = df.where(df.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 papers for 1eac022a97d683eace8815545ce3153f are 100088, 211804, 557229, 478707, 4302361\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())\n",
    "\n",
    "def cbrs_tf_idf(u, k):\n",
    "    sim_df = u.withColumn('Similarity', compute_sim(u.user_profile, u.paper_profile))\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    cbrs = get_r.select(\"userID\", \"paperID\")\n",
    "    cbrs = cbrs.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(cbrs.paperID)).alias(\"top_papers\"))\n",
    "    \n",
    "    print(\"The top \" + str(k) + \" papers for \" + str(cbrs.head()[0]) + \" are \" + str(cbrs.head()[1]))\n",
    "\n",
    "cbrs_tf_idf(df_selected, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|paperId|            features|   topicDistribution|\n",
      "+-------+--------------------+--------------------+\n",
      "| 159967|(64,[6,16,17,21,2...|[9.45110635789367...|\n",
      "|2212959|(64,[6,26,48],[1....|[0.00441872016704...|\n",
      "| 333353|(64,[2,7,10,19,24...|[0.56343393574946...|\n",
      "| 438129|(64,[4,30],[1.0,1...|[0.00885841139294...|\n",
      "| 166220|(64,[20,29,37,38,...|[0.00240762022449...|\n",
      "|2883810|(64,[24,59],[1.0,...|[0.00885841139294...|\n",
      "|1288940|(64,[8,25,39,46,5...|[0.26311822294429...|\n",
      "|5251453|(64,[5,15,22,29,3...|[0.22258093015662...|\n",
      "|7515828|(64,[24,34,59],[1...|[0.00663593507796...|\n",
      "|2739852|(64,[3,9,10,12,14...|[0.00110272213561...|\n",
      "|5961524|(64,[1,2,7,8,24],...|[0.90392588951444...|\n",
      "|    272|(64,[7,17,23,28,4...|[0.00176498105827...|\n",
      "|6573750|(64,[18,55],[1.0,...|[0.00885841139294...|\n",
      "|  77265|(64,[2,56],[1.0,1...|[0.67868904983830...|\n",
      "| 820297|(64,[11,19,35],[1...|[0.00663593507796...|\n",
      "|1042553|(64,[17,21,27,35,...|[0.00155716299784...|\n",
      "|2883820|(64,[8,50,57],[1....|[0.00530497595923...|\n",
      "|5434882|(64,[2,4,29,37,45...|[0.63770778171787...|\n",
      "|1332540|     (64,[35],[1.0])|[0.51688715820944...|\n",
      "|1777140|(64,[16,41,48,49,...|[0.00378619395911...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### LDA ###\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# just rename the column to use it with the built-in methods\n",
    "termFrequencyVector = vector_df.select('paperId', col('term_frequency_sparse').alias('features'))\n",
    "# Trains a LDA model\n",
    "# set k=40 to have 40 different topics\n",
    "lda = LDA(k=40)\n",
    "model = lda.fit(termFrequencyVector)\n",
    "\n",
    "# each topic is described by 5 terms\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "#topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# it shows the probabilty of each topic for each paper\n",
    "transformed = model.transform(termFrequencyVector)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              userId|    lda_user_profile|        user_profile|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|d503571e44a0373eb...|[0.00189117886967...|(64,[0,1,2,3,4,5,...|\n",
      "|f1e1cd4ff25018273...|[1.07412582463749...|(64,[0,1,2,3,4,5,...|\n",
      "|bbcd9dae3160ddcb9...|[0.52461795827945...|(64,[0,1,2,3,4,5,...|\n",
      "|a0bbf6bb9b1c818f3...|[0.00331207990297...|(64,[0,1,2,3,4,5,...|\n",
      "|1eac022a97d683eac...|[2.07879795154980...|(64,[0,1,2,3,4,5,...|\n",
      "|cf9c7f356092c34be...|[0.00189117886967...|(64,[0,1,2,3,4,5,...|\n",
      "|3b715ebaf1f8f81a1...|[0.26412840422374...|(64,[0,1,2,3,4,5,...|\n",
      "|b36c3189bb1457cd0...|[0.01331922332444...|(64,[0,1,2,3,4,5,...|\n",
      "|f3c28e50db4ce8ad8...|[0.00189117886967...|(64,[0,1,2,3,4,5,...|\n",
      "|b656009a6efdc8b1a...|[0.93239989082039...|(64,[0,1,2,3,4,5,...|\n",
      "|d85f7d83f27b3f533...|[0.25812162666935...|(64,[0,1,2,3,4,5,...|\n",
      "|f05bcffe7951de9e5...|[0.03399150910360...|(64,[0,1,2,3,4,5,...|\n",
      "|4c8912d1b04471cf5...|[0.13545374047422...|(64,[0,1,2,3,4,5,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "\n",
    "lda_user_profile = transformed.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "lda_user_profile = lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "lda_user_profile = lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "lda_user_profile = lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "lda_user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(lda_user_profile, [\"userID\"])\n",
    "\n",
    "df_lda = df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "#df_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+\n",
      "|              userID| paperID|        user_profile|       paper_profile|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|  166220|(64,[0,1,2,3,4,5,...|(64,[20,29,37,38,...|\n",
      "|1eac022a97d683eac...|  820297|(64,[0,1,2,3,4,5,...|(64,[11,19,35],[2...|\n",
      "|1eac022a97d683eac...| 1279898|(64,[0,1,2,3,4,5,...|(64,[24,33,51,56]...|\n",
      "|1eac022a97d683eac...|  920055|(64,[0,1,2,3,4,5,...|(64,[24,38,59],[2...|\n",
      "|1eac022a97d683eac...|  965334|(64,[0,1,2,3,4,5,...|(64,[2,51,54,59],...|\n",
      "|1eac022a97d683eac...|      99|(64,[0,1,2,3,4,5,...|(64,[7,17,18,20,2...|\n",
      "|1eac022a97d683eac...| 7355647|(64,[0,1,2,3,4,5,...|(64,[3,12,41,43,4...|\n",
      "|1eac022a97d683eac...|    4280|(64,[0,1,2,3,4,5,...|(64,[24],[2.45175...|\n",
      "|1eac022a97d683eac...| 4302361|(64,[0,1,2,3,4,5,...|(64,[29],[2.45175...|\n",
      "|1eac022a97d683eac...|   72879|(64,[0,1,2,3,4,5,...|(64,[24,47,50,60]...|\n",
      "|1eac022a97d683eac...|   90558|(64,[0,1,2,3,4,5,...|(64,[1,9,10,14,18...|\n",
      "|1eac022a97d683eac...| 3614773|(64,[0,1,2,3,4,5,...|(64,[49,57,63],[2...|\n",
      "|1eac022a97d683eac...| 3281478|(64,[0,1,2,3,4,5,...|(64,[14,28,46,47,...|\n",
      "|1eac022a97d683eac...|  849862|(64,[0,1,2,3,4,5,...|(64,[21,52],[7.48...|\n",
      "|1eac022a97d683eac...| 5394760|(64,[0,1,2,3,4,5,...|(64,[6,15,37,38],...|\n",
      "|1eac022a97d683eac...|     268|(64,[0,1,2,3,4,5,...|(64,[24,55],[2.45...|\n",
      "|1eac022a97d683eac...| 1453145|(64,[0,1,2,3,4,5,...|(64,[5,9,59,61],[...|\n",
      "|1eac022a97d683eac...|11191048|(64,[0,1,2,3,4,5,...|(64,[4,34,45,48],...|\n",
      "|1eac022a97d683eac...|   80546|(64,[0,1,2,3,4,5,...|(64,[2,10,12,18,2...|\n",
      "|1eac022a97d683eac...|  557229|(64,[0,1,2,3,4,5,...|(64,[35,37,41,42,...|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected_lda = df_lda.where(df_lda.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "df_selected_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 papers for 1eac022a97d683eace8815545ce3153f are 849862, 211804, 249, 664041, 81501\n"
     ]
    }
   ],
   "source": [
    "cbrs_tf_idf(df_selected_lda, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 4 Sampling and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o605.sample. Trace:\npy4j.Py4JException: Method sample([class java.lang.Boolean, class java.lang.Integer, class java.lang.Long]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:272)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-cb99e58706a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampled_user_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msampled_user_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, withReplacement, fraction, seed)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Negative fraction value: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithReplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n\u001b[1;32m    322\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                     format(target_id, \".\", name, value))\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o605.sample. Trace:\npy4j.Py4JException: Method sample([class java.lang.Boolean, class java.lang.Integer, class java.lang.Long]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:272)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "sampled_user_profile = user_profile.sample(False, 1).limit(10)\n",
    "sampled_user_profile.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
