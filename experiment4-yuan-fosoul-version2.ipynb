{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-04\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We reduced our dataset for both the user_libraries and papers due to out of memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|             paperID|\n",
      "+--------------------+--------------------+\n",
      "|28d3f81251d94b097...|[3929762, 503574,...|\n",
      "|d0c9aaa788153daea...|[2080631, 6343346...|\n",
      "|f05bcffe7951de9e5...|[1158654, 478707,...|\n",
      "|ca4f1ba4094011d9a...|            [278019]|\n",
      "|d1d41a15201915503...|[6610569, 6493797...|\n",
      "|f2f77383828ea6d39...|[943458, 238121, ...|\n",
      "|9c883d02115400f7b...|[3509971, 3509965...|\n",
      "|b656009a6efdc8b1a...|[771870, 181369, ...|\n",
      "|cf9c7f356092c34be...|             [90558]|\n",
      "|0f5cbb39410a9278f...|           [9344598]|\n",
      "|d85f7d83f27b3f533...|[7610843, 3633347...|\n",
      "|586c867a0688250ac...|[464760, 466011, ...|\n",
      "|10fdfaf945d5c27ad...|           [2010550]|\n",
      "|589b870a611c25fa9...|[1283233, 1305474...|\n",
      "|90f1a3e6fcdbf9bc5...|[115945, 11733005...|\n",
      "|7e070a9da96672e05...|           [1071959]|\n",
      "|3b715ebaf1f8f81a1...|[4119394, 3378798...|\n",
      "|488fb15e8c77f8054...|[1523301, 5281566...|\n",
      "|3fdf355e59949c79d...|[7077220, 1289842...|\n",
      "|c6b59086a0bbac141...|[2230995, 3050075...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example2.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "user_df_pre = user_df\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "user_df_pre.show()\n",
    "\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./example_paper.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "#w_df.show()\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|              userID|paperID|\n",
      "+--------------------+-------+\n",
      "|28d3f81251d94b097...|1277953|\n",
      "|28d3f81251d94b097...|3153930|\n",
      "|28d3f81251d94b097...|3153931|\n",
      "|28d3f81251d94b097...|1388555|\n",
      "|28d3f81251d94b097...|4186128|\n",
      "|28d3f81251d94b097...|1814546|\n",
      "|28d3f81251d94b097...|7499794|\n",
      "|28d3f81251d94b097...| 921623|\n",
      "|28d3f81251d94b097...| 970776|\n",
      "|28d3f81251d94b097...| 311321|\n",
      "|28d3f81251d94b097...|7499801|\n",
      "|28d3f81251d94b097...| 118812|\n",
      "|28d3f81251d94b097...| 290846|\n",
      "|28d3f81251d94b097...|4067359|\n",
      "|28d3f81251d94b097...| 688160|\n",
      "|28d3f81251d94b097...|1118240|\n",
      "|28d3f81251d94b097...|  81954|\n",
      "|28d3f81251d94b097...|4460578|\n",
      "|28d3f81251d94b097...| 340004|\n",
      "|28d3f81251d94b097...|2363430|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "unrated_df = user_df_pre.withColumn(\"unrated\", get_unrated(user_df_pre.paperID))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"paperID\")\n",
    "\n",
    "unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"unrated\")\n",
    "\n",
    "unrated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 1 Vector representation for the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('stemmed', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"stemmed\")\n",
    "\n",
    "# Create a new df to store the paperID and stemmed tokens\n",
    "paper_terms = df\n",
    "\n",
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "df = df.select(\"paperID\", f.explode(\"stemmed\").alias(\"tokens\")).distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "df = df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "#important_words.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| âˆ’ 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|              terms_|\n",
      "+-------+--------------------+\n",
      "| 159967|[scale, predict, ...|\n",
      "|2212959|[map, cluster, cl...|\n",
      "| 333353|[pattern, current...|\n",
      "| 438129|  [collect, resourc]|\n",
      "| 166220|[challeng, curren...|\n",
      "|2883810|     [offer, featur]|\n",
      "|1288940|[web, web, web, w...|\n",
      "|5251453|[pattern, effect,...|\n",
      "|7515828|[dna, offer, featur]|\n",
      "|2739852|[web, emerg, scal...|\n",
      "|5961524|[offer, addit, ev...|\n",
      "|    272|[current, dynam, ...|\n",
      "|6573750|       [cell, genet]|\n",
      "|  77265|   [pattern, evolut]|\n",
      "| 820297|[simpl, perform, ...|\n",
      "|1042553|[power, power, po...|\n",
      "|2883820|[technolog, curre...|\n",
      "|5434882|[current, emerg, ...|\n",
      "|1332540|             [simpl]|\n",
      "|1777140|[map, challeng, p...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\"))\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "tf_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paperID|term_frequency_sparse                                                                                                              |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|159967 |(64,[0,5,16,21,24,35,38,40,44,52,58],[12.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                               |\n",
      "|2212959|(64,[1,16,58],[1.0,3.0,1.0])                                                                                                       |\n",
      "|333353 |(64,[3,4,13,29,36,37,39,41,43,46,61],[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                |\n",
      "|438129 |(64,[22,48],[1.0,1.0])                                                                                                             |\n",
      "|166220 |(64,[8,12,18,24,29,31,49],[2.0,3.0,1.0,1.0,1.0,1.0,1.0])                                                                           |\n",
      "|2883810|(64,[4,46],[1.0,1.0])                                                                                                              |\n",
      "|1288940|(64,[2,10,33,38,41,57],[5.0,1.0,1.0,1.0,1.0,1.0])                                                                                  |\n",
      "|5251453|(64,[0,3,10,11,17,18,33,34,53,63],[1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,2.0])                                                       |\n",
      "|7515828|(64,[4,6,46],[1.0,1.0,1.0])                                                                                                        |\n",
      "|2739852|(64,[0,2,9,15,16,19,23,27,30,32,35,39,43,44,46,47,54,62],[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|5961524|(64,[13,46,56,57,61],[5.0,1.0,1.0,1.0,1.0])                                                                                        |\n",
      "|272    |(64,[7,10,17,20,27,29,32,44,52,60,61],[3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])                                               |\n",
      "|6573750|(64,[19,20],[1.0,1.0])                                                                                                             |\n",
      "|77265  |(64,[3,13],[1.0,1.0])                                                                                                              |\n",
      "|820297 |(64,[26,36,55],[1.0,1.0,1.0])                                                                                                      |\n",
      "|1042553|(64,[5,17,32,40,45,52,55],[3.0,3.0,3.0,3.0,1.0,2.0,1.0])                                                                           |\n",
      "|2883820|(64,[14,29,57],[1.0,2.0,1.0])                                                                                                      |\n",
      "|5434882|(64,[13,15,18,29,47,48,49],[1.0,1.0,4.0,1.0,1.0,1.0,1.0])                                                                          |\n",
      "|1332540|(64,[55],[1.0])                                                                                                                    |\n",
      "|1777140|(64,[0,1,5,21,31],[2.0,1.0,1.0,1.0,1.0])                                                                                           |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "# Get the number of distinct terms\n",
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "vector_df = model.transform(tf_df)\n",
    "vector_df = vector_df.select(\"paperID\", col(\"vectors\").alias(\"term_frequency_sparse\"))\n",
    "\n",
    "vector_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|paperID|            features|\n",
      "+-------+--------------------+\n",
      "| 159967|(64,[0,5,16,21,24...|\n",
      "|2212959|(64,[1,16,58],[2....|\n",
      "| 333353|(64,[3,4,13,29,36...|\n",
      "| 438129|(64,[22,48],[2.45...|\n",
      "| 166220|(64,[8,12,18,24,2...|\n",
      "|2883810|(64,[4,46],[2.219...|\n",
      "|1288940|(64,[2,10,33,38,4...|\n",
      "|5251453|(64,[0,3,10,11,17...|\n",
      "|7515828|(64,[4,6,46],[2.2...|\n",
      "|2739852|(64,[0,2,9,15,16,...|\n",
      "|5961524|(64,[13,46,56,57,...|\n",
      "|    272|(64,[7,10,17,20,2...|\n",
      "|6573750|(64,[19,20],[2.49...|\n",
      "|  77265|(64,[3,13],[2.255...|\n",
      "| 820297|(64,[26,36,55],[2...|\n",
      "|1042553|(64,[5,17,32,40,4...|\n",
      "|2883820|(64,[14,29,57],[2...|\n",
      "|5434882|(64,[13,15,18,29,...|\n",
      "|1332540|(64,[55],[2.40919...|\n",
      "|1777140|(64,[0,1,5,21,31]...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TF-IDF with built-in function ###\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "tf_idf_built_in.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "user_profile = tf_idf_built_in.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "user_profile = user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "user_profile = user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userId|        user_profile|\n",
      "+--------------------+--------------------+\n",
      "|d503571e44a0373eb...|(64,[6,11,13,19,3...|\n",
      "|f1e1cd4ff25018273...|(64,[8,9,12,18,37...|\n",
      "|bbcd9dae3160ddcb9...|(64,[5,9,18,36,42...|\n",
      "|a0bbf6bb9b1c818f3...|(64,[5,18,42],[8....|\n",
      "|1eac022a97d683eac...|(64,[0,4,5,6,10,1...|\n",
      "|cf9c7f356092c34be...|(64,[2,4,19,23,30...|\n",
      "|3b715ebaf1f8f81a1...|(64,[2,8,9,10,12,...|\n",
      "|b36c3189bb1457cd0...|(64,[46],[2.45175...|\n",
      "|f3c28e50db4ce8ad8...|(64,[6,11,13,19,3...|\n",
      "|b656009a6efdc8b1a...|(64,[2,3,4,6,7,8,...|\n",
      "|d85f7d83f27b3f533...|(64,[17,18,20,28,...|\n",
      "|f05bcffe7951de9e5...|(64,[0,1,2,4,5,6,...|\n",
      "|4c8912d1b04471cf5...|(64,[0,3,4,6,10,2...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_sparse(x):        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(x)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in x if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "\n",
    "to_sparse_udf = udf(lambda x: to_sparse(x), VectorUDT())\n",
    "\n",
    "user_profile = user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+\n",
      "|              userID|paperID|        user_profile|       paper_profile|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "|f1e1cd4ff25018273...| 166220|(64,[8,9,12,18,37...|(64,[8,12,18,24,2...|\n",
      "|f1e1cd4ff25018273...| 820297|(64,[8,9,12,18,37...|(64,[26,36,55],[2...|\n",
      "|f1e1cd4ff25018273...|1042553|(64,[8,9,12,18,37...|(64,[5,17,32,40,4...|\n",
      "|f1e1cd4ff25018273...|3010240|(64,[8,9,12,18,37...|(64,[18,50],[2.45...|\n",
      "|f1e1cd4ff25018273...| 523772|(64,[8,9,12,18,37...|(64,[55],[2.40919...|\n",
      "|f1e1cd4ff25018273...|1279898|(64,[8,9,12,18,37...|(64,[3,43,46,47],...|\n",
      "|f1e1cd4ff25018273...| 920055|(64,[8,9,12,18,37...|(64,[4,12,46],[2....|\n",
      "|f1e1cd4ff25018273...| 965334|(64,[8,9,12,18,37...|(64,[4,13,34,47],...|\n",
      "|f1e1cd4ff25018273...|     99|(64,[8,9,12,18,37...|(64,[7,8,14,16,17...|\n",
      "|f1e1cd4ff25018273...|7355647|(64,[8,9,12,18,37...|(64,[0,7,9,15,44,...|\n",
      "|f1e1cd4ff25018273...|   4280|(64,[8,9,12,18,37...|(64,[46],[2.45175...|\n",
      "|f1e1cd4ff25018273...|  72879|(64,[8,9,12,18,37...|(64,[5,29,35,46],...|\n",
      "|f1e1cd4ff25018273...|  90558|(64,[8,9,12,18,37...|(64,[2,4,19,23,30...|\n",
      "|f1e1cd4ff25018273...|3614773|(64,[8,9,12,18,37...|(64,[10,14,31],[2...|\n",
      "|f1e1cd4ff25018273...|3281478|(64,[8,9,12,18,37...|(64,[7,24,27,30,3...|\n",
      "|f1e1cd4ff25018273...| 849862|(64,[8,9,12,18,37...|(64,[28,40],[4.58...|\n",
      "|f1e1cd4ff25018273...|5394760|(64,[8,9,12,18,37...|(64,[12,49,53,58]...|\n",
      "|f1e1cd4ff25018273...|    268|(64,[8,9,12,18,37...|(64,[20,46],[6.76...|\n",
      "|f1e1cd4ff25018273...| 531300|(64,[8,9,12,18,37...|(64,[24,39],[2.18...|\n",
      "|f1e1cd4ff25018273...|1453145|(64,[8,9,12,18,37...|(64,[4,37,54,63],...|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+\n",
      "|              userID| paperID|        user_profile|       paper_profile|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|  166220|(64,[0,4,5,6,10,1...|(64,[8,12,18,24,2...|\n",
      "|1eac022a97d683eac...|  820297|(64,[0,4,5,6,10,1...|(64,[26,36,55],[2...|\n",
      "|1eac022a97d683eac...| 1279898|(64,[0,4,5,6,10,1...|(64,[3,43,46,47],...|\n",
      "|1eac022a97d683eac...|  920055|(64,[0,4,5,6,10,1...|(64,[4,12,46],[2....|\n",
      "|1eac022a97d683eac...|  965334|(64,[0,4,5,6,10,1...|(64,[4,13,34,47],...|\n",
      "|1eac022a97d683eac...|      99|(64,[0,4,5,6,10,1...|(64,[7,8,14,16,17...|\n",
      "|1eac022a97d683eac...| 7355647|(64,[0,4,5,6,10,1...|(64,[0,7,9,15,44,...|\n",
      "|1eac022a97d683eac...|    4280|(64,[0,4,5,6,10,1...|(64,[46],[2.45175...|\n",
      "|1eac022a97d683eac...| 4302361|(64,[0,4,5,6,10,1...|(64,[18],[2.45175...|\n",
      "|1eac022a97d683eac...|   72879|(64,[0,4,5,6,10,1...|(64,[5,29,35,46],...|\n",
      "|1eac022a97d683eac...|   90558|(64,[0,4,5,6,10,1...|(64,[2,4,19,23,30...|\n",
      "|1eac022a97d683eac...| 3614773|(64,[0,4,5,6,10,1...|(64,[10,14,31],[2...|\n",
      "|1eac022a97d683eac...| 3281478|(64,[0,4,5,6,10,1...|(64,[7,24,27,30,3...|\n",
      "|1eac022a97d683eac...|  849862|(64,[0,4,5,6,10,1...|(64,[28,40],[4.58...|\n",
      "|1eac022a97d683eac...| 5394760|(64,[0,4,5,6,10,1...|(64,[12,49,53,58]...|\n",
      "|1eac022a97d683eac...|     268|(64,[0,4,5,6,10,1...|(64,[20,46],[6.76...|\n",
      "|1eac022a97d683eac...| 1453145|(64,[0,4,5,6,10,1...|(64,[4,37,54,63],...|\n",
      "|1eac022a97d683eac...|11191048|(64,[0,4,5,6,10,1...|(64,[1,6,15,48],[...|\n",
      "|1eac022a97d683eac...|   80546|(64,[0,4,5,6,10,1...|(64,[6,11,13,19,3...|\n",
      "|1eac022a97d683eac...|  557229|(64,[0,4,5,6,10,1...|(64,[0,3,4,23,35,...|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "# def to_dense(x):\n",
    "#     return DenseVector(x.toArray())\n",
    "\n",
    "# to_dense_udf = udf(lambda x: to_dense(x), VectorUDT())\n",
    "\n",
    "# df = df.withColumn('paper_profile_dense', to_dense_udf(df.paper_profile))\n",
    "# df = df.drop(\"paper_profile\")\n",
    "\n",
    "df_selected = df.where(df.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 papers for 1eac022a97d683eace8815545ce3153f are 100088, 211804, 557229, 478707, 4302361\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())\n",
    "\n",
    "def cbrs_tf_idf(u, k):\n",
    "    sim_df = u.withColumn('Similarity', compute_sim(u.user_profile, u.paper_profile))\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    cbrs = get_r.select(\"userID\", \"paperID\")\n",
    "    cbrs = cbrs.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(cbrs.paperID)).alias(\"top_papers\"))\n",
    "    \n",
    "    print(\"The top \" + str(k) + \" papers for \" + str(cbrs.head()[0]) + \" are \" + str(cbrs.head()[1]))\n",
    "\n",
    "cbrs_tf_idf(df_selected, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|paperId|            features|   topicDistribution|\n",
      "+-------+--------------------+--------------------+\n",
      "| 159967|(64,[0,5,16,21,24...|[8.62882233031433...|\n",
      "|2212959|(64,[1,16,58],[1....|[0.00403466100484...|\n",
      "| 333353|(64,[3,4,13,29,36...|[0.00151064718998...|\n",
      "| 438129|(64,[22,48],[1.0,...|[0.00808946256218...|\n",
      "| 166220|(64,[8,12,18,24,2...|[0.00219823645336...|\n",
      "|2883810|(64,[4,46],[1.0,1...|[0.00808946256218...|\n",
      "|1288940|(64,[2,10,33,38,4...|[0.00219823645336...|\n",
      "|5251453|(64,[0,3,10,11,17...|[0.00142170763106...|\n",
      "|7515828|(64,[4,6,46],[1.0...|[0.00605953480064...|\n",
      "|2739852|(64,[0,2,9,15,16,...|[0.00100678528591...|\n",
      "|5961524|(64,[13,46,56,57,...|[0.00241838815250...|\n",
      "|    272|(64,[7,10,17,20,2...|[0.00161145714901...|\n",
      "|6573750|(64,[19,20],[1.0,...|[0.00808946256218...|\n",
      "|  77265|(64,[3,13],[1.0,1...|[0.00808946256218...|\n",
      "| 820297|(64,[26,36,55],[1...|[0.00605953480064...|\n",
      "|1042553|(64,[5,17,32,40,4...|[0.00142170763106...|\n",
      "|2883820|(64,[14,29,57],[1...|[0.00484400525584...|\n",
      "|5434882|(64,[13,15,18,29,...|[0.00219823645336...|\n",
      "|1332540|     (64,[55],[1.0])|[0.01216455585825...|\n",
      "|1777140|(64,[0,1,5,21,31]...|[0.00345705127365...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### LDA ###\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# just rename the column to use it with the built-in methods\n",
    "termFrequencyVector = vector_df.select('paperId', col('term_frequency_sparse').alias('features'))\n",
    "# Trains a LDA model\n",
    "# set k=40 to have 40 different topics\n",
    "lda = LDA(k=40)\n",
    "model = lda.fit(termFrequencyVector)\n",
    "\n",
    "# each topic is described by 5 terms\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "#topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# it shows the probabilty of each topic for each paper\n",
    "transformed = model.transform(termFrequencyVector)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              userId|    lda_user_profile|        user_profile|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|d503571e44a0373eb...|[0.00172668387563...|(64,[0,1,2,3,4,5,...|\n",
      "|f1e1cd4ff25018273...|[0.02428867942528...|(64,[0,1,2,3,4,5,...|\n",
      "|bbcd9dae3160ddcb9...|[0.01922333031259...|(64,[0,1,2,3,4,5,...|\n",
      "|a0bbf6bb9b1c818f3...|[0.00302411344949...|(64,[0,1,2,3,4,5,...|\n",
      "|1eac022a97d683eac...|[0.04247925079504...|(64,[0,1,2,3,4,5,...|\n",
      "|cf9c7f356092c34be...|[0.00172668387563...|(64,[0,1,2,3,4,5,...|\n",
      "|3b715ebaf1f8f81a1...|[0.01709020374542...|(64,[0,1,2,3,4,5,...|\n",
      "|b36c3189bb1457cd0...|[0.01216455585825...|(64,[0,1,2,3,4,5,...|\n",
      "|f3c28e50db4ce8ad8...|[0.00172668387563...|(64,[0,1,2,3,4,5,...|\n",
      "|b656009a6efdc8b1a...|[0.06713783507144...|(64,[0,1,2,3,4,5,...|\n",
      "|d85f7d83f27b3f533...|[0.00830105652950...|(64,[0,1,2,3,4,5,...|\n",
      "|f05bcffe7951de9e5...|[0.03103687891449...|(64,[0,1,2,3,4,5,...|\n",
      "|4c8912d1b04471cf5...|[0.02614633273163...|(64,[0,1,2,3,4,5,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "\n",
    "lda_user_profile = transformed.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "lda_user_profile = lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "lda_user_profile = lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "lda_user_profile = lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "lda_user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(lda_user_profile, [\"userID\"])\n",
    "\n",
    "df_lda = df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "#df_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+\n",
      "|              userID| paperID|        user_profile|       paper_profile|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|  166220|(64,[0,1,2,3,4,5,...|(64,[8,12,18,24,2...|\n",
      "|1eac022a97d683eac...|  820297|(64,[0,1,2,3,4,5,...|(64,[26,36,55],[2...|\n",
      "|1eac022a97d683eac...| 1279898|(64,[0,1,2,3,4,5,...|(64,[3,43,46,47],...|\n",
      "|1eac022a97d683eac...|  920055|(64,[0,1,2,3,4,5,...|(64,[4,12,46],[2....|\n",
      "|1eac022a97d683eac...|  965334|(64,[0,1,2,3,4,5,...|(64,[4,13,34,47],...|\n",
      "|1eac022a97d683eac...|      99|(64,[0,1,2,3,4,5,...|(64,[7,8,14,16,17...|\n",
      "|1eac022a97d683eac...| 7355647|(64,[0,1,2,3,4,5,...|(64,[0,7,9,15,44,...|\n",
      "|1eac022a97d683eac...|    4280|(64,[0,1,2,3,4,5,...|(64,[46],[2.45175...|\n",
      "|1eac022a97d683eac...| 4302361|(64,[0,1,2,3,4,5,...|(64,[18],[2.45175...|\n",
      "|1eac022a97d683eac...|   72879|(64,[0,1,2,3,4,5,...|(64,[5,29,35,46],...|\n",
      "|1eac022a97d683eac...|   90558|(64,[0,1,2,3,4,5,...|(64,[2,4,19,23,30...|\n",
      "|1eac022a97d683eac...| 3614773|(64,[0,1,2,3,4,5,...|(64,[10,14,31],[2...|\n",
      "|1eac022a97d683eac...| 3281478|(64,[0,1,2,3,4,5,...|(64,[7,24,27,30,3...|\n",
      "|1eac022a97d683eac...|  849862|(64,[0,1,2,3,4,5,...|(64,[28,40],[4.58...|\n",
      "|1eac022a97d683eac...| 5394760|(64,[0,1,2,3,4,5,...|(64,[12,49,53,58]...|\n",
      "|1eac022a97d683eac...|     268|(64,[0,1,2,3,4,5,...|(64,[20,46],[6.76...|\n",
      "|1eac022a97d683eac...| 1453145|(64,[0,1,2,3,4,5,...|(64,[4,37,54,63],...|\n",
      "|1eac022a97d683eac...|11191048|(64,[0,1,2,3,4,5,...|(64,[1,6,15,48],[...|\n",
      "|1eac022a97d683eac...|   80546|(64,[0,1,2,3,4,5,...|(64,[6,11,13,19,3...|\n",
      "|1eac022a97d683eac...|  557229|(64,[0,1,2,3,4,5,...|(64,[0,3,4,23,35,...|\n",
      "+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected_lda = df_lda.where(df_lda.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "df_selected_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 papers for 1eac022a97d683eace8815545ce3153f are 81501, 129, 90558, 11191048, 80546\n"
     ]
    }
   ],
   "source": [
    "cbrs_tf_idf(df_selected_lda, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 4 Sampling and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_user_profile = user_profile.sample(False, 1).limit(10)\n",
    "sampled_user_profile.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
