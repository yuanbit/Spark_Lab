{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-04\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex4').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import re\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We reduced our dataset for both the user_libraries and papers due to out of memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user ratings into Dataframe\n",
    "#user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./example0.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\"))\n",
    "\n",
    "# df to be used in 4.4\n",
    "sampled_users = user_df\n",
    "\n",
    "user_df_pre = user_df\n",
    "user_df_pre = user_df_pre.withColumn(\"paperID\", split(col(\"paperID\"), \",\").cast(ArrayType(IntegerType())).alias(\"paperID\"))\n",
    "#user_df_pre.show()\n",
    "\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"papers\"), f.explode(f.split(\"paperID\", \",\")).alias(\"paperID\"))\n",
    "user_df = user_df.drop(\"papers\")\n",
    "\n",
    "# Get a dataframe of the distinct papers\n",
    "d_paper = user_df.select(\"paperID\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopwordList = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "#w_df = spark.read.csv('./papers.csv')\n",
    "w_df = spark.read.csv('./paper0.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c13\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c13\").alias(\"title\"), col(\"_c14\").alias(\"abstract\"))\n",
    "w_df = w_df.na.fill({'title': '', 'abstract': ''}) # to replace null values with empty string\n",
    "# Get text from title and abstract\n",
    "w_df = w_df.select(col(\"paperID\"), concat(col(\"title\"), lit(\" \"), col(\"abstract\")).alias(\"words\"))\n",
    "#w_df.show()\n",
    "\n",
    "# Transform the distinct paperIDs dataframe to a list\n",
    "paper_list = list(d_paper.select('paperID').toPandas()['paperID'])\n",
    "# Map each distinct paper into int\n",
    "paper_list = list(map(int, paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call in udf\n",
    "def unrated(papers):\n",
    "    # Transform the list of distinct papers and the list of rated papers of each user to a set\n",
    "    # Substract the two sets to get the list of unrated papers for each user\n",
    "    # Transform back to list\n",
    "    unrated = list(set(paper_list) - set(papers))\n",
    "    \n",
    "    return unrated\n",
    "\n",
    "\n",
    "# udf to get a list of unrated papers with the length of rated papers for each user\n",
    "get_unrated = udf(lambda x: unrated(x), ArrayType(IntegerType()))\n",
    "\n",
    "# Add a new column of unrated papers for each user\n",
    "unrated_df = user_df_pre.withColumn(\"unrated\", get_unrated(user_df_pre.paperID))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"paperID\")\n",
    "\n",
    "unrated_df = unrated_df.withColumn(\"paperID\", explode(unrated_df.unrated))\n",
    "\n",
    "unrated_df = unrated_df.drop(\"unrated\")\n",
    "\n",
    "#unrated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. 1 Vector representation for the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|    terms|count|\n",
      "+---------+-----+\n",
      "|     cell|   31|\n",
      "|   experi|   31|\n",
      "|     time|   31|\n",
      "|   compar|   30|\n",
      "|   measur|   30|\n",
      "|     user|   30|\n",
      "|  problem|   30|\n",
      "|   requir|   30|\n",
      "| challeng|   29|\n",
      "|  pattern|   28|\n",
      "|   featur|   28|\n",
      "|    paper|   28|\n",
      "|    emerg|   28|\n",
      "|   effect|   28|\n",
      "|  current|   27|\n",
      "|  predict|   27|\n",
      "|technolog|   27|\n",
      "|      web|   27|\n",
      "|   import|   26|\n",
      "|      map|   26|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting words from the papers and keeping \"-\" and \"_\"\n",
    "tokenizer = RegexTokenizer(inputCol=\"words\", outputCol=\"tokens\", pattern=\"[a-zA-Z-_]+\", gaps=False) \n",
    "# Built-in tokenizer\n",
    "tokenized = tokenizer.transform(w_df)\n",
    "tokenized = tokenized.select(\"paperID\", \"tokens\")\n",
    "\n",
    "# udf to remove \"-\" and \"_\" from the tokens\n",
    "remove_hyphen_udf = udf(lambda x: [re.sub('[-|_]', '', word) for word in x], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = tokenized.withColumn('tokens', remove_hyphen_udf(col('tokens')))\n",
    "\n",
    "# udf to remove words less than 3 letters\n",
    "remove_short_words = udf(lambda x: [item for item in x if len(item) >= 3], ArrayType(StringType()))\n",
    "# Apply udf to the tokens\n",
    "df = df.withColumn('tokens', remove_short_words(col('tokens')))\n",
    "\n",
    "# Built-in function to remove stopwords from our custom list\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\" , stopWords=stopwordList)\n",
    "df = remover.transform(df)\n",
    "df = df.select(\"paperID\", \"filtered\")\n",
    "\n",
    "# Apply stemming with NLTK\n",
    "# Built-in class from NLTK\n",
    "ps = PorterStemmer()\n",
    "# udf to apply stemming\n",
    "stemming = udf(lambda x: [ps.stem(item) for item in x], ArrayType(StringType()))\n",
    "# apply udf to tokens\n",
    "df = df.withColumn('stemmed', stemming(col('filtered')))\n",
    "df = df.select(\"paperID\", \"stemmed\")\n",
    "\n",
    "# Create a new df to store the paperID and stemmed tokens\n",
    "paper_terms = df\n",
    "\n",
    "# Explode/Split the tokens in the list for each paperID and get the distinct tokens\n",
    "df = df.select(\"paperID\", f.explode(\"stemmed\").alias(\"tokens\")).distinct().orderBy(\"paperID\")\n",
    "\n",
    "# Assign count of 1 to each token w.r.t. the paperID since the tokens are distinct\n",
    "df = df.groupBy(\"paperID\",\"tokens\").count()\n",
    "\n",
    "# Get the number of distinct papers\n",
    "num_papers = w_df.select(\"paperID\").distinct().count()\n",
    "\n",
    "# Get the value of ten percent of the number of papers\n",
    "ten_percent = math.ceil(num_papers*.1)\n",
    "\n",
    "# Create a new df with the tokens and count (without paperID)\n",
    "df2 = df.select(\"tokens\", \"count\")\n",
    "# Count the number of papers containing the tokens\n",
    "df2 = df2.groupBy(\"tokens\").agg(f.collect_list(\"tokens\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))\n",
    "# Filter out tokens that appeared in more than 10 percent of the papers\n",
    "df2 = df2.drop(\"duplicated_values\").orderBy((col(\"count\")).desc()).filter(col(\"count\") < ten_percent)\n",
    "# Filter out tokens that appeared in less than 20 papers\n",
    "# Limit the df to 1000 tokens\n",
    "df2 = df2.filter(col(\"count\") >= 20).limit(1000)\n",
    "# Create a new df with terms and count\n",
    "important_words = df2.select(col(\"tokens\").alias(\"terms\"), col(\"count\"))\n",
    "# Output the set of important words, T\n",
    "important_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|index|     terms|\n",
      "+-----+----------+\n",
      "|    0| character|\n",
      "|    1|     addit|\n",
      "|    2|     level|\n",
      "|    3|   program|\n",
      "|    4|  interest|\n",
      "|    5|      year|\n",
      "|    6|   resourc|\n",
      "|    7|    reveal|\n",
      "|    8|  demonstr|\n",
      "|    9|experiment|\n",
      "|   10|       key|\n",
      "|   11|  properti|\n",
      "|   12|     learn|\n",
      "|   13|     field|\n",
      "|   14|  principl|\n",
      "|   15|    access|\n",
      "|   16| distribut|\n",
      "|   17|     enabl|\n",
      "|   18|    effici|\n",
      "|   19|    common|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new df where each term is replaced by a unique index that takes a value from the range between 0 and |T| âˆ’ 1\n",
    "df = important_words.withColumn(\"row_num\", row_number().over(Window.orderBy(\"count\"))-1)\n",
    "\n",
    "# Create a df to store the indices and the corresponding terms\n",
    "terms_index_hash = df.select(col(\"row_num\").alias(\"index\"), \"terms\")\n",
    "\n",
    "terms_index_hash.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "num_terms = terms_index_hash.select(\"terms\").distinct().count()\n",
    "\n",
    "print(num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_terms = paper_terms.select(\"paperID\", f.explode(\"stemmed\").alias(\"terms\"))\n",
    "\n",
    "# Join p_terms with the terms_index_hash to replace the terms with the indices\n",
    "joined_df = terms_index_hash.join(p_terms, [\"terms\"])\n",
    "joined_df = joined_df.drop(\"index\")\n",
    "\n",
    "# Create a new df to compute the term frequency vectors\n",
    "tf_df = joined_df\n",
    "\n",
    "tf_df = tf_df.groupby(\"paperID\").agg(f.concat_ws(\", \", f.collect_list(tf_df.terms)).alias(\"terms\"))\n",
    "tf_df = tf_df.withColumn(\"terms_\", split(col(\"terms\"), \",\\s*\").cast(ArrayType(StringType())).alias(\"terms\"))\n",
    "tf_df = tf_df.drop(\"terms\")\n",
    "#tf_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_\", outputCol=\"vectors\")\n",
    "model = cv.fit(tf_df)\n",
    "vector_df = model.transform(tf_df)\n",
    "vector_df = vector_df.select(\"paperID\", col(\"vectors\").alias(\"term_frequency_sparse\"))\n",
    "\n",
    "#vector_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF with built-in function ###\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency_sparse\", outputCol=\"features\")\n",
    "idfModel = idf.fit(vector_df)\n",
    "rescaledData = idfModel.transform(vector_df)\n",
    "tf_idf_built_in = rescaledData.select(\"paperID\", \"features\")\n",
    "\n",
    "#tf_idf_built_in.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "user_profile = tf_idf_built_in.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "user_profile = user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "user_profile = user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(x):        \n",
    "    # store the indices of non-zero elements\n",
    "    nonzero_indices = np.nonzero(x)[0].tolist()\n",
    "    # store the value of non-zero elements\n",
    "    nonzero_counts = [num for num in x if num]\n",
    "    # combine them to make a sparse vector\n",
    "    sparse = SparseVector(num_terms, nonzero_indices, nonzero_counts)\n",
    "    return sparse\n",
    "\n",
    "to_sparse_udf = udf(lambda x: to_sparse(x), VectorUDT())\n",
    "\n",
    "user_profile = user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "#user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(user_profile, [\"userID\"])\n",
    "\n",
    "df = df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "# def to_dense(x):\n",
    "#     return DenseVector(x.toArray())\n",
    "\n",
    "# to_dense_udf = udf(lambda x: to_dense(x), VectorUDT())\n",
    "\n",
    "# df = df.withColumn('paper_profile_dense', to_dense_udf(df.paper_profile))\n",
    "# df = df.drop(\"paper_profile\")\n",
    "\n",
    "#### CHANGE THE USER HERE!!! ###\n",
    "df_selected = df.where(df.userID==\"589b870a611c25fa99bd3d7295ac0622\")\n",
    "#df_selected = df.where(df.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "#df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 papers for 589b870a611c25fa99bd3d7295ac0622 are 9045137, 115945, 8310458, 3728173, 11733005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cos_sim(u, p):\n",
    "    result = (np.dot(u, p))/(np.linalg.norm(u) * np.linalg.norm(p))\n",
    "    result = result.item()\n",
    "    return result\n",
    "\n",
    "compute_sim = udf(cos_sim, FloatType())\n",
    "\n",
    "def cbrs(u, k):\n",
    "    sim_df = u.withColumn('Similarity', compute_sim(u.user_profile, u.paper_profile))\n",
    "    window = Window.partitionBy(col(\"userID\")).orderBy((col(\"Similarity\")).desc())\n",
    "    sim_df = sim_df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= k)\n",
    "    get_r = sim_df.select(\"userID\", \"paperID\", col(\"row_number\").alias(\"rank\"))\n",
    "    cbrs_df = get_r.select(\"userID\", \"paperID\")\n",
    "    cbrs_df = cbrs_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(cbrs_df.paperID)).alias(\"top_papers\"))\n",
    "    \n",
    "    return cbrs_df\n",
    "\n",
    "k = 5\n",
    "\n",
    "user_rec = cbrs(df_selected, k)\n",
    "\n",
    "print(\"The top \" + str(k) + \" papers for \" + str(user_rec.head()[0]) + \" are \" + str(user_rec.head()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA ###\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# just rename the column to use it with the built-in methods\n",
    "termFrequencyVector = vector_df.select('paperId', col('term_frequency_sparse').alias('features'))\n",
    "# Trains a LDA model\n",
    "# set k=40 to have 40 different topics\n",
    "lda = LDA(k=40)\n",
    "model = lda.fit(termFrequencyVector)\n",
    "\n",
    "# each topic is described by 5 terms\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "#topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# it shows the probabilty of each topic for each paper\n",
    "transformed = model.transform(termFrequencyVector)\n",
    "#transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "\n",
    "lda_user_profile = transformed.join(user_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "lda_user_profile = lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "lda_user_profile = lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "lda_user_profile = lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "#lda_user_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = unrated_df.join(transformed, [\"paperID\"]).join(lda_user_profile, [\"userID\"])\n",
    "\n",
    "df_lda = df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "#df_lda.show()\n",
    "\n",
    "df_selected_lda = df_lda.where(df_lda.userID==\"1eac022a97d683eace8815545ce3153f\")\n",
    "\n",
    "#df_selected_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|userID|top_papers|\n",
      "+------+----------+\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbrs(df_selected_lda, 5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 4 Sampling and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+---------+\n",
      "|              userID|             paperID|libSize|trainSize|\n",
      "+--------------------+--------------------+-------+---------+\n",
      "|589b870a611c25fa9...|[1283233, 1305474...|      8|        6|\n",
      "|90f1a3e6fcdbf9bc5...|[115945, 11733005...|      5|        4|\n",
      "+--------------------+--------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "\n",
    "sampled_users = user_df_pre.orderBy(rand()).limit(2)\n",
    "\n",
    "get_len_udf = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "sampled_users = sampled_users.withColumn(\"libSize\", get_len_udf(\"paperID\"))\n",
    "\n",
    "get_train = udf(lambda x: int(x*0.8), IntegerType())\n",
    "\n",
    "sampled_users = sampled_users.withColumn(\"trainSize\", get_train(\"libSize\"))\n",
    "\n",
    "sampled_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              userID| paperID|\n",
      "+--------------------+--------+\n",
      "|589b870a611c25fa9...| 1029499|\n",
      "|589b870a611c25fa9...| 1305474|\n",
      "|589b870a611c25fa9...| 1283233|\n",
      "|589b870a611c25fa9...|  945604|\n",
      "|589b870a611c25fa9...|  956315|\n",
      "|589b870a611c25fa9...|  967275|\n",
      "|90f1a3e6fcdbf9bc5...| 8310458|\n",
      "|90f1a3e6fcdbf9bc5...| 9045137|\n",
      "|90f1a3e6fcdbf9bc5...|11733005|\n",
      "|90f1a3e6fcdbf9bc5...|  115945|\n",
      "+--------------------+--------+\n",
      "\n",
      "+--------------------+-------+\n",
      "|              userID|paperID|\n",
      "+--------------------+-------+\n",
      "|589b870a611c25fa9...|1001231|\n",
      "|589b870a611c25fa9...| 352713|\n",
      "|90f1a3e6fcdbf9bc5...|3728173|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_exploded = sampled_users.withColumn(\"paperID\", explode(col(\"paperID\")))\n",
    "\n",
    "window = Window.partitionBy(col(\"userID\")).orderBy(rand())\n",
    "#.orderBy(\"userID\")\n",
    "\n",
    "sampled_exploded = sampled_exploded.select(col('*'), row_number().over(window).alias('row_number'))\n",
    "\n",
    "training_df = sampled_exploded.where(col('row_number') <= col(\"trainSize\"))\n",
    "training_df = training_df.select(\"userID\", \"paperID\").orderBy(\"userID\")\n",
    "training_df.show()\n",
    "\n",
    "test_df = sampled_exploded.where(col('row_number') > col(\"trainSize\"))\n",
    "test_df = test_df.select(\"userID\", \"paperID\").orderBy(\"userID\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user profile using the tf_idf dataframe and the users' library dataframe\n",
    "training_user_profile = tf_idf_built_in.join(training_df, [\"paperID\"]).orderBy(\"userID\").select('userId', 'features')\n",
    "\n",
    "# convert the dataframe to RDD to sum up the tf_idf vector of each user and then convert back to dataframe\n",
    "training_user_profile = training_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "\n",
    "training_user_profile = training_user_profile.select(\"userId\", col(\"features\").alias(\"user_profile\"))\n",
    "\n",
    "training_user_profile = training_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"user_profile\")))\n",
    "\n",
    "train_df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(training_user_profile, [\"userID\"])\n",
    "\n",
    "train_df = train_df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\"))\n",
    "\n",
    "train_df = unrated_df.join(tf_idf_built_in, [\"paperID\"]).join(training_user_profile, [\"userID\"])\n",
    "\n",
    "train_df = train_df.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\")).orderBy(\"userID\")\n",
    "\n",
    "#train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a user profile based on the LDA results\n",
    "training_lda_user_profile = transformed.join(training_df, [\"paperID\"]).orderBy(\"userID\").select('userId', col('topicDistribution').alias('features'))\n",
    "\n",
    "training_lda_user_profile = training_lda_user_profile.rdd.mapValues(lambda v: v.toArray()).reduceByKey(lambda x, y: x + y).mapValues(lambda x: DenseVector(x))\\\n",
    ".toDF([\"userId\", \"features\"])\n",
    "training_lda_user_profile = training_lda_user_profile.select(\"userId\", col(\"features\").alias(\"lda_user_profile\"))\n",
    "\n",
    "training_lda_user_profile = training_lda_user_profile.withColumn(\"user_profile\", to_sparse_udf(col(\"lda_user_profile\")))\n",
    "\n",
    "train_df_lda = unrated_df.join(transformed, [\"paperID\"]).join(training_lda_user_profile, [\"userID\"])\n",
    "\n",
    "train_df_lda = train_df_lda.select(\"userID\", \"paperID\", \"user_profile\", col(\"features\").alias(\"paper_profile\")).orderBy(\"userID\")\n",
    "\n",
    "#train_df_lda.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. 5 (Off-line evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF recommender\n"
     ]
    }
   ],
   "source": [
    "# a) Generate 10 recommendations\n",
    "\n",
    "def castToArray(df, colName):\n",
    "    dff = df.withColumn(colName, split(col(colName), \", \").cast(ArrayType(IntegerType())))\n",
    "    return dff\n",
    "\n",
    "# TF-IDF\n",
    "print(\"TF-IDF recommender\")\n",
    "tf_rec = cbrs(train_df, 10).orderBy(\"userID\")\n",
    "tf_rec  = castToArray(tf_rec, \"top_papers\")\n",
    "tf_rec.show(truncate=False)\n",
    "\n",
    "# print(\"LDA recommender\")\n",
    "# LDA\n",
    "lda_rec  = cbrs(train_df_lda, 10).orderBy(\"userID\")\n",
    "lda_rec  = castToArray(lda_rec, \"top_papers\")\n",
    "lda_rec.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_collected = test_df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(test_df.paperID)).alias(\"paperID\"))\n",
    "test_df_collected = test_df_collected.withColumn(\"paperID\", split(col(\"paperID\"), \",\\s*\").cast(ArrayType(IntegerType())).alias(\"paperID\")).orderBy(\"userID\")\n",
    "\n",
    "test_df_collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+\n",
      "|              userID|         test_set|           train_set|\n",
      "+--------------------+-----------------+--------------------+\n",
      "|589b870a611c25fa9...|[1283233, 956315]|[9045137, 115945,...|\n",
      "|90f1a3e6fcdbf9bc5...|         [115945]|[1001231, 352713,...|\n",
      "+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_tf_test = test_df_collected.join(tf_rec, \"userID\")\n",
    "\n",
    "joined_tf_test = joined_tf_test.select(\"userID\", col(\"paperID\").alias(\"test_set\"), col(\"top_papers\").alias(\"train_set\"))\n",
    "\n",
    "joined_tf_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------+--------------------------------------------------+----+\n",
      "|userID                          |test_set         |train_set                                         |Hits|\n",
      "+--------------------------------+-----------------+--------------------------------------------------+----+\n",
      "|589b870a611c25fa99bd3d7295ac0622|[1283233, 956315]|[9045137, 115945, 8310458, 3728173, 11733005]     |[]  |\n",
      "|90f1a3e6fcdbf9bc550e866116bbcea5|[115945]         |[1001231, 352713, 956315, 1305474, 967275, 945604]|[]  |\n",
      "+--------------------------------+-----------------+--------------------------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getHits(train, test):\n",
    "    return list(set(train).intersection(test))\n",
    "\n",
    "getHits_udf = udf(getHits, ArrayType(IntegerType()))\n",
    "\n",
    "tf_hits = joined_tf_test.withColumn('Hits', getHits_udf(joined_tf_test.train_set, joined_tf_test.test_set))\n",
    "\n",
    "tf_hits.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
