{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-01\n",
    "### Amirreza Fosoul and Bithiah Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "spark = SparkSession.builder.appName('ex1').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. 2 (Loading the dataset into an RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRatingsRDD = spark.sparkContext.textFile('./users_libraries.txt')\n",
    "paperTermsRDD = spark.sparkContext.textFile('./papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pair (userID, [paperIDs])\n",
    "def makeKeyValue(line):\n",
    "    line = line.split(';')\n",
    "    return (line[0], line[1].split(','))\n",
    "\n",
    "userRatingsPaired = userRatingsRDD.map(lambda line: makeKeyValue(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stop words and store them in a list\n",
    "stopWords = []\n",
    "with open('./stopwords_en.txt', 'r') as stopWords:\n",
    "    stopWords = stopWords.readlines()\n",
    "    stopWords = [word.replace('\\n', '') for word in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the words inside the abstract text of each paper\n",
    "def splitWords(paper):\n",
    "    # replace multiple whitespaces with a single one\n",
    "    paper = re.sub('\\s\\s+', ' ', paper)\n",
    "    # split the text by comma(ignore commas inside the quoted strings)\n",
    "    result = re.split(''',(?=(?:[^'\"]|'[^']*'|\"[^\"]*\")*$)''', paper)\n",
    "    # remove the punctuation signs from the text\n",
    "    result[-1] = \"\".join((char for char in result[-1] if char not in string.punctuation))\n",
    "    # make list of words of the abstract (the last index of the paper detail is the abstract)\n",
    "    abstract = re.split(' ', result[-1].strip())\n",
    "    abstract_words = []\n",
    "    # ignore the words mentioned in the stop words list\n",
    "    for word in abstract:\n",
    "        if word not in stopWords:\n",
    "            abstract_words.append(word)\n",
    "    # return the pair (paperID, abstractWords)\n",
    "    return (result[0], abstract_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. 3 (Joining collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the papers without abstracts\n",
    "paperTermsPaired = paperTermsRDD.map(lambda paper: splitWords(paper)).filter(lambda x: x[1] != [''])\n",
    "\n",
    "# make the pair ((paperID, word), 1) to start counting\n",
    "paper_word_count = paperTermsPaired.flatMap(lambda x: [((x[0], word) , 1) for word in x[1]])#.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# reduce based on (paperID, word) to sum up the word frequency of each paper\n",
    "paper_word_count = paper_word_count.reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "#convert from ((paperID, word), frequency) to (paperID, (word, frequency))\n",
    "paper_word_count = paper_word_count.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
    "\n",
    "user_paperid = userRatingsPaired.flatMap(lambda x: [(paperID, x[0]) for paperID in x[1]])\n",
    "\n",
    "# let's join two RDDs to have list of words for each user\n",
    "joinedRDD = paper_word_count.join(user_paperid) \n",
    "# joinedRDD is in the format of (paperID, ((word, counter), userID))\n",
    "#joinedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the rdd task is 1274.2255811691284\n"
     ]
    }
   ],
   "source": [
    "# sort the words for each user and return top 10 words\n",
    "def top10Words(words):\n",
    "    result = sorted(words, key=lambda x: x[1], reverse=True)\n",
    "    if len(result) >= 10:\n",
    "        return result[:10]\n",
    "    else:\n",
    "        # if the user has less than 10 words, return the whole sorted list\n",
    "        return result\n",
    "\n",
    "# in this part first we get rid of paperID and then change the format of joinedRDD to ((userID, word), counter)\n",
    "# then calculate the frquency of each (userID, word) and format it like (userID, (word, counter)). After that group them by the userID and \n",
    "# return top 10 words for each user\n",
    "user_words = joinedRDD.map(lambda x: ((x[1][1], x[1][0][0]), x[1][0][1])).reduceByKey(lambda a, b: a+b).map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().map(lambda x: (x[0], top10Words(list(x[1]))))\n",
    "\n",
    "start_rdd = time.time()\n",
    "\n",
    "user_words.collect()\n",
    "\n",
    "end_rdd = time.time()\n",
    "\n",
    "runtime_rdd = end_rdd - start_rdd\n",
    "\n",
    "# Get runtime\n",
    "print(\"Execution time for the rdd task is \" + str(runtime_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a88e213aa257eda198f6bd990d3d6fe2',\n",
       "  [('software', 67),\n",
       "   ('system', 47),\n",
       "   ('code', 38),\n",
       "   ('design', 37),\n",
       "   ('analysis', 36),\n",
       "   ('paper', 34),\n",
       "   ('program', 32),\n",
       "   ('objectoriented', 28),\n",
       "   ('approach', 28),\n",
       "   ('programs', 28)]),\n",
       " ('dd70a8664990fd9b300b1596b9fe3963',\n",
       "  [('information', 110),\n",
       "   ('users', 79),\n",
       "   ('systems', 65),\n",
       "   ('social', 48),\n",
       "   ('user', 47),\n",
       "   ('system', 47),\n",
       "   ('people', 46),\n",
       "   ('results', 46),\n",
       "   ('work', 43),\n",
       "   ('design', 43)]),\n",
       " ('06de0a81ec0fe1e9e983961ee02224ad',\n",
       "  [('social', 56),\n",
       "   ('design', 46),\n",
       "   ('research', 29),\n",
       "   ('paper', 20),\n",
       "   ('book', 20),\n",
       "   ('visualization', 19),\n",
       "   ('work', 18),\n",
       "   ('data', 17),\n",
       "   ('information', 17),\n",
       "   ('community', 15)]),\n",
       " ('d57661d5d9e869cf40eb7623fa288592',\n",
       "  [('tagging', 74),\n",
       "   ('tags', 57),\n",
       "   ('users', 55),\n",
       "   ('social', 49),\n",
       "   ('systems', 44),\n",
       "   ('paper', 36),\n",
       "   ('tag', 35),\n",
       "   ('collaborative', 28),\n",
       "   ('web', 28),\n",
       "   ('user', 22)]),\n",
       " ('d5c5cc742fdbcfc356c2aaf45f0ec848',\n",
       "  [('user', 308),\n",
       "   ('web', 245),\n",
       "   ('information', 230),\n",
       "   ('users', 210),\n",
       "   ('systems', 210),\n",
       "   ('search', 183),\n",
       "   ('social', 108),\n",
       "   ('data', 107),\n",
       "   ('paper', 103),\n",
       "   ('system', 99)]),\n",
       " ('68cd9ed7ddb6baad51c44e9344c28964',\n",
       "  [('dna', 105),\n",
       "   ('rna', 47),\n",
       "   ('structures', 41),\n",
       "   ('molecular', 26),\n",
       "   ('origami', 26),\n",
       "   ('model', 26),\n",
       "   ('design', 24),\n",
       "   ('3d', 24),\n",
       "   ('structure', 23),\n",
       "   ('force', 21)]),\n",
       " ('8656247f88f438a9f5ecae72f238e00d',\n",
       "  [('social', 38),\n",
       "   ('medical', 23),\n",
       "   ('science', 22),\n",
       "   ('technology', 21),\n",
       "   ('studies', 16),\n",
       "   ('article', 15),\n",
       "   ('body', 13),\n",
       "   ('health', 13),\n",
       "   ('technologies', 13),\n",
       "   ('field', 12)]),\n",
       " ('23c1a7106be13304aec605d1cb0631fa',\n",
       "  [('web', 119),\n",
       "   ('data', 66),\n",
       "   ('information', 58),\n",
       "   ('book', 41),\n",
       "   ('text', 38),\n",
       "   ('applications', 32),\n",
       "   ('knowledge', 32),\n",
       "   ('ontologies', 29),\n",
       "   ('mining', 29),\n",
       "   ('biological', 27)]),\n",
       " ('06f960298c70fd725a4480580a5d6d57',\n",
       "  [('visualization', 105),\n",
       "   ('information', 100),\n",
       "   ('data', 86),\n",
       "   ('web', 61),\n",
       "   ('design', 51),\n",
       "   ('services', 42),\n",
       "   ('software', 41),\n",
       "   ('tools', 37),\n",
       "   ('systems', 35),\n",
       "   ('usability', 34)]),\n",
       " ('b7640351a824f4e554921e14cb2fc021',\n",
       "  [('gene', 839),\n",
       "   ('data', 815),\n",
       "   ('genes', 805),\n",
       "   ('expression', 610),\n",
       "   ('networks', 462),\n",
       "   ('protein', 445),\n",
       "   ('network', 398),\n",
       "   ('interactions', 362),\n",
       "   ('proteins', 360),\n",
       "   ('human', 346)])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the ranking of 10 users\n",
    "user_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving rdd result into a file:\n",
    "\n",
    "## Save rdd result into a file (resulting in a directory with multiple txt files)\n",
    "#user_words.saveAsTextFile(\"rdd_output10.txt\")\n",
    "\n",
    "## Read in the result files\n",
    "#output = spark.sparkContext.textFile('./rdd_output10.txt')\n",
    "## Output into a single result file\n",
    "#output.coalesce(1).saveAsTextFile('./rddOutput_final2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. 4 (Basic Analysis for Recommender Systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 28416\n",
      "Execution Time: 2.6447298526763916 seconds\n",
      "\n",
      "Number of items: 172079\n",
      "Execution Time: 4.546404123306274 seconds\n",
      "\n",
      "Number of ratings: 828481\n",
      "Execution Time: 0.28208041191101074 seconds\n",
      "\n",
      "Max number of ratings a user has given: 1922 by user f7d5debb9c7d4d9ab81c63cd26578e23\n",
      "Execution Time: 1.477747917175293 seconds\n",
      "\n",
      "Min number of ratings a user has given: 1 by user ca4f1ba4094011d9a8757b1bfcadae5b\n",
      "Execution Time: 0.0007727146148681641 seconds\n",
      "\n",
      "Average number of ratings of users: 29.155440596846848\n",
      "Execution Time: 0.0003135204315185547 seconds\n",
      "\n",
      "Standard deviation for ratings of users: 81.1751761367\n",
      "Execution Time: 0.3057866096496582 seconds\n",
      "\n",
      "Min number of ratings an item has received: 3 for the paper 503574\n",
      "Max number of ratings an item has received: 924 for the paper 80546\n",
      "Execution Time: 4.50620174407959 seconds\n",
      "\n",
      "Average number of ratings of items: 4.81453867119172\n",
      "Execution Time: 0.4759519100189209 seconds\n",
      "\n",
      "Standard deviation for ratings of items: 5.47780229231\n",
      "Execution Time: 1.2369961738586426 seconds\n"
     ]
    }
   ],
   "source": [
    "# Number of (distinct) user, number of (distinct) items, and number of ratings\n",
    "\n",
    "start_time = time.time()\n",
    "num_users = userRatingsRDD.count()\n",
    "print('Number of users: ' + str(num_users))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "num_items = paperTermsRDD.count()\n",
    "print('\\nNumber of items: ' + str(num_items))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "num_ratings = userRatingsPaired.map(lambda x: len(x[1])).sum()\n",
    "print('\\nNumber of ratings: ' + str(num_ratings))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "#number of ratings for each user, sorted descending\n",
    "\n",
    "start_time = time.time()\n",
    "num_ratings_user = userRatingsPaired.map(lambda entry: (entry[0],len(entry[1]))).sortBy(lambda x: x[1])\n",
    "num_ratings_user_collected = num_ratings_user.collect()\n",
    "\n",
    "print('\\nMax number of ratings a user has given: ' + str(num_ratings_user_collected[::-1][0][1]) + \n",
    "      ' by user ' + num_ratings_user_collected[::-1][0][0])\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "print('\\nMin number of ratings a user has given: ' + str(num_ratings_user_collected[0][1]) + \n",
    "      ' by user ' + num_ratings_user_collected[0][0])\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "# average number of ratings of users\n",
    "print('\\nAverage number of ratings of users: ' + str(num_ratings/num_users))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "# Standard deviation for ratings of users\n",
    "print('\\nStandard deviation for ratings of users: ' + str(num_ratings_user.map(lambda x: x[1]).stdev()))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "# Min/Max number of ratings an item has received\n",
    "start_time = time.time()\n",
    "result = userRatingsPaired.flatMap(lambda data: data[1]).map(lambda paper: (paper, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda x: x[1])\n",
    "paper_ratings = result.collect()\n",
    "\n",
    "print('\\nMin number of ratings an item has received: ' + str(paper_ratings[0][1]) + ' for the paper ' + str(paper_ratings[0][0]))\n",
    "print('Max number of ratings an item has received: ' + str(paper_ratings[::-1][0][1]) + ' for the paper ' + str(paper_ratings[::-1][0][0]))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "# Average number of ratings of items\n",
    "start_time = time.time()\n",
    "total_num_items = result.map(lambda x:x[1]).sum()\n",
    "print('\\nAverage number of ratings of items: ' + str(total_num_items/num_items))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n",
    "\n",
    "# Standard deviation for ratings of items\n",
    "start_time = time.time()\n",
    "print('\\nStandard deviation for ratings of items: ' + str(result.map(lambda x: x[1]).stdev()))\n",
    "print('Execution Time: ' + str(time.time()-start_time) + ' seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. 5 (Loading the dataset into Data Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### DATAFRAME ########################################################\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark.sql.functions as f\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Read user ratings into Dataframe\n",
    "user_df = spark.read.option(\"delimiter\", \";\").csv('./users_libraries.txt')\n",
    "user_df = user_df.select(col(\"_c0\").alias(\"userID\"), col(\"_c1\").alias(\"paperID\")) \n",
    "\n",
    "# Read in the stopwords as a list\n",
    "with open('./stopwords_en.txt') as file:\n",
    "    stopWords = file.read().splitlines()\n",
    "\n",
    "# Read in records of paper information\n",
    "w_df = spark.read.csv('./papers.csv')\n",
    "w_df = w_df.select(\"_c0\", \"_c14\")\n",
    "w_df = w_df.select(col(\"_c0\").alias(\"paperID\"), col(\"_c14\").alias(\"words\"))\n",
    "\n",
    "# Remove punctuations and extra spaces\n",
    "words_df = w_df.withColumn(\"words\", regexp_replace(col(\"words\"), '[^\\w\\s]', \"\"))\n",
    "words_df = words_df.withColumn(\"words\", regexp_replace(col(\"words\"), '\\s\\s+', \" \"))\n",
    "# Split each word in the abstracts\n",
    "words_df = words_df.withColumn(\"words\", split(\"words\", \" \"))\n",
    "# # Remove papers that has an empty abstract\n",
    "words_df = words_df.filter((col(\"words\").isNotNull()))\n",
    "# User defined function to filter stopwords from the column of words\n",
    "sd = udf(lambda x: [item for item in x if item not in stopWords], ArrayType(StringType()))\n",
    "words_df = words_df.withColumn(\"words\", sd(col(\"words\")))\n",
    "\n",
    "# words_df pre-explode (pre-split)\n",
    "words_df_pre = words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6 (Tasks on top of DataFrames)\n",
    "\n",
    "### 1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split (explode) the paperID list per user into individual rows\n",
    "# Select the paperID w.r.t userID and split the list of paperIDs\n",
    "# Split the list of paperIDs per user by a comma\n",
    "# pos is the position of each paper in the array\n",
    "# val is the paperID\n",
    "user_df = user_df.select(\"userID\", f.split(\"paperID\", \",\").alias(\"paperID\"), f.posexplode(f.split(\"paperID\", \",\")).alias(\"pos\", \"val\"))\n",
    "user_df = user_df.drop(\"val\")\n",
    "# Get paperID w.r.t. position in the paperID list\n",
    "user_df = user_df.select(\"userID\",f.expr(\"paperID[pos]\").alias(\"paperID\"))\n",
    "user_df = user_df.select(\"userID\", \"paperID\")\n",
    "\n",
    "# Count the number of ratings per user\n",
    "numRatings_user = user_df.groupBy('userID').count()\n",
    "\n",
    "## Example:\n",
    "## Print a table with columns of userID and paperID\n",
    "#user_df.where(user_df.userID==\"df5002093c4d5a8448f4002bbb26d43f\").show()\n",
    "## Print a table with userID and the number of ratings w.r.t. the user\n",
    "#numRatings_user.where(user_df.userID==\"df5002093c4d5a8448f4002bbb26d43f\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split (explode) the list of words into a column of tokens and\n",
    "# count the number of occurences of the tokens per paper\n",
    "words_df = words_df.select(\"paperID\", f.explode(\"words\").alias(\"token\")).groupBy(\"paperID\", \"token\").count()\n",
    "\n",
    "# Join the users and papers dataframe\n",
    "df = user_df.join(words_df, [\"paperID\"])\n",
    "\n",
    "# For each user merge the tokens and sum up the duplicated tokens\n",
    "df = df.groupBy(\"userID\", \"token\").agg(f.collect_list(\"token\").alias(\"duplicated_values\"), f.sum(\"count\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              userID|            Rankings|\n",
      "+--------------------+--------------------+\n",
      "|03237605301d9dd8e...|genes 31, gene 24...|\n",
      "|03a537c35019f00cf...|bc 20, mass 12, a...|\n",
      "|04e7b99118265af25...|learning 23, info...|\n",
      "|066349c2a8a65717c...|gprd 15, 95 13, c...|\n",
      "|08f340d9d9ede55ae...|projects 3, boinc...|\n",
      "|0cf98522336c994a4...|analysis 23, prog...|\n",
      "|0d875b8672933b9a5...|web 5, 20 3, worl...|\n",
      "|0de84fbc3c077868e...|health 36, global...|\n",
      "|125683f9d7eed5af3...|mirnas 63, cancer...|\n",
      "|13a5067c6e8d3ced6...|health 159, devel...|\n",
      "|157f60f600cffe9a9...|library 8, librar...|\n",
      "|164a0015552fbe011...|transporters 7, a...|\n",
      "|1675f231acf9d1cbb...|nrf2 30, expressi...|\n",
      "|188b2723f7349804c...|india 3, rushdies...|\n",
      "|1936cc3b52835fdfc...|neurons 36, stimu...|\n",
      "|1a37d3c3eaf90d925...|model 2, neuron 2...|\n",
      "|1c7a5a7c09813054f...|immune 5, model 5...|\n",
      "|2086aff81a8d6f9d9...|photography 24, w...|\n",
      "|211103152ac9c903c...|career 30, career...|\n",
      "|236ef17c11f7f43ac...|agave 16, 1 13, f...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for the dataframe task is 227.91062259674072\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "df = df.drop(\"duplicated_values\")\n",
    "\n",
    "# Order the token count in descending order w.r.t. users\n",
    "window = Window.partitionBy(col(\"userID\")).orderBy((col(\"count\")).desc())\n",
    "\n",
    "# Create a new column with the row numbers of the count w.r.t. user \n",
    "# Get the first 10 rows\n",
    "df = df.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= 10)\n",
    "\n",
    "# Concatenate the word and the count as one column\n",
    "df = df.withColumn(\"word_count\", f.concat(col(\"token\"), lit(\" \"), col(\"count\")))\n",
    "df = df.drop(\"token\", \"count\")\n",
    "\n",
    "# Get a dataframe with a column of unique users and a column of the top 10 occurences of words\n",
    "df = df.groupby(\"userID\").agg(f.concat_ws(\", \", f.collect_list(df.word_count)).alias(\"Rankings\"))\n",
    "\n",
    "# Compute runtime\n",
    "start = time.time()\n",
    "df.show()\n",
    "end = time.time()\n",
    "\n",
    "runtime = end - start\n",
    "\n",
    "# Get runtime\n",
    "print(\"Execution time for the dataframe task is \" + str(runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 (Basic Analysis for Recommender Systems Using Data Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join paper and user dataframes to get the count of ratings per item\n",
    "paper_user = user_df.join(words_df_pre, [\"paperID\"])\n",
    "paper_user = paper_user.drop(\"words\")\n",
    "\n",
    "# Count the number of ratings per paper\n",
    "numRatings_paper = paper_user.groupBy('paperID').count()\n",
    "\n",
    "## Example showing the list of users who rated a paper\n",
    "#paper_user.where(paper_user.paperID==\"1103674\").show()\n",
    "\n",
    "## Example showing the count of ratings of a paper\n",
    "#numRatings_paper.where(numRatings_paper.paperID==\"1103674\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of distinct users is 28416\n",
      "Execution time: 0.883568286895752\n",
      "\n",
      "The number of distinct papers is 172079\n",
      "Execution time: 2.7177493572235107\n",
      "\n",
      "The number of ratings is 828481\n",
      "Execution time: 0.3659782409667969\n",
      "\n",
      "The min number of rating(s) given by a user is 1\n",
      "Execution time: 0.8987569808959961\n",
      "\n",
      "The max number of ratings given by a user is 1922\n",
      "Execution time: 1.2685301303863525\n",
      "\n",
      "The average number of ratings of users is 29.16\n",
      "Execution time: 1.797194480895996\n",
      "\n",
      "The standard deviation for ratings of users is 81.18\n",
      "Execution time: 1.4369292259216309\n",
      "\n",
      "The min number of rating(s) of a paper is 3\n",
      "Execution time: 50.070183992385864\n",
      "\n",
      "The max number of ratings a paper has received is 924\n",
      "Execution time: 48.16281700134277\n",
      "\n",
      "The average number of ratings of papers is 5.0\n",
      "Execution time: 48.622234582901\n",
      "\n",
      "The standard deviation for ratings of papers is 6.08\n",
      "Execution time: 48.87841033935547\n"
     ]
    }
   ],
   "source": [
    "############################################# DATA ANALYSIS #######################################################\n",
    "import time\n",
    "start = time.time()\n",
    "## NUMBER OF DISTINCT USERS ##\n",
    "\n",
    "numUsers = user_df.select(\"userID\").distinct().count()\n",
    "print(\"\\nThe number of distinct users is \" + str(numUsers))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "## NUMBER OF DISTINCT PAPERS ##\n",
    "start = time.time()\n",
    "numPapers = w_df.distinct().count()\n",
    "print(\"\\nThe number of distinct papers is \" + str(numPapers))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "# ## NUMBER OF RATINGS ##\n",
    "start = time.time()\n",
    "numRatings = user_df.count()\n",
    "print(\"\\nThe number of ratings is \" + str(numRatings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "# ################################### DATA ANALYSIS W.R.T USERS #################################################### \n",
    "\n",
    "# ## MIN NUMBER OF RATINGS GIVEN BY A USER ##\n",
    "start = time.time()\n",
    "min_num_ratings = numRatings_user.select(\"count\").agg(min(\"count\")).head()[0]\n",
    "print(\"\\nThe min number of rating(s) given by a user is \" + str(min_num_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "# ## MAX NUMBER OF RATINGS GIVEN BY A USER ##\n",
    "start = time.time()\n",
    "max_num_ratings = numRatings_user.select(\"count\").agg(max(\"count\")).head()[0]\n",
    "print(\"\\nThe max number of ratings given by a user is \" + str(max_num_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "# ## AVERAGE NUMBER OF RATINGS OF USERS ## \n",
    "start = time.time()\n",
    "avg_num_ratings = numRatings_user.select(\"count\").agg(mean(\"count\")).head()[0]\n",
    "avg_num_ratings = float(\"{0:.2f}\".format(avg_num_ratings))\n",
    "print(\"\\nThe average number of ratings of users is \" + str(avg_num_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "# ## STANDARD DEVIATION FOR RATINGS OF USERS ## \n",
    "start = time.time()\n",
    "std_ratings = numRatings_user.select(\"count\").agg(stddev(\"count\")).head()[0]\n",
    "std_ratings = float(\"{0:.2f}\".format(std_ratings))\n",
    "print(\"\\nThe standard deviation for ratings of users is \" + str(std_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "# ####################################### DATA ANALYSIS W.R.T PAPERS ################################################\n",
    "\n",
    "# ## MIN NUMBER OF RATINGS A PAPER HAS RECEIVED ##\n",
    "start = time.time()\n",
    "min_paper_ratings = numRatings_paper.select(\"count\").agg(min(\"count\")).head()[0]\n",
    "print(\"\\nThe min number of rating(s) of a paper is \" + str(min_paper_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "# ## MAX NUMBER OF RATINGS A PAPER HAS RECEIVED ##\n",
    "start = time.time()\n",
    "max_paper_ratings = numRatings_paper.select(\"count\").agg(max(\"count\")).head()[0]\n",
    "print(\"\\nThe max number of ratings a paper has received is \" + str(max_paper_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "# ## AVERAGE NUMBER OF RATINGS OF ITEMS\n",
    "\n",
    "start = time.time()\n",
    "avg_paper_ratings = numRatings_paper.select(\"count\").agg(mean(\"count\")).head()[0]\n",
    "avg_paper_ratings = float(\"{0:.2f}\".format(avg_paper_ratings))\n",
    "print(\"\\nThe average number of ratings of papers is \" + str(avg_paper_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))\n",
    "\n",
    "# ## STANDARD DEVIATION FOR RATINGS OF ITEMS ##\n",
    "start = time.time()\n",
    "std_paper_ratings = numRatings_paper.select(\"count\").agg(stddev(\"count\")).head()[0]\n",
    "std_paper_ratings = float(\"{0:.2f}\".format(std_paper_ratings))\n",
    "print(\"\\nThe standard deviation for ratings of papers is \" + str(std_paper_ratings))\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the dataframe to file:\n",
    "\n",
    "## Write df to a parquet directory resulting in multiple parquet files\n",
    "#df.write.parquet(\"DF_Final_Results6.parquet\")\n",
    "\n",
    "## Read in parquet directory and convert to a single csv file\n",
    "#output = spark.read.parquet('DF_Final_Results6.parquet')\n",
    "#output.toPandas().to_csv(\"result_empty11.csv\", header=False, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "The execution time for the rdd task was about 20 minutes (1203 s) while for the dataframe task was about 4 minutes (228 s). Using dataframe in Spark is more efficient because the SparkSQL library incorporates performance optimization, therefore the runtime for making queries is faster.\n",
    "\n",
    "Source (https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
